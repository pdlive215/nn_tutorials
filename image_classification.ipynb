{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONVOLUTIONAL NEURAL NETWORKS FOR IMAGE CLASSIFICATION**\n",
    "\n",
    "*Patrick Donnelly, Groupware Technology*\n",
    "\n",
    "This lab introduces the architectures of *convolutional neural networks*, a set of algorithms that have come to dominate computer vision research and applications over the past seven years. We examine some of the most influential architectures in chronological order: LeNet-5, AlexNet, VGG, Inception v3, and ResNet. It will help to have some familiarity with neural network operations and concepts. We cover this in a separate workshop: \"An Introduction to Neural Networks.\" We implement each network in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to classify an image. Let's consider the hypothetically simple case of a 5x5 \"image\" of pixels that can take the values 0 or 1. Think of the image as a number on a scoreboard. When a pixel equals 1, the light turns on. We can thus define an image in Python using a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_zero = [[0,1,1,1,0],[1,0,0,0,1],[1,0,0,0,1],[1,0,0,0,1],[0,1,1,1,0]]\n",
    "a_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we want to classify this image as a digit. Looks like a zero, no? We define our zero label using a **one-hot encoding**. A one-hot encoding is simply a vector where each index corresponds to a class. The image is labeled by assigning a $1$ to the index of the image's class. Every other entry is assigned a $0$. In the case of digit classification, we can simply label the index corresponding to the digit with a $1$ and leave the others as $0$s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_encoding = [1,0,0,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we need is an algorithm that takes **a_zero** as an input and generates **zero_encoding** as an output with sufficient accuracy. Of course in \"real life\" we do this with thousands or millions of images, and each image is far larger than our 5x5 array of one-bit pixels. The advantage of keeping our data (or datum!) simple is that we can focus on the logic of the algorithm and wrap our head around the operations transforming our input into (predicted) output.\n",
    "\n",
    "The simplest transformation (at a certain level of generality) we can apply to our input is linear. If we apply a linear transformation to an input pixel, we multiply that pixel by a **weight** and add a **bias**. We then sum the transformed inputs connecting our input **neuron** (e.g. pixel) with our output neuron (corresponding to a particular class).\n",
    "\n",
    "If this is getting too abstract (as it was for the author of this workshop when he first encountered neural networks), consider our specific example. Let's represent **a_zero** (a list of lists) as a **flattened** vector (list):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_zero_vector = [0,1,1,1,0,1,0,0,0,1,1,0,0,0,1,1,0,0,0,1,0,1,1,1,0]\n",
    "a_zero_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say we want to \"linearly transform\" this vector of input pixels into our class vector. We'll first need to **matrix multiply** the input by a **weight matrix**. We'll import a library called **numpy** to perform matrix operations in Python. By convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **np.array** to represent our lists as numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0],\n",
       "       [1, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 1],\n",
       "       [0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_zero_array = np.array(a_zero)\n",
    "a_zero_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_zero_vector_array = np.array(a_zero_vector)\n",
    "a_zero_vector_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_encoding_array = np.array(zero_encoding)\n",
    "zero_encoding_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the dimensions of our arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(a_zero_array.shape)\n",
    "print(zero_encoding_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to reshape these arrays as (25,1) and (10,1) vectors so that we can do matrix math:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_zero_array_reshaped = a_zero_array.reshape(25,1)\n",
    "print(a_zero_array_reshaped.shape)\n",
    "a_zero_array_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_encoding_array_reshaped = zero_encoding_array.reshape(10,1)\n",
    "print(zero_encoding_array_reshaped.shape)\n",
    "zero_encoding_array_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct our weight matrix. If $A$ is an $nxm$ matrix and $B$ is an $mxp$ matrix, the product of $A$ and $B$ is an $nxp$ matrix. See https://en.wikipedia.org/wiki/Matrix_multiplication\n",
    "\n",
    "For our example, we can let $B$ be our (25,1) input vector. Thus $m=25$ and $p=1$. Our output is our (10,1) output vector, where $n=10$ and (again) $p=1$. Thus the dimensions of $A$ are $nxm$: (10,25). We can randomly initialize the entries in this matrix using **np.random.randn**, which will sample each value from a normal distribution with mean $0$ and variance $1$. We'll call our weight matrix $W$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08959282,  1.23659129,  0.56369847, -0.8525387 ,  0.559432  ,\n",
       "         1.47691024, -0.08397301,  1.1315573 , -0.02773398, -0.31005974,\n",
       "        -0.52285462, -0.11918065,  0.11545375, -1.02465101, -0.31175265,\n",
       "        -1.15691582,  0.67346433, -0.2259513 , -1.08184905,  1.78045084,\n",
       "        -0.24103242, -0.40644409, -0.13537547,  0.71744809, -0.79537288],\n",
       "       [ 0.73186254,  1.37300205, -0.60917055,  0.7650649 ,  0.83751162,\n",
       "         0.07958813, -0.32937862, -0.01131647,  0.69321975,  0.51753006,\n",
       "        -0.69461994, -0.6962348 ,  1.60917693, -0.11310379,  0.39129755,\n",
       "         0.11881143,  0.44642317,  0.66268916, -0.63407709,  1.33305708,\n",
       "         2.16614866, -0.20464374,  0.12178699,  0.83161651, -0.43977785],\n",
       "       [ 0.62921154,  0.06338423,  0.37627663,  0.61492115, -0.60100287,\n",
       "         2.50959962, -0.01203361, -0.73906838, -0.78917858, -0.16397109,\n",
       "         0.55888729, -0.21641854,  0.088418  ,  3.04592433,  0.47819311,\n",
       "         1.7837566 , -1.15263144,  0.72841872, -0.83216096,  0.38431854,\n",
       "        -1.36984204, -0.27607446, -1.49808407, -0.71798364,  0.95945093],\n",
       "       [-0.48485467, -1.0309519 ,  1.40702032,  0.04534918,  1.46041939,\n",
       "        -0.35895975, -0.45378907, -0.85644143, -1.22764679,  0.03155874,\n",
       "        -0.33994052, -0.11987077, -1.06010659,  0.72849375,  0.69104946,\n",
       "        -2.10334955,  1.31789969, -0.21035879, -0.05770752, -0.37653846,\n",
       "        -0.22713011, -2.0271157 ,  1.27979618,  1.97195824, -1.25281345],\n",
       "       [ 0.52512606,  0.44130129, -0.0992023 , -0.54035451, -0.05558043,\n",
       "        -0.94335682,  1.05487748, -0.62881944,  1.45405094, -0.17422929,\n",
       "         0.03207458, -0.69823422, -1.16551477,  1.08943628, -1.81102731,\n",
       "        -0.1475723 , -0.09043269, -0.87516799, -0.41958659, -1.23767714,\n",
       "         0.1098689 , -0.41355758, -0.68308929,  0.83382493, -0.89250298],\n",
       "       [ 0.83291556,  0.27840595, -0.44333404, -0.3466131 ,  0.47674557,\n",
       "         1.35085081, -0.69764705, -0.75466094, -0.7082859 ,  1.70644041,\n",
       "         0.24964184, -0.49535478, -1.08263568, -0.35553655, -0.16100473,\n",
       "         0.59179917, -1.44720694,  0.89792373,  0.61090206, -0.01139158,\n",
       "        -0.14941321, -0.04799288,  1.07653309, -1.29847406,  0.27687162],\n",
       "       [-0.91233114, -1.22751437, -1.16839285,  1.13634043, -0.15251943,\n",
       "         0.28991789, -0.3208255 , -0.48109789,  1.18818058,  1.19946461,\n",
       "        -0.92729884,  0.82860527, -0.36599969, -0.73561707,  0.21544986,\n",
       "        -0.41992124, -0.71357994,  1.67781386, -0.56604835,  0.01198321,\n",
       "         1.37458448, -1.2817678 , -0.05378098, -0.93438421,  0.11537928],\n",
       "       [ 0.15161816,  0.62293071, -1.4000886 ,  0.39380871, -1.57255157,\n",
       "         0.60684331,  0.49720739, -1.32937991, -0.12860352, -0.85713716,\n",
       "        -0.63617645,  0.2863228 , -0.00681658,  0.44547767,  0.84769622,\n",
       "         0.38942274, -1.49653649, -1.23006425, -0.6447319 , -0.68092785,\n",
       "        -0.25592277, -0.00606243,  0.10551539,  0.40202452, -0.50966086],\n",
       "       [-0.78489573,  1.03690769, -0.94484565,  0.68781089, -1.13657515,\n",
       "         1.76073412,  0.22722446, -1.34485911, -0.51933911, -0.6008076 ,\n",
       "        -0.70102982, -1.71029815,  0.77280573,  1.93174139, -0.18741753,\n",
       "         1.12931472, -0.52574695, -1.07961148,  0.07633602,  0.34073117,\n",
       "         0.52094719, -1.97746496,  0.45696913, -2.61397591, -0.6616975 ],\n",
       "       [ 0.30577559,  0.41355516,  1.85799326, -0.87098203, -1.21308637,\n",
       "        -0.89684179,  0.68880261, -0.00361143,  1.03694631,  1.17873105,\n",
       "        -0.06549845,  1.16765552, -0.75331025,  0.34176956,  1.61178232,\n",
       "         0.91892361, -0.19555693,  1.76182231,  0.28821082, -0.00450123,\n",
       "         0.47253903,  1.59133926,  0.45775684, -1.11385382, -0.6146932 ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.random.randn(10,25)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also initialize a bias vector $b$ to add to the product of our weight matrix $W$ and our input vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.60884757],\n",
       "       [ 0.69341915],\n",
       "       [ 2.13290841],\n",
       "       [-0.18740879],\n",
       "       [-0.94227445],\n",
       "       [ 1.17850932],\n",
       "       [ 0.18730949],\n",
       "       [ 1.0631529 ],\n",
       "       [-0.68747027],\n",
       "       [ 0.06825139]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randn(10,1)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural network involves...\n",
    "\n",
    "1) Mutiplying a weight matrix $W$ by our input vector\n",
    "\n",
    "2) Adding a bias vector to the product of the weight matrix and input vector\n",
    "\n",
    "...as well as some stuff that we'll cover later, and in more detail in \"An Introduction to Neural Networks\"\n",
    "\n",
    "3) Using an **activation function** to transform the output of our matrix multiplication and addition\n",
    "\n",
    "4) Repeating steps 1-3\n",
    "\n",
    "5) Applying a **softmax** operation to normalize our activated neurons\n",
    "\n",
    "6) Computing our loss\n",
    "\n",
    "7) Updating our weights\n",
    "\n",
    "In our example, we have connected each input neuron to each output neuron. This is what is known as a **fully-connected** or **dense** neural network. (The lexicon of deep learning is full of synonyms.) Hence we're assuming that the class of an image is optimally learned (in terms of accuracy and computational efficiency) by summing up weighted pixel values and adding a bias to the output.\n",
    "\n",
    "It turns out there's a better way of doing things. Enter the **convolutional neural network** (**CNN**, **conv net**, what did we just say about these synonyms?) Rather than learning a weight associated with each input-output pair, a convolutional neural network learns a set of **shared weights** (**filters**, **kernels**, **receptive fields**). These shared weights have a **shape**, **volume**, and **stride**. Filters are typically square and identical across all shared weights in a **layer** connecting input and output neurons. The **volume** defines how many filters to learn for a particular layer. \n",
    "\n",
    "To compute the output of a convolutional layer, we **stride** each filter across the input matrix (using our example, think of the image as a 5x5 matrix rather than a 25x1 vector) and take the **Hadamard product** (**Schur product**, **entrywise product**): we simply multiply the values of each entry in the filter with the corresponding values in the input matrix. We then sum over all the outputs. Some people call this the dot product. I don't know about that, but I'm not a mathematician.\n",
    "\n",
    "We implement convolutions in NumPy in our introductory neural network course. If you're having trouble wrapping your head around the convolutional operation, you might also want to check out Stanford course on convolutional neural networks: http://cs231n.github.io. Chris Olah also has a great blog that explains convolutions (among other things): https://colah.github.io/posts/2014-07-Understanding-Convolutions/\n",
    "\n",
    "We have a lot to cover, so let's import PyTorch before we get too lost in the convolutional weeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a **framework** for defining and executing neural network operations. It's developed by Facebook, along with Caffe2, originally built by a Berkeley PhD who sold out.** Other frameworks include TensorFlow (Google), MXNet (Amazon), and Keras (which is actually a higher-level API that simplifies coding neural networks in TensorFlow and other frameworks). PyTorch is already simple enough, so we don't need Keras.\n",
    "\n",
    "** n.b. Pat has a PhD from UC Berkeley\n",
    "\n",
    "PyTorch enables us to define neural network layers in one line of code using the **nn** and **nn.functional** models. Let's import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **nn.linear** to define a dense layer (conventionally **fc** for fully-connected) connecting 25 inputs with 10 outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(25,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a neural network, we define a class **Net** that inherits from **nn.Module**. Within this __Net__, we define our neural network architecture using an **init** module, along with our __forward propagation__ using **forward**. Forward propagation is the sequential application of operations to transform our input (image) into output (class vectors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(25,10)\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has some good tutorials with sample neural networks. I'm using this one as a reference: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
    "\n",
    "For some reason the PyTorch documentation is slow to load, and I don't like the design. It's almost enough to make me switch to TensorFlow.\n",
    "\n",
    "Now we can call an instance **net** of our network **Net()** and print it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc): Linear(in_features=25, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a simple network comprised of one dense (fully-connected, linear) layer, 25 input nodes (features), and 10 output nodes. **bias=True** indicates that we're adding a bias vector to our matrix multiplication. We can remove our bias vector by setting **bias** to **False**, for instance in **self.fc = nn.Linear(25,10,bias=False)**\n",
    "\n",
    "What if we want to replace our dense layer with a convolutional layer? We use **nn.Conv2d** to specify that we're doing a **two-dimensional convolution**. Our convolution is two-dimensional because we're applying a 2D filter to a corresponding 2D input. Here's an article that will help you to understand other types of convolutions: https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215\n",
    "\n",
    "Remember what we need to define a convolution: shape, volume, and stride! In PyTorch, we define volume using the parameter **out_channels**. We also specify our number of input channels with **in_channels**. If we're using our toy example of a 5x5 image of one-bit pixels, we only have one input channel. We define shape using **kernel_size** (the width and height of the kernel, assuming a square kernel). Stride is simply **stride**. Let's say we want to apply a 2x2 kernel with stride 1 and generate 4 output channels from our single input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=2, stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can replace our fully-connected layer with our convolutional layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv): Conv2d(1, 4, kernel_size=(2, 2), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=2, stride=1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so we've learned how to implement a convolution in PyTorch! However, convolutional neural networks generally contain at least six operations (and often more):\n",
    "\n",
    "1) **Convolutions** to extract visual information in the form of shared weights\n",
    "\n",
    "2) **Pooling** to downsample the output of convolutional operations\n",
    "\n",
    "3) **Activations** to learn nonlinear features\n",
    "\n",
    "4) **Flattening** to reshape the three-dimensional output into a one-dimensional vector\n",
    "\n",
    "5) **Linear transformation** to downsample the flattened output to a vector whose length corresponds to the number of classes\n",
    "\n",
    "6) **Softmax** to convert the raw output into a probability vector\n",
    "\n",
    "Let's continue with pooling. If you're unsatisfied by the mathematical complexity of neural networks so far (it's just multiplications and additions??), be prepared to be further disappointed. The most common way to apply pooling is to take the maximum value over a receptive field. With operations like this and words like that, it's surprising that computer vision isn't dominated by English majors. We call this **max pooling**.\n",
    "\n",
    "In PyTorch, we apply max pooling (over a two-dimensional receptive field) using **nn.MaxPool2d**. Again, we specify __kernel_size__ and **stride** (but not input and output channels). Let's say we want to pool over a 2x2 area with stride of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = nn.MaxPool2d(kernel_size=2, stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add this to our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv): Conv2d(1, 4, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=2, stride=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.conv(x))\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on to our **activation** function. There are lots of ways in which we could introduce a nonlinearity into our network. For more details, see the \"Introduction to Neural Networks\" tutorial, as well as this overview of activation functions: http://cs231n.github.io/neural-networks-1/#actfun. Actually, you all should just take the Stanford course; it's amazing.\n",
    "\n",
    "We need our activation function to do a couple of things:\n",
    "\n",
    "1) It needs to be nonlinear, of course. This is the whole point - we want to learn a nonlinear function between input and output.\n",
    "\n",
    "2) It needs to enable the network to train effectively. When we update a weight in our network, we take the partial derivative of our loss function with respect to the weight, scale the derivative by our learning rate, and subtract this value from our weight. This requires us to apply the **chain rule**, where we take the derivative of each output with respect to each input all the way through the network. If the derivative is zero (or close to zero) for any layer (set of input/output pairs), we won't be able to update our weights. Long story short, we need the derivative of our activation function to be nonzero in order to learn. It's okay if the derivative is zero for some inputs. As we'll see in just a second, this is a property of one of the most commonly-used activation functions for convolutional neural networks.\n",
    "\n",
    "Enter the **rectified linear unit (ReLU)**. All ReLU does is set our negative inputs to zero! We could probably write this function ourselves, but PyTorch makes it even simpler. We just pass our input to **F.relu()** (remember we have the functional API __F__ in addition to the **nn** API). Let's add a ReLU to our network after the convolution and pooling operations. Note that this doesn't change the output of **print(net)**, even though it'll change the value of our outputs for sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv): Conv2d(1, 4, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=2, stride=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv(x)))\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a second to think about how we've transformed the dimensions of our input. We can be totally explicit with this since our datum is small.\n",
    "\n",
    "We started with our input $X$, equal to our \"zero\" image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 1, 1, 0],\n",
       " [1, 0, 0, 0, 1],\n",
       " [1, 0, 0, 0, 1],\n",
       " [1, 0, 0, 0, 1],\n",
       " [0, 1, 1, 1, 0]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = a_zero\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we stride a $2x2$ filter across this image? Let's assume for the sake of example that our filter is simply a $2x2$ array of ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1], [1, 1]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = [[1,1],[1,1]]\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start striding across $X$. Since all of the values of F are 1, the elementwise product is simply the input values at every point. Thus we just need to sum over $2x2$ regions of $X$, then stride one column at a time, then do this for the next two rows, all the way until we have strided across the entire array. Every time we begin to stride across a new pair of rows, we similarly begin to output the values of our convolutions in a new row:\n",
    "\n",
    "                    Rows (0,1), Columns (0,1): 0+1+1+0 = 2 [Enter in (0,0)]\n",
    "    Stride 1 right: Rows (0,1), Columns (1,2): 1+1+0+0 = 2 [Enter in (0,1)]\n",
    "    Stride 1 right: Rows (0,1), Columns (2,3): 1+1+0+0 = 2 [Enter in (0,2)]\n",
    "    Stride 1 right: Rows (0,1), Columns (3,4): 1+0+0+1 = 2 [Enter in (0,3)]\n",
    "    Stride 1 down:  Rows (1,2), Columns (0,1): 1+0+1+0 = 2 [Enter in (1,0)]\n",
    "    Stride 1 right: Rows (1,2), Columns (1,2): 0+0+0+0 = 0 [Enter in (1,1)]\n",
    "    Stride 1 right: Rows (1,2), Columns (2,3): 0+0+0+0 = 0 [Enter in (1,2)]\n",
    "    Stride 1 right: Rows (1,2), Columns (3,4): 0+1+0+1 = 2 [Enter in (1,3)]\n",
    "    Stride 1 down:  Rows (2,3), Columns (0,1): 1+0+1+0 = 2 [Enter in (2,0)]\n",
    "    Stride 1 right: Rows (2,3), Columns (1,2): 0+0+0+0 = 0 [Enter in (2,1)]\n",
    "    Stride 1 right: Rows (2,3), Columns (2,3): 0+0+0+0 = 0 [Enter in (2,2)]\n",
    "    Stride 1 right: Rows (2,3), Columns (3,4): 0+1+0+1 = 2 [Enter in (2,3)]\n",
    "    Stride 1 down:  Rows (3,4), Columns (0,1): 1+0+0+1 = 2 [Enter in (3,0)]\n",
    "    Stride 1 right: Rows (3,4), Columns (1,2): 0+0+1+1 = 2 [Enter in (3,1)]\n",
    "    Stride 1 right: Rows (3,4), Columns (2,3): 0+0+1+1 = 2 [Enter in (3,2)]\n",
    "    Stride 1 right: Rows (3,4), Columns (3,4): 0+1+1+0 = 2 [Enter in (3,3)]\n",
    "    \n",
    "This yields our first output **feature map**. We'll call it $V_0$ (for volume):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 2, 2, 2], [2, 0, 0, 2], [2, 0, 0, 2], [2, 2, 2, 2]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_0 = [[2,2,2,2],[2,0,0,2],[2,0,0,2],[2,2,2,2]]\n",
    "V_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then repeat this operation three times, yielding a total of four feature maps (equivalent to our number of **output_features**). Just for example, let's assume our second feature map $V_1$ is **initialized** as an array of twos (doubling our output relative to the first array), our third feature map $V_2$ is an array of threes (tripling our output relative to the first array), and our fourth feature map $V_3$ is an array of zeroes (yielding an array of zeroes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 2, 2, 2], [2, 0, 0, 2], [2, 0, 0, 2], [2, 2, 2, 2]]\n",
      "[[4, 4, 4, 4], [4, 0, 0, 4], [4, 0, 0, 4], [4, 4, 4, 4]]\n",
      "[[6, 6, 6, 6], [6, 0, 0, 6], [6, 0, 0, 6], [6, 6, 6, 6]]\n",
      "[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "V_1 = [[4,4,4,4],[4,0,0,4],[4,0,0,4],[4,4,4,4]]\n",
    "V_2 = [[6,6,6,6],[6,0,0,6],[6,0,0,6],[6,6,6,6]]\n",
    "V_3 = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
    "print(V_0)\n",
    "print(V_1)\n",
    "print(V_2)\n",
    "print(V_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pool over each $2x2$ region of each feature map {V_0, V_1, V_2, V_3}. Consider $V_0$:\n",
    "\n",
    "        Rows (0,1), Columns (0,1): 2 [Enter in (0,0)]\n",
    "        Rows (0,1), Columns (2,3): 2 [Enter in (0,1)]\n",
    "        Rows (2,3), Columns (0,1): 2 [Enter in (1,0)]\n",
    "        Rows (2,3), Columns (2,3): 2 [Enter in (1,1)]\n",
    "        \n",
    "Let's define $P_0$ as the output of pooling over $V_0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 2], [2, 2]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_0 = [[2,2],[2,2]]\n",
    "P_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then do the same for $V_1$, $V_2$, and $V_3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 2], [2, 2]]\n",
      "[[4, 4], [4, 4]]\n",
      "[[6, 6], [6, 6]]\n",
      "[[0, 0], [0, 0]]\n"
     ]
    }
   ],
   "source": [
    "P_1 = [[4,4],[4,4]]\n",
    "P_2 = [[6,6],[6,6]]\n",
    "P_3 = [[0,0],[0,0]]\n",
    "print(P_0)\n",
    "print(P_1)\n",
    "print(P_2)\n",
    "print(P_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up:\n",
    "\n",
    "1) We applied convolutons to our 5x5 input (25 pixels or nodes) to learn four 4x4 arrays (4x4x4 = 64 nodes)\n",
    "\n",
    "2) We used pooling to downsample our four 4x4 arrays (64 nodes) to four 2x2 arrays (4x2x2 = 16 nodes)\n",
    "\n",
    "3) Now we need to downsample further from 16 nodes to 10 nodes (the number of classes in our output vector)\n",
    "\n",
    "4) However, we first need to **concatenate** or \"flatten\" our output into a single dimension. PyTorch makes it easy to flatten __tensors__ (multidimensional arrays) using the **view** method. We pass two arguments to __view__: a $-1$ (indicating that we want to flatten the tensor to one dimension) and a $16$ (the number of nodes in our tensor). Let's add it to our network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv): Conv2d(1, 4, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=2, stride=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv(x)))\n",
    "        x = x.view(-1,16)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're at a vector of 16 nodes. We need to get from 16 to 10 (the number of classes in our output vector. A **linear** layer with 16 input features and 10 output features should do the trick! We'll call it **fc** (again, for fully connected) just like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv): Conv2d(1, 4, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=2, stride=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(16,10)\n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        x = F.relu(self.pool(self.conv(x)))\n",
    "        x = x.view(-1,16)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what features our network learns from one forward pass of our 5x5 \"image.\" First we'll need to convert the image to a tensor using the **torch.tensor** method. Let's again call our sample input $X$. __requires_grad=False__ means that we don't want to calculate a **gradient** for $X$. $X$ is an input of constant pixel values, not a tensor of variable weights to update through backpropagation! Finally, __float()__ converts our input into the correct data type for doing tensor operations (though maybe not optimal precision for performance, but that's okay for now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 0.],\n",
       "        [1., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 1.],\n",
       "        [0., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor(a_zero_array, requires_grad=False).float()\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing before we feed our input tensor through the network. We need to add two additional dimensions: **batch_size** and **channels**. Both of these are just __1__: we have only one observation (our sample 5x5 image) and one channel (our 1-bit pixel value). In practice, we might be dealing with with RGB images or video, and we might be feeding multiple observations through the network at once (this is one way in which we take advantage of the parallel architecture of GPUs and other accelerators). \n",
    "\n",
    "We again use **x.view** to reshape our input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 1., 1., 0.],\n",
       "          [1., 0., 0., 0., 1.],\n",
       "          [1., 0., 0., 0., 1.],\n",
       "          [1., 0., 0., 0., 1.],\n",
       "          [0., 1., 1., 1., 0.]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.view(1,1,5,5)\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also gonna need to create a tensor for our label. PyTorch makes this easy too. Just define a tensor with \\[0\\]! Again, we'll want to turn off gradient updates - it's a label, not a parameter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([0], requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass our input through an instance of our neural network. We'll call our predicted output $Y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 1., 1., 0.],\n",
       "          [1., 0., 0., 0., 1.],\n",
       "          [1., 0., 0., 0., 1.],\n",
       "          [1., 0., 0., 0., 1.],\n",
       "          [0., 1., 1., 1., 0.]]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv): Conv2d(1, 4, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 1., 1., 1., 0.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [1., 0., 0., 0., 1.],\n",
      "          [0., 1., 1., 1., 0.]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4813,  0.0130,  0.1218, -0.2017, -0.2990, -0.3195, -0.0297,  0.1796,\n",
       "         -0.1663,  0.1432]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = net(X)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a vector of length 10! We haven't applied softmax yet, so it hasn't been normalized to a probability distribution yet, but you can see that the network thinks our 0 is an 8. We haven't updated weights and biases yet, so that's completely meaningless...\n",
    "\n",
    "All that's left to do is pass the raw output through a softmax function, compute loss, update weights, and repeat until satisfied! Since we still haven't even gotten to LeNet, we'll skim on the details (and the math). Here's a non-technical desciption of all three things:\n",
    "\n",
    "1) The softmax function converts our raw output to a probability vector by exponentiating each entry of the output vector (the input to our softmax function), $e^{i}$ for each value $i$, then dividing each $e^{i}$ by the sum of all exponentiated outputs.\n",
    "\n",
    "2) For classification, our loss is simply the negative natural logarithm of the predicted probability for the class corresponding to our ground truth. For instance, if we have a \"hot dog\" / \"not hot dog\" classifier, and our algorithm (correctly) predicts \"hot dog\" with probability 0.8, our loss is $-ln(0.8) = 0.22$. Note that our model still has a positive loss since the probability of \"hot dog\" is 0.8 (and thus \"not hot dog\" is 0.2), even though the \"ground truth\" is \"hot dog\" with probability 1 and \"not hot dog\" with probability 0. Assuming we are 100% confident in our observation... Make sense?\n",
    "\n",
    "3) We update weights and biases by taking the derivative of our loss function with respect to each, scaling by our learning rate, and subtracting from our current values.\n",
    "\n",
    "In PyTorch, we compute our softmax and calculate our loss in one go. **SGD** means that we are  applying **stochastic gradient descent**. This is the process described three sentences ago in __3)__ but by evaluating the gradient on randomly sampling observations. Of course this doesn't matter since we only have one observations but best practices yadda yadda. We apply this to our weights and biases (**net.parameters**) with a __learning rate__ of 0.1. The learning rate is the constant by which we scale our gradient updates.\n",
    "\n",
    "**nn.CrossEntropyLoss** is the negative log loss described in __2)__, and the **backward()** method automatically updates our parameters using the computed loss. It really is magic!\n",
    "\n",
    "Let's make sure to import the **optim** package first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate our loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "loss = ce_loss(Y, y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's update our weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So we've built a simple classifier network and \"trained\" it on our toy example. How about we use PyTorch to implement some \"real\" convolutional neural networks?\n",
    "\n",
    "Let's start with LeNet-5 (LeCun et al. 1998). Geoff Hinton is the Godfather of Deep Learning, but Yann LeCun is perhaps the underboss in the world of convolutional nets (Alex Krizhevsky is the consigliere?) In 1989, LeCun and his coauthors developed a convolutional neural network trained with backpropagation to recognize handwritten digits. This network was used to classify zip codes using data provided by the U.S. Postal Service. Nine years later, LeCun, along with Leon Bottou, Yoshua Bengio, and Patrick Haffner developed LeNet-5. Commercial banks used LeNet-5 to recognize handwritten digits on checks.\n",
    "\n",
    "LeNet-5 has (as you may have guessed) five layers, though I'm not sure if the \"5\" refers to the number of layers or maybe it was the fifth version of LeNet? There are more layers if you count pooling and activation as layers rather than \"things in the forward method that are applied to the layers defined in the constructor.\"\n",
    "\n",
    "Let's start with our template. We'll rename it LeNet, which now means we're plagiarizing unless I cite this: https://github.com/kuangliu/pytorch-cifar/blob/master/models/lenet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet()\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        # layers go here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # pooling and activations go here\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of LeNet is simple:\n",
    "\n",
    "1) A convolutional layer that takes 1 input channel for monochrome images, as is the case with our example, or 3 input channels for color images, and applies 6 5x5 convolutional filters with a stride of 1.\n",
    "\n",
    "2) Another convolutional layer that takes 6 input channels (the output of the first convolutional layer) and applies 16 convolutional filters with a stride of 1.\n",
    "\n",
    "3) A fully-connected layer that takes 400 input channels (the 16 filters of width 5 and height 5 from the prior convolutional layer) as input and applies a linear transformation to generate 120 output channels.\n",
    "\n",
    "4) A fully-connected layer that takes 120 input channels (the output of the prior fully-connected layer) and applies a linear transformation to generate 84 output channels (where did these numbers come from?)\n",
    "\n",
    "5) A fully-connected layer that takes 84 input channels (the output of the prior fully-connected layer) and applies a linear transformation to generate 10 output channels (corresponding to the number of classes).\n",
    "\n",
    "Let's add this to our network! We don't need to state our argument names. For instance, **self.conv1 = nn.Conv2d(1,6,5)** will do the trick instead of what we have below (stride defaults to 1). However, let's keep it explicit for now so that we understand what our network is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)   \n",
    "        return x\n",
    "    \n",
    "net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add activation and pooling functions! For LeNet-5, we apply a ReLU activation to every layer except for **fc3**, which we are already going to pass through our **softmax** nonlinearity (to generate a probability vector for the purposes of calculating loss and updating weights and biases).\n",
    "\n",
    "We also apply max pooling (2x2 region) to the output of our convolutional layers **after** passing that output through our ReLU activation. One thought (or actual!) experiment you might want to try is swapping pooling and ReLU. Does this change the output of our network? Does this generate a more accurate model? What about the time it takes to train the network?\n",
    "\n",
    "Here's LeNet-5, with ReLU and max pooling added. This is all we need to define our network architecture (convolutional, fully-connected, and max pooling layers) and forward propagation (activation) modules. We can drop **self.pool** anywhere we'd like among the layers in our constructor since the sequence of forward propagation is defined in the subsequent __forward__ module.\n",
    "\n",
    "We also need to make sure that we flatten the output after applying our second pooling operation, since our linear layers will expect a (one-dimensional) vector as input rather than a three-dimensional tensor with width and height (the dimensions of the feature maps learned from applying convolutional and pooling operations) and depth (the number of feature maps learned). Again, we do this using __x.view(-1, 16 * 5 * 5)__, where **-1** tells the little man (woman?) inside of PyTorch to flatten the input, and __16 * 5 * 5__ is the size of our input: 16 5x5 feature maps.\n",
    "\n",
    "Let's also define our convolutional and fully-connected layers without explicitly specifying parameter names. Remember, we don't need to specify our stride if it is equal to our default value of 1. We'll keep our pooling parameters explicit for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)   \n",
    "        return x\n",
    "    \n",
    "net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll soon find out (if you haven't already) that there are many ways to define the same network. Instead of using **nn.MaxPool2d** and defining a pooling layer inside of our constructor (__init__), we can use **F.max_pool2d** and define pooling in our __forward__ method. The latter approach is used in \n",
    "https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py and in https://github.com/kuangliu/pytorch-cifar/blob/master/models/lenet.py. Note that the first of these two examples can be understood as a modified LeNet with 3x3 rather than 5x5 convolutions. It also takes a single input channel rather than three. LeNet-5 has (likely) been used primarily on single-channel inputs, since handwritten digits tend to be monochrome (unless someone is getting creative with their cheques...)\n",
    "\n",
    "Note also that pooling stride defaults to kernel_size. Thus we can just define 2x2 pooling on with stride of 2 as **F.max_pool2d(x, 2)**. You can see this in the source code: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/pooling.py\n",
    "\n",
    "Here's how LeNet-5 looks when we swap in **F.max_pool2d** for __nn.MaxPool2d__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)   \n",
    "        return x\n",
    "    \n",
    "net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going forward we'll use **F.max_pool2d** for two-dimensional max pooling. I think it's a bit cleaner.\n",
    "\n",
    "If we try to pass our toy image through LeNet, we'll get a dimensional mismatch. We simply can't apply multiple 5x5 kernels and 2x2 pooling operations to an input image that's only 5x5 to begin with... Instead, LeNet wants a 32x32 one-channel input. We can also edit the network to take three-channel, e.g. RGB (red-green-blue) color images, but let's stick with monochrome.\n",
    "\n",
    "Let's generate a random 32x32 image. We can stick with one-bit pixels. It doesn't really matter. **torch.randint** will get the job done. One of the cool things is that you can use pretty much everything from NumPy (including __numpy.random.randint__) in PyTorch. We pass the parameters **(2, size=(32,32))** to specify that we want to sample integers between $0$ and $2$ (exclusive) and output the results in a $32x32$ array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32])\n",
      "tensor([[1, 0, 0,  ..., 0, 1, 0],\n",
      "        [0, 0, 0,  ..., 0, 1, 0],\n",
      "        [0, 0, 1,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [0, 1, 1,  ..., 0, 1, 0],\n",
      "        [0, 0, 1,  ..., 1, 1, 1],\n",
      "        [1, 0, 0,  ..., 1, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randint(2, size=(32,32))\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did with our 5x5 image, we need to reshape $X$ to include dimensions for **batch_size** and __channels__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n",
      "tensor([[[[1, 0, 0,  ..., 0, 1, 0],\n",
      "          [0, 0, 0,  ..., 0, 1, 0],\n",
      "          [0, 0, 1,  ..., 1, 1, 0],\n",
      "          ...,\n",
      "          [0, 1, 1,  ..., 0, 1, 0],\n",
      "          [0, 0, 1,  ..., 1, 1, 1],\n",
      "          [1, 0, 0,  ..., 1, 0, 1]]]])\n"
     ]
    }
   ],
   "source": [
    "X = X.view(1,1,32,32)\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to cast (the values of?) $X$ as a float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0551, -0.0085, -0.0425, -0.0719, -0.1065, -0.0156,  0.0019, -0.0425,\n",
       "          0.0327,  0.0532]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = net(X)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute loss and gradient, update weights, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "loss = ce_loss(Y, y)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Now let's think about the complexity of our networks. We started with a super-simple linear transformation (with bias) of our 25 input pixels into 10 output classes. So we learned 25x10 + 10 = 250+10 = 260 parameters.\n",
    "\n",
    "Then we switched to a convolutional neural network. We learned 4 2x2 filters. By default in PyTorch, we also added one bias per convolutional filter. So our convolutions generated 4x2x2 + 4 = 16+4 = 20 parameters. With ReLu and max pooling, we're not actually learning any additional parameters, even though we obviously have to do some computation. So we'll exclude these operations from our parameter count. Finally, we got our linear layer, where we take our 16 inputs (4 x 2x2 filters) and learn 16x10 weights and 10 biases, or 16x10 + 10 = 170 additional parameters. One of the cool things is that we've actually *decreased* the number of parameters by adding a convolution before our linear layer. Another reason to use convolutions!\n",
    "\n",
    "Now let's take a look at LeNet-5, layer-by-layer:\n",
    "\n",
    "1) We learn 6 5x5 weights plus bias in our first convolutional layer for 6x5x5 + 6 = 156 total parameters\n",
    "\n",
    "2) We learn 16 5x5 weights plus bias in our second convolutional layer for 16x5x5 + 16 = 416 total parameters\n",
    "\n",
    "3) We learn 400x120 + 120 = 48,000 + 120 = 48,120 parameters in our first fully-connected layer\n",
    "\n",
    "4) We learn 120x84 + 84 = 10,080 + 84 = 10,164 parameters in our second fully-connected layer\n",
    "\n",
    "5) We learn 84x10 + 10 = 840 + 10 = 850 parameters in our third fully-connected layer\n",
    "\n",
    "In total, LeNet-5 learns 156 + 416 + 48,120 + 10,164 + 850 = 59,706 parameters. That's a lot more than our toy networks but we're just getting started..\n",
    "\n",
    "Breaking down our parameters:\n",
    "\n",
    "1) 98.68% are weights from our fully-connected layers\n",
    "\n",
    "2) 0.92% are weights from our convolutional kernels\n",
    "\n",
    "3) 0.36% are biases from our fully-connected layers\n",
    "\n",
    "4) 0.04% are biases added to our convolutional weights\n",
    "\n",
    "Judging from parameter count alone, this doesn't look like a convolutional neural network. But those are some powerful convolutions. They get the work done while learning far fewer parameters. We should also be aware of the fact that a single convolution is more computationally expensive than a simple matrix multiplication. We gotta stride each of those things across our source image and subsequently learned feature maps.\n",
    "\n",
    "LeNet-5 gets the job done with handwritten digits, as long as we're cool with only 5% the digits on our cheques getting misread, which is most of us, no? But what if we're trying to do more complex stuff like build a cat classifier with RGB images? We're gonna need a bigger network.\n",
    "\n",
    "In 2006, Fei-Fei Li of Princeton and UIUC (later Stanford) came up with the idea for ImageNet (http://image-net.org. I found out this information at https://en.wikipedia.org/wiki/ImageNet#History_of_the_database). Starting in 2010, the curators of ImageNet began hosting the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). The most well-known version of ILSVRC, used since the 2012 challenge uses 1,281,167 training images, each labeled as one of 1000 classes. Each research team uses this training set to generate a model which is then evaluated on a test set.\n",
    "\n",
    "When ILSVRC debuted in 2010, the dominant image classification competition was the PASCAL VOC (Visual Object Classes) challenge. ILSVRC 2010 was actually held as a \"sideshow\" in conjunction with PASCAL VOC 2012. In 2011, Xerox Research Centre Europe (XRCE) won the competition using a combination of Fisher vectors, product quantization, and support vector machine linear classifiers (learned through backpropagation! - http://image-net.org/challenges/LSVRC/2011/ilsvrc11.pdf) Don't worry what this stuff means. I'm not up on my Fisher vectors either.\n",
    "\n",
    "What's important is that XRCE's submission wasn't a neural network. Even eight years ago (writing in 2019), neural networks were simply too costly to train given available compute resources. If you think about it, fully-connected layers are just about the dumbest way to learn features. You take all your inputs and outputs and learn a linear connection between the two of them. Let's say we're doing classification on a single 256x256x3 image. ImageNet images come in all sizes, so it's necessary to resize these images (to a common size) prior to feeding them into a neural network (or classification algorithm more generally). 256x256 is a common shape, and we'll retain our 3 color channels. That's 65,536 pixels per channel, *already more than the total parameters of LeNet-5*, or 65,536x3 = 196,608 total input nodes. Yikes. What if we want to learn a *single* linear transformation (with bias, but that's computationally cheap) between this input and our 1,000-class output? That's 196,608 x 1,000 + 1,000 = 196,609,000 total parameters. **WHOA**.\n",
    "\n",
    "Lessons learned:\n",
    "\n",
    "1) We're gonna need a bigger network\n",
    "\n",
    "2) We're gonna need a more computationally efficient algorithm\n",
    "\n",
    "3) We're gonna need thousands of CPUs, or maybe two (2011-era) GPUs?\n",
    "\n",
    "3a) If we go the GPU route, we're going to need to implement our network in CUDA or OpenCL\n",
    "\n",
    "Fortunately, we don't need to worry about 3) or 3a). We're experimenting with a single image, and we'll just do a single forward and backward pass. We can do that on CPU. If we want to do this on GPU, PyTorch of course supports that.\n",
    "\n",
    "But in 2012 it wasn't so clear that (convolutional) neural networks were the way to go. You'll notice that there's a bit of a temporal gap between LeNet-5 (1998) and AlexNet (2012 - which we'll get to in just a moment).\n",
    "\n",
    "For the reasons we just examined, neural networks weren't the optimal solution to many (computer vision, artificial intelligence, machine learning) problems, given available data, compute, and algorithmic constraints. However:\n",
    "\n",
    "1) ImageNet solved the issue of insufficient data, at least for 1000-class image classification\n",
    "2) GPUs can do arithmetic (especially matrix multiplications) in parallel, a massive speedup over CPU arithmetic\n",
    "3) NVIDIA developed software for general purpose computing (CUDA) using GPUs, enabling researchers to take advantage of the parallel architecture of GPUs for computing (not just graphics)\n",
    "\n",
    "In the decades in which neural networks had fallen out of favor, Geoff Hinton and his researchers at the University of Toronto continued to develop novel algorithms. In 2012, his graduate student Alex Krizhevsky, in collaboration with Hinton and Ilya Sutskever submitted a convolutional neural network (nicknamed \"AlexNet\") to the ILSVRC competition and *absolutely destroyed* the field. You can see the results here: http://www.image-net.org/challenges/LSVRC/2012/results.html. They called their team \"SuperVision,\" ha.\n",
    "\n",
    "It's been a while since we've done some code, so let's get the template ready for AlexNet. Or SuperVision. Why don't people call it that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet()\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "\n",
    "        # layers go here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # pooling and activations go here\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net = AlexNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out AlexNet's architecture. For several reasons, one might expect AlexNet to be more complex than LeNet-5 (59,706 parameters). First off, the *number of input nodes alone* (196,608) is greater than the number of total parameters of LeNet-5. Of course, we don't need to learn connections from each of the input nodes, but if we want to exploit the additional detail of the images, we'll need a \"bigger\" network (in terms of parameters).\n",
    "\n",
    "If we're learning 1000 classes rather than 10, we'll also need to learn many more features to differentiate between these classes. To take a concrete example from ILSVRC 2012, think of the complexity of information necessary to distinguish an Angora rabbit from a wood rabbit (see https://gist.github.com/xkumiyu/dd200f3f51986888c9151df4f2a9ef30). Now think about what we need to differentiate a zero (one circle) from an eight (two vertically-stacked circles). We will most likely need to learn more (convolutional) feature( map)s.\n",
    "\n",
    "AlexNet is available as part of PyTorch's **torchvision** module of \"popular datasets, model architectures, and common image transformations for computer vision\" (see https://pytorch.org/docs/stable/torchvision/models.html#classification and https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py). When building convolutional neural networks, it's common to import torchvision along with pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that LeNet learned 6 5x5 kernels with stride of 1 in our first layer, then learned 16 with the same dimensions and stride in our second layer. If we do this on a 256x256x3 RGB image (rather than a 32x32 grayscale image), we're going to end up learning fairly large feature maps. AlexNet's first convolutional layer manages to downsample the source image(s) a bit more, learning 11x11 kernels with stride of 4. It turns out that this doesn't exactly match the dimensions of the input image (256x256x3), so we also have to add **padding**: a row and column of zeroes on each edge of the image. For our first layer, __padding=2__. We have 3 input channels (red, green, and blue) and we learn 64 output feature maps from our 11x11 kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=11, stride=4, padding=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # pooling and activations go here\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net = AlexNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also apply max pooling over a 3x3 region with stride of 2. This leads to greater downsampling relative to LeNet, which pools over a 2x2 region with stride of 2. However, we only pool after our first, second, and fifth convolutional layers. AlexNet has five convolutional layers. We apply ReLU after each convolutional layer (before pooling).\n",
    "\n",
    "What about our other convolutional layers?\n",
    "\n",
    "We already saw that **conv1** takes 3 input channels and applies an 11x11 kernel with stride of 4, padding of 2, and 64 output channels\n",
    "\n",
    "**conv2** takes 64 input channels (output of __conv1__) and applies a 5x5 kernel with stride of 1, padding of 2, and 192 output channels\n",
    "\n",
    "**conv3** takes 192 input channels (output of __conv2__) and applies a 3x3 kernel with stride of 1, padding of 1, and 384 output channels\n",
    "\n",
    "**conv4** takes 384 input channels (output of __conv3__) and applies a 3x3 kernel with stride of 1, padding of 1, and 256 output channels\n",
    "\n",
    "**conv5** takes 256 input channels (output of __conv4__) and applies a 3x3 kernel with stride of 1, padding of 1, and 256 output channels\n",
    "\n",
    "Let's add these layers to our network, along with our 3x3 pooling layers with stride of 2 and our ReLU activation functions. We can pass **in_channels** and __out_channels__ as our first two arguments without stating them explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  (conv2): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (conv4): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=2)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.conv1(x), kernel_size=3, stride=2)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(self.conv2(x), kernel_size=3, stride=2)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(self.conv3(x), kernel_size=3, stride=2)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "    \n",
    "net = AlexNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce another PyTorch API. Just as we can define our layers with **torch.nn** and our pooling and activations with __torch.nn.functional__, we can also \"stack\" our layers (including pooling and activation functions) using **torch.nn.Sequential**. This allows us to define multiple methods of our __self__ class that we can then call during **forward()**. We separate each \"layer\" (convolution, pooling, and activation) with a comma. Note also that we are using __nn.ReLU__ and **nn.MaxPool2d** rather than __F.relu__ and **F.max_pool2d** (note the difference in casing).\n",
    "\n",
    "__torchvision__ uses **nn.Sequential** to define AlexNet (https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):  # length of our output vector = num_classes\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(  # we pass our input through these layers (features)\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2), # no need to specify \"self\"\n",
    "            nn.ReLU(inplace=True), # a neat trick to decrease memory usage [0]\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # include trailing comma\n",
    "        )\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "    \n",
    "net = AlexNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[0\\] See https://discuss.pytorch.org/t/whats-the-difference-between-nn-relu-and-nn-relu-inplace-true/948\n",
    "\n",
    "The output of **print(net)** is slightly prettier, I think! Of course we still need to add our fully-connected layers. AlexNet has three of these. But first we do something called __Adaptive Average Pooling (AdaptiveAvgPool2d)__ in PyTorch. This applies average pooling to our input *given a specified output*. Average pooling sums up over the pooling region and divides by the number of observations in the region. It's more precise than max pooling but it costs slightly more to compute.\n",
    "\n",
    "In other words, rather than applying a specified kernel size and stride to the input, it applies pooling based on the *target size* of the output (in this case 6x6). See https://discuss.pytorch.org/t/what-is-adaptiveavgpool2d/26897. We define a method **self.avgpool** and pass our input to the method in __forward__\n",
    "\n",
    "What about our fully-connected layers? The **vision** implementation of AlexNet defines these layers in a separate __classifier__ method. We apply **classifier** to the reshaped output of the adaptive average pooling of the output of our convolutional, ReLU, and pooling operations. That's a mouthful, but hopefully it's starting to make sense, and will make more sense once we see it in code.\n",
    "\n",
    "Recall that we learn 256 kernels in our final convolutional layer. We then apply adaptive average pooling so that each kernel is now of dimensions 6x6. These flattened parameters are thus the input to our first **nn.Linear** (fully-connected) layer:\n",
    "\n",
    "The first **nn.Linear** takes a 256x6x6 input and generates an output of size 4096\n",
    "\n",
    "The second **nn.Linear** takes an input of size 4096 and generate an output of equal size\n",
    "\n",
    "Finally, the third **nn.Linear** takes an input of size 4096 and generates an output of size equal to the number of classes defined in __num_classes__. For ImageNet, **num_classes=1000**. This is the default value in __vision/torchvision/models/alexnet.py__\n",
    "\n",
    "We also need to make sure that we layer some activation functions between our **nn.Linear**s. Remember it's necessary to apply activations between each pair of layers in order to learn nonlinear features.\n",
    "\n",
    "One more thing! We add something called **dropout** to our __classifier__ of fully-connected layers. Dropout randomly sets some of our parameters to zero to prevent our network from **overfitting**. If we think of our data as sampled from a true distribution with error, overfitting occurs when we learn the error rather than the true distribution from the data. If we apply an \"overfit\" model to new data sampled from our true distribution, it will be biased. See https://en.wikipedia.org/wiki/Overfitting for more on overfitting, and http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf for the paper on dropout (Hinton, Krizhevsky, and Sutskever again! - along with two other collaborators from Toronto).\n",
    "\n",
    "We add dropout using **nn.Dropout()** before our first and second linear layers. By default, __nn.Dropout()__ keeps parameters with probability 0.5 and sets the remainder to zero.\n",
    "\n",
    "Let's take a look at our network with the **avgpool** and __classifier__ methods added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):  # length of our output vector = num_classes\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(  # we pass our input through these layers (features)\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2), # no need to specify \"self\"\n",
    "            nn.ReLU(inplace=True), # a neat trick to decrease memory usage [0]\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # include trailing comma\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)  # flattens our set of 6x6 kernels\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "net = AlexNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! Let's generate a fake 256x256x3 image and feed it through AlexNet. We'll make the pixel values binary again. As an exercise, you could generate your own synthetic image with 8-bit pixels instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n",
      "tensor([[[0, 1, 0,  ..., 0, 1, 1],\n",
      "         [1, 0, 1,  ..., 0, 1, 0],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 1, 0],\n",
      "         [1, 0, 0,  ..., 0, 1, 1]],\n",
      "\n",
      "        [[1, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [1, 1, 0,  ..., 0, 1, 1],\n",
      "         ...,\n",
      "         [1, 0, 1,  ..., 0, 1, 1],\n",
      "         [0, 1, 1,  ..., 1, 1, 0],\n",
      "         [1, 0, 0,  ..., 0, 1, 0]],\n",
      "\n",
      "        [[0, 1, 1,  ..., 1, 0, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 0,  ..., 1, 0, 1],\n",
      "         ...,\n",
      "         [0, 0, 1,  ..., 1, 1, 0],\n",
      "         [0, 1, 0,  ..., 1, 1, 0],\n",
      "         [0, 1, 0,  ..., 1, 1, 0]]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randint(2, size=(3,256,256))\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a fourth dimension for **batch_size**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "tensor([[[[0, 1, 0,  ..., 0, 1, 1],\n",
      "          [1, 0, 1,  ..., 0, 1, 0],\n",
      "          [0, 0, 1,  ..., 1, 1, 1],\n",
      "          ...,\n",
      "          [0, 0, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 1, 0],\n",
      "          [1, 0, 0,  ..., 0, 1, 1]],\n",
      "\n",
      "         [[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [0, 0, 0,  ..., 1, 1, 1],\n",
      "          [1, 1, 0,  ..., 0, 1, 1],\n",
      "          ...,\n",
      "          [1, 0, 1,  ..., 0, 1, 1],\n",
      "          [0, 1, 1,  ..., 1, 1, 0],\n",
      "          [1, 0, 0,  ..., 0, 1, 0]],\n",
      "\n",
      "         [[0, 1, 1,  ..., 1, 0, 1],\n",
      "          [1, 1, 1,  ..., 1, 1, 1],\n",
      "          [1, 1, 0,  ..., 1, 0, 1],\n",
      "          ...,\n",
      "          [0, 0, 1,  ..., 1, 1, 0],\n",
      "          [0, 1, 0,  ..., 1, 1, 0],\n",
      "          [0, 1, 0,  ..., 1, 1, 0]]]])\n"
     ]
    }
   ],
   "source": [
    "X = X.view(1,3,256,256)\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and cast our synthetic \"image\" as a float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've already set **net** equal to an instance __AlexNet()__ of AlexNet, we can pass our image **X** through __net__ \n",
    "\n",
    "We'll call our output **Y** again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6008e-03, -1.1606e-02, -3.6402e-03, -1.2425e-02,  4.6868e-03,\n",
       "         -2.0302e-02,  1.9750e-02,  1.6332e-03,  2.1267e-02,  7.0487e-03,\n",
       "         -3.8987e-03,  3.3410e-03, -8.5150e-03,  1.8240e-02,  1.1947e-02,\n",
       "          7.6247e-03, -9.0806e-04,  1.4194e-03,  5.7656e-03,  3.9215e-03,\n",
       "         -1.1260e-02,  7.1236e-03, -5.4486e-03,  1.1531e-02, -3.0117e-03,\n",
       "         -7.3532e-04, -6.9984e-03, -1.0789e-02, -1.7334e-02,  1.6429e-02,\n",
       "          1.5632e-02,  5.6069e-03,  3.9309e-03,  1.2510e-03,  5.0644e-03,\n",
       "          8.0094e-03, -1.5284e-02,  9.1914e-04, -1.2278e-03, -9.5449e-03,\n",
       "          1.0507e-02,  6.3436e-03, -8.1913e-03, -9.7031e-03, -1.1655e-02,\n",
       "          2.4470e-02,  3.1370e-03, -8.1217e-03,  8.9174e-03, -1.6260e-02,\n",
       "         -5.5201e-03, -6.8232e-03, -6.6632e-04, -5.6171e-03, -7.9334e-04,\n",
       "          9.3163e-03, -3.5516e-03,  2.2007e-02, -2.3209e-02, -4.7882e-03,\n",
       "         -1.3573e-02,  4.2442e-03,  6.9117e-03,  6.2985e-03, -1.0943e-02,\n",
       "          1.1973e-02, -1.3436e-02,  7.9482e-03, -1.1060e-02,  9.5180e-04,\n",
       "         -1.2923e-02, -8.6794e-03,  2.2849e-02,  5.9486e-03,  5.1517e-03,\n",
       "         -2.3729e-03,  1.8170e-02, -1.3128e-02, -7.3608e-03, -5.0482e-04,\n",
       "          6.5635e-03, -1.7370e-02,  4.7608e-03,  2.2261e-02,  1.0059e-02,\n",
       "          1.5514e-02, -8.4786e-03,  8.7798e-04,  1.2235e-02, -2.0716e-03,\n",
       "         -1.0789e-02, -5.3023e-03,  6.7528e-03,  9.1688e-03, -1.9462e-03,\n",
       "          8.7997e-03,  1.5488e-02,  1.7992e-02, -1.7040e-02, -2.0140e-03,\n",
       "         -2.2249e-02,  1.7099e-02,  2.5905e-03, -1.9471e-02, -1.8107e-02,\n",
       "         -8.2931e-03, -1.1930e-02, -1.7139e-02,  1.1105e-02, -1.2819e-02,\n",
       "         -5.7733e-04, -2.6515e-02,  1.4564e-02, -9.5051e-03, -3.6427e-03,\n",
       "         -1.2579e-02,  1.0115e-02,  8.4290e-03, -7.4198e-03,  2.0970e-02,\n",
       "         -3.1386e-03, -1.2779e-02, -9.0145e-03,  3.6221e-03,  1.2713e-02,\n",
       "          1.7604e-02, -1.6871e-02, -1.5193e-02,  1.1336e-02, -1.3103e-02,\n",
       "         -1.1230e-02, -1.0224e-02,  1.5591e-03,  1.6479e-02, -1.5862e-02,\n",
       "         -2.9945e-03,  8.4440e-03,  6.0372e-03, -4.5623e-04, -1.4851e-02,\n",
       "          1.5698e-02,  8.9513e-04,  8.0239e-03,  7.9369e-03,  6.4205e-04,\n",
       "          8.6600e-03,  1.3792e-02,  6.3732e-03,  5.4060e-03, -1.1584e-03,\n",
       "          3.7482e-03,  9.2627e-03, -9.4785e-03, -1.0756e-03,  1.3678e-03,\n",
       "         -2.1493e-02, -1.1814e-02, -2.1445e-02,  1.5556e-02, -4.1190e-03,\n",
       "         -7.6081e-03, -6.6881e-03,  8.8395e-03,  1.4010e-02,  9.2269e-03,\n",
       "          3.9818e-03, -2.6327e-03,  9.7845e-04, -2.7213e-03, -7.6542e-03,\n",
       "         -1.2035e-02,  1.7046e-03,  2.1648e-03, -1.3777e-02, -9.6291e-03,\n",
       "         -1.4668e-02, -4.1889e-03,  8.9622e-04, -8.0938e-03,  1.2425e-03,\n",
       "         -1.4683e-02,  2.7596e-03, -1.6742e-02,  1.2621e-02,  7.8003e-03,\n",
       "          1.2941e-02,  3.4779e-03, -3.2821e-03,  3.2446e-03,  2.8913e-03,\n",
       "         -4.2916e-03, -8.1173e-03, -1.5729e-03,  1.0205e-02, -5.5878e-04,\n",
       "         -1.2124e-03, -9.1822e-03,  1.0658e-02, -2.4794e-02, -1.7300e-03,\n",
       "         -4.3516e-03,  6.5132e-06,  1.6534e-03, -8.6293e-03, -7.0989e-03,\n",
       "         -3.5607e-03, -1.7485e-03, -3.2181e-03, -3.8802e-03,  6.5174e-04,\n",
       "         -1.1011e-02, -6.9056e-03, -9.1397e-03, -1.4478e-03, -4.9209e-03,\n",
       "          8.2105e-03,  1.2228e-02, -2.0490e-03, -1.8368e-03, -1.7807e-02,\n",
       "          6.2070e-03, -1.3102e-02, -8.6702e-03, -1.1257e-02,  1.2634e-02,\n",
       "         -9.6722e-03, -1.8775e-02,  9.0732e-03, -6.4425e-03, -5.1835e-03,\n",
       "         -8.2111e-03,  4.1501e-03,  8.9463e-03, -2.9582e-03,  5.0975e-03,\n",
       "         -7.8989e-03,  1.0448e-02,  1.7303e-02,  7.9067e-03,  2.8230e-03,\n",
       "          3.8704e-04, -2.3384e-02,  9.4020e-03,  1.1153e-02,  2.1233e-02,\n",
       "         -1.7617e-02,  9.1139e-03, -3.3677e-03, -1.5018e-02, -3.5509e-03,\n",
       "         -3.6814e-03, -2.8843e-03, -1.4056e-02, -1.2463e-02, -1.4342e-02,\n",
       "          3.9635e-03, -1.1821e-02,  7.2991e-03, -2.2045e-02,  1.2718e-02,\n",
       "         -1.4654e-02, -1.2761e-02, -8.2108e-03, -7.9001e-03,  1.9554e-02,\n",
       "         -1.7960e-02, -1.8211e-02, -1.3715e-02, -4.4914e-03,  1.4903e-02,\n",
       "         -5.3360e-03, -7.9189e-03, -4.1325e-03,  1.3209e-03, -7.8230e-04,\n",
       "          1.2342e-02,  9.5638e-03, -9.7314e-03,  1.0053e-03, -1.5385e-02,\n",
       "         -6.7914e-03, -9.0258e-03, -6.3658e-03, -8.6901e-03,  4.8075e-03,\n",
       "         -1.7070e-02,  9.5796e-03, -7.9152e-04,  7.8428e-03,  1.3508e-02,\n",
       "         -2.8080e-03, -1.3444e-02, -1.6666e-02,  2.2692e-02, -6.8827e-04,\n",
       "         -5.8202e-04, -1.8771e-02, -7.9222e-05, -4.2937e-03, -7.7442e-03,\n",
       "         -1.3528e-02, -1.6651e-02, -2.1664e-03,  1.1530e-02, -1.3546e-04,\n",
       "         -5.2342e-03, -7.2557e-03,  9.7548e-04,  7.2640e-03,  1.0266e-02,\n",
       "         -3.7321e-03,  4.4038e-03,  1.3448e-02, -9.6127e-03,  1.3919e-02,\n",
       "          1.4903e-02,  2.5220e-03,  7.4654e-03, -3.1660e-03, -3.9017e-03,\n",
       "          1.4016e-02,  9.7280e-03, -2.6535e-03,  8.1942e-03, -1.9615e-02,\n",
       "          5.4861e-03, -2.3836e-02, -4.2639e-03,  1.1030e-02, -1.4109e-02,\n",
       "          2.3759e-03,  5.4812e-03,  1.5260e-02,  6.4326e-03,  1.9010e-02,\n",
       "          9.7704e-03,  2.1042e-02, -9.3967e-03,  3.8451e-04, -1.6791e-02,\n",
       "          8.9402e-03,  9.3195e-04,  2.0413e-02,  1.2965e-02,  1.3329e-03,\n",
       "          1.5246e-02, -5.4700e-03, -1.4608e-02,  1.0278e-02, -4.6298e-03,\n",
       "          1.3649e-03,  1.6367e-02,  4.3032e-03, -1.0529e-02,  1.5577e-02,\n",
       "          4.7048e-03,  1.6903e-02, -2.7823e-03, -1.2094e-02,  1.8946e-03,\n",
       "         -9.3852e-03, -5.8840e-03, -2.2392e-02,  2.2256e-03, -8.1963e-03,\n",
       "         -5.0319e-03,  1.7286e-02, -9.4638e-04, -1.9024e-02, -3.1683e-03,\n",
       "          9.3851e-03,  1.7093e-02, -1.0158e-02, -9.4797e-03, -7.0157e-03,\n",
       "         -8.8847e-03, -1.1246e-03,  8.0371e-03, -1.2531e-03, -1.0105e-02,\n",
       "          1.4263e-02,  1.6334e-03, -7.5039e-03, -1.0529e-02,  2.4109e-02,\n",
       "          6.2299e-03, -1.3650e-02, -2.2415e-02, -4.6108e-03, -7.8761e-03,\n",
       "         -4.8613e-03, -1.6551e-02,  1.7335e-02, -1.0882e-03,  7.5188e-03,\n",
       "          1.1550e-02, -1.9406e-03,  1.2326e-02,  1.8808e-02,  1.1650e-02,\n",
       "         -3.9228e-03,  1.5416e-02,  1.4687e-02, -1.4491e-03,  8.4358e-03,\n",
       "         -8.1405e-03, -1.4380e-02,  4.7011e-03,  1.2283e-02, -6.1790e-03,\n",
       "         -1.6925e-03, -1.4393e-02, -8.3954e-03,  1.8067e-02,  1.0726e-02,\n",
       "          1.4773e-02,  2.3596e-04,  2.5924e-03,  1.1365e-04, -1.4040e-02,\n",
       "          1.0030e-02, -1.1259e-02,  1.8413e-02,  1.0651e-02,  8.8382e-03,\n",
       "         -5.7141e-03, -5.6234e-04, -3.2275e-03,  5.8190e-04,  1.2439e-02,\n",
       "         -1.4806e-02,  1.5528e-03,  6.4130e-03, -6.3086e-03,  3.7567e-03,\n",
       "         -1.8248e-02, -1.1246e-02, -2.1592e-03,  2.0008e-03,  4.7959e-03,\n",
       "         -2.4977e-02, -2.7199e-03,  2.2801e-02,  4.4255e-03,  4.9067e-03,\n",
       "          1.3166e-02, -1.7540e-02,  7.8487e-03, -1.8644e-02, -2.9271e-03,\n",
       "          9.5005e-04, -3.1536e-03,  1.9320e-03,  1.8234e-02, -2.5889e-02,\n",
       "         -1.2275e-03,  1.6587e-02,  5.8008e-03, -7.6253e-03, -1.1207e-02,\n",
       "          4.1830e-03, -1.3568e-02, -1.3329e-02,  7.8487e-03, -3.7086e-03,\n",
       "         -8.8396e-03, -5.7623e-04, -1.2642e-03, -1.5820e-02,  4.3203e-03,\n",
       "         -3.1363e-03, -1.7392e-02,  4.4541e-03,  9.8295e-03,  1.0348e-02,\n",
       "         -3.8641e-03, -2.4426e-03, -3.7523e-03,  1.1021e-02,  2.7755e-03,\n",
       "          2.3481e-03, -1.6478e-02, -1.4456e-02, -8.8989e-03, -1.2552e-02,\n",
       "          4.1978e-03,  1.0152e-02,  1.0895e-03, -9.0126e-03,  1.9761e-02,\n",
       "         -8.4229e-03,  3.1664e-03,  1.9021e-02, -2.6132e-02, -2.0247e-02,\n",
       "         -2.3634e-03, -7.1483e-03,  4.5939e-03, -1.0004e-02,  4.9063e-03,\n",
       "          1.6961e-03, -9.8055e-03, -3.1055e-02,  2.8633e-02, -1.1990e-04,\n",
       "          2.3641e-02,  1.0531e-02,  5.6845e-03, -7.0098e-03, -2.3752e-03,\n",
       "          1.8979e-04,  1.0605e-02,  2.8442e-02,  1.1942e-02, -6.3197e-03,\n",
       "         -3.0050e-03,  2.8178e-03,  3.9900e-03,  3.4811e-04,  1.5500e-02,\n",
       "         -7.9773e-03, -2.2984e-04,  1.0056e-02,  1.4193e-03,  1.2048e-02,\n",
       "         -6.0666e-03, -4.9777e-03,  2.3858e-02, -9.8000e-03, -7.0158e-03,\n",
       "          1.7077e-02, -4.3049e-03, -2.2908e-02, -1.9001e-02, -1.6729e-02,\n",
       "          1.2865e-02, -2.1358e-02,  1.2437e-02, -1.4265e-02,  2.4784e-03,\n",
       "         -1.3203e-02, -6.1479e-04, -9.4892e-03,  8.3943e-03,  6.4207e-03,\n",
       "         -2.0371e-03, -1.7517e-03, -9.1780e-03,  2.5883e-03, -8.4650e-03,\n",
       "          1.7273e-02, -1.2592e-02, -7.5111e-03, -5.0240e-03, -1.3585e-02,\n",
       "         -1.2818e-02, -1.6245e-02,  3.8514e-03,  1.0042e-02, -4.1706e-03,\n",
       "         -7.3769e-03, -1.6845e-02,  1.5929e-02, -2.3806e-02,  2.1133e-02,\n",
       "         -8.4495e-03, -1.7683e-03,  9.8885e-03,  5.9851e-03,  4.2995e-03,\n",
       "          9.0471e-03,  1.6599e-02,  2.4016e-02,  1.0072e-02, -2.2734e-03,\n",
       "         -3.7317e-03, -9.9966e-03,  1.0428e-02, -2.0471e-03, -4.0841e-03,\n",
       "         -5.1441e-03, -4.4670e-03, -9.8122e-03,  1.1699e-02,  8.5471e-03,\n",
       "         -1.1612e-02,  1.4450e-02,  2.8030e-03,  1.4364e-02, -9.4764e-03,\n",
       "         -1.2184e-02,  3.7935e-03, -2.0552e-02, -5.5865e-04,  2.2497e-02,\n",
       "         -9.6578e-03, -4.9446e-03,  3.1565e-03,  9.2830e-03,  7.8560e-03,\n",
       "         -8.7270e-03, -4.4463e-03, -9.1738e-03, -1.5937e-02, -7.8128e-03,\n",
       "          3.1025e-05, -6.0556e-03, -3.7705e-03,  1.0508e-02,  1.2866e-02,\n",
       "          1.3665e-03,  1.1469e-03, -1.0052e-02, -1.5991e-02, -1.9069e-02,\n",
       "          7.5571e-03,  1.2431e-03,  4.7470e-03,  7.6095e-03,  1.1834e-02,\n",
       "          1.4401e-02, -4.3302e-03, -9.4581e-03, -1.0116e-02, -1.5875e-03,\n",
       "         -9.5914e-03, -1.2179e-03, -1.2441e-02,  5.2436e-03,  5.4466e-03,\n",
       "         -5.9089e-03,  1.5153e-02,  1.8385e-03,  8.9573e-03, -1.4409e-03,\n",
       "          2.5240e-02,  2.1626e-03,  1.6782e-02,  2.1907e-03, -1.3681e-02,\n",
       "          2.8112e-03,  1.6305e-03,  2.6510e-02, -4.3969e-03,  1.9510e-02,\n",
       "          5.4231e-03,  1.0974e-02,  1.7283e-02, -5.0340e-03, -1.5736e-02,\n",
       "          1.2271e-02, -7.9282e-03, -1.0579e-02,  7.0548e-03, -1.0271e-02,\n",
       "          2.1263e-02,  8.3949e-03, -1.3905e-02,  2.5227e-03, -2.3329e-02,\n",
       "          1.1681e-02,  3.2734e-03,  9.3818e-04, -1.1593e-02,  1.8158e-02,\n",
       "         -2.0403e-02, -8.3718e-03,  2.2116e-02, -5.6446e-03, -5.4763e-03,\n",
       "          7.8142e-03,  3.0327e-03,  3.6584e-03, -2.8542e-02,  1.5397e-02,\n",
       "          1.4703e-02,  1.8142e-02,  1.6265e-03,  2.4678e-03,  6.9624e-03,\n",
       "          3.3151e-03,  2.5172e-03, -1.2907e-02,  1.3279e-02, -6.8914e-03,\n",
       "         -1.6448e-02, -6.9878e-03, -1.1094e-02,  1.9373e-02,  5.6330e-04,\n",
       "          5.8988e-03, -6.1059e-03,  9.5392e-03, -1.4357e-02, -2.0157e-03,\n",
       "         -6.4177e-03,  1.3905e-02,  6.8796e-03, -7.8878e-03,  7.2153e-03,\n",
       "         -2.3690e-03,  6.3723e-03, -6.2417e-03, -9.2658e-03,  8.4116e-03,\n",
       "         -1.2371e-02,  2.5276e-03,  1.5821e-04,  4.9741e-03, -2.7086e-02,\n",
       "         -8.2380e-03,  1.2381e-02,  3.7411e-03,  9.9616e-03,  1.5364e-03,\n",
       "          1.8409e-02, -2.8698e-03, -3.8999e-03, -1.9727e-02,  1.4340e-02,\n",
       "         -5.5100e-03,  2.0217e-02,  8.1338e-03, -8.0278e-03, -1.0100e-02,\n",
       "          3.9344e-03,  1.1015e-02, -1.7363e-02,  1.3232e-03,  4.7837e-03,\n",
       "         -1.3202e-02, -3.5458e-03, -1.3759e-02,  1.7535e-02, -4.1810e-03,\n",
       "          1.3635e-02, -1.3738e-02,  1.9122e-03, -6.3337e-03,  1.0222e-02,\n",
       "          8.1258e-03, -7.6235e-03, -1.4911e-03, -1.3544e-02, -2.4820e-03,\n",
       "          1.0537e-02,  1.3688e-02,  1.2033e-02,  1.1759e-02, -8.0973e-03,\n",
       "         -2.3093e-04, -7.9717e-04,  1.2141e-03, -9.5590e-03,  2.1881e-02,\n",
       "         -8.8364e-05,  1.1640e-02,  1.5732e-03, -1.6066e-03, -1.5421e-02,\n",
       "         -1.4567e-02, -9.4729e-03,  9.3581e-03, -1.3939e-02, -6.5181e-03,\n",
       "         -1.5961e-02,  9.9691e-03, -1.6402e-02,  8.2089e-03,  3.8878e-03,\n",
       "          5.6754e-04, -2.0029e-02,  2.3708e-03, -1.1932e-02, -1.2217e-02,\n",
       "         -6.6471e-04, -4.2772e-03, -1.0864e-02,  3.9474e-03,  1.3331e-02,\n",
       "         -1.3436e-02,  1.8272e-02,  4.5794e-03,  5.9772e-03, -4.5752e-03,\n",
       "         -2.1271e-02,  1.9713e-02,  2.0286e-03,  2.9488e-02,  1.4461e-02,\n",
       "         -1.7475e-02, -2.3563e-03, -1.6229e-02, -2.4638e-03,  1.1688e-02,\n",
       "          9.0183e-03,  9.3317e-03,  5.1780e-03,  1.9399e-02, -1.9032e-02,\n",
       "          2.2740e-02, -3.0651e-03, -2.2141e-03, -5.6673e-03, -4.5058e-03,\n",
       "          8.5043e-03, -6.4524e-03,  1.2012e-03,  2.5470e-04,  1.2401e-03,\n",
       "          1.1881e-02,  2.3451e-03,  1.7006e-02,  1.2868e-02,  1.4908e-02,\n",
       "         -4.7436e-03, -1.1752e-03,  1.2038e-02,  1.2137e-02, -2.0076e-02,\n",
       "         -1.0428e-02, -9.1359e-03, -9.4188e-03,  1.3345e-02,  3.3098e-02,\n",
       "         -1.7823e-02, -2.0824e-02, -1.7472e-03, -2.8923e-03,  1.5946e-02,\n",
       "         -2.9711e-03, -5.5586e-04,  3.7831e-03,  4.2384e-03, -1.3957e-02,\n",
       "          5.5302e-03,  6.3243e-03, -1.1316e-02,  1.0657e-03,  1.7730e-03,\n",
       "         -1.1710e-02,  1.4033e-02,  5.3658e-03, -1.5048e-02, -8.7135e-03,\n",
       "         -6.5910e-03, -1.0799e-02,  1.1574e-02,  2.1447e-03,  1.1304e-02,\n",
       "          1.5532e-03, -6.6195e-03,  1.2357e-02, -1.2480e-02,  1.2121e-03,\n",
       "          2.1185e-03,  1.4145e-02, -1.6569e-02,  4.5876e-04, -4.9589e-03,\n",
       "         -3.0302e-03,  6.3109e-03,  5.7729e-03, -2.4853e-02,  2.6940e-02,\n",
       "          1.2379e-02, -1.3673e-02,  5.7128e-03, -4.7836e-03,  9.6563e-03,\n",
       "          2.2029e-02,  8.1937e-03,  1.9049e-03,  1.1014e-02,  2.1226e-02,\n",
       "         -6.1683e-04,  1.4643e-03, -2.1950e-02, -1.2618e-02, -1.3344e-02,\n",
       "          1.4249e-03,  2.0414e-02,  1.0448e-02,  8.3075e-03, -1.1024e-02,\n",
       "         -7.0559e-03,  1.2676e-02,  2.2698e-03,  8.5996e-03, -1.2912e-02,\n",
       "         -7.6514e-04, -1.4872e-02,  1.0587e-02,  1.4495e-02,  4.9603e-03,\n",
       "         -7.5269e-03,  6.8469e-03, -1.5470e-02, -6.1814e-03,  5.6947e-03,\n",
       "          3.5062e-04,  6.1304e-03, -7.9488e-03, -5.6476e-04, -1.8516e-02,\n",
       "         -9.1794e-03,  1.5099e-02, -4.4870e-03, -6.1432e-03, -1.4253e-03,\n",
       "         -1.3717e-02, -9.3082e-03, -2.9339e-03,  1.6553e-03,  6.1360e-03,\n",
       "          1.0374e-02,  2.0308e-03,  4.9649e-03,  1.2407e-02, -3.7005e-03,\n",
       "         -2.4654e-03,  5.2209e-03, -7.4639e-03,  8.3230e-03,  9.9109e-03,\n",
       "         -1.8641e-02,  1.0595e-03, -1.7433e-02, -1.0395e-03, -8.0786e-03,\n",
       "         -3.4307e-03,  1.8553e-02, -1.5263e-02, -1.2960e-02, -3.0073e-03,\n",
       "          1.0318e-02, -4.4642e-03, -4.0568e-03,  1.0199e-02,  3.6343e-03,\n",
       "         -1.0487e-02, -1.1804e-02,  1.0399e-02,  1.5216e-03, -1.6846e-02,\n",
       "         -1.2066e-02,  5.5772e-03, -3.0120e-04, -9.1808e-03, -8.3696e-03,\n",
       "          1.7915e-02,  1.7738e-02,  1.0185e-02, -9.0110e-03, -5.3698e-03,\n",
       "          1.7625e-02, -5.7307e-03,  2.0368e-02, -1.0772e-03, -2.5795e-03,\n",
       "          1.1565e-02,  7.3568e-03, -1.6019e-02, -1.3182e-02, -3.7399e-03,\n",
       "         -1.5232e-02,  1.0537e-02,  9.3779e-03,  1.7876e-02, -1.3122e-02,\n",
       "         -1.0759e-02, -5.1797e-03, -9.3474e-03, -4.0479e-03, -2.4435e-02,\n",
       "          2.2446e-02,  1.6467e-02, -1.2918e-02, -6.5575e-03, -1.7750e-02,\n",
       "          9.3457e-04, -2.2049e-03, -1.4913e-02,  1.1284e-02, -1.2291e-02,\n",
       "          8.3534e-03,  3.7073e-03,  1.3338e-03, -1.0585e-02, -1.5311e-02,\n",
       "         -1.2394e-03, -2.5266e-03,  6.5887e-03, -1.1868e-02,  1.3334e-02,\n",
       "          6.4194e-04,  9.7179e-03,  7.9814e-03, -7.9269e-03, -1.1473e-02]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = net(X)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **torch.argmax** to identify the index with the maximum value. You can check out the corresponding ImageNet class here! https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(824)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next architecture we'll examine is **VGG**. VGG is just the abbreviation for the Oxford Visual Geometry Group. Here's an article about VGG from VGG: http://www.robots.ox.ac.uk/~vgg/research/very_deep/\n",
    "\n",
    "VGG is one of the most straightforward convolutional neural networks. The torchvision code might look intimidating at first, since it's designed for multiple configurations of VGG: https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
    "\n",
    "As we'll see with ResNet, a neural network can come in multiple flavors. In the case of VGG, it's possible to have 11, 13, 16, or 19 layers, each with or without **batch normalization** (we won't worry about what that is - you can check out the paper if you're curious (https://arxiv.org/pdf/1502.03167.pdf), or just google \"batch norm(alization)\")). We define which VGG we want to use by setting __features__\n",
    "\n",
    "Let's start with our generic VGG template. Once we've defined this, we'll examine specific \"flavors\" of VGG (e.g. VGG16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, num_classes=1000): # omitting weight initialization argument\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features  # used for defining specific type of VGG, e.g. VGG19\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7)) # average pooling with (7, 7) output\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),  # 512 kernels from last conv; (7, 7) pool output\n",
    "            nn.ReLU(True), # inplace=True\n",
    "            nn.Dropout(), # 0.5 by default\n",
    "            nn.Linear(4096, 4096), # 4096 output from last nn.Linear\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)  # flattens the output of average pooling\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next method of VGG defines specific VGG networks using the **make_layers** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layers(cfg):  # omitting batch normalization argument here and below\n",
    "    layers = []  # defining empty list\n",
    "    in_channels = 3  # for three-channel (e.g. RGB) images\n",
    "    for v in cfg:\n",
    "        if v == 'M':  # add pooling layers for each cfg = 'M' (see 'cfgs' below)\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:  # add convolutional layers for each cfg != 'M', where out_channels=v\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "    return nn.Sequential(*layers)  # return nn.Sequential with added layers\n",
    "\n",
    "cfgs = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],  # used for vgg11\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],  # used for vgg13\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],  # used for vgg16\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],  # used for vgg19\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then have a separate **\\_vgg** function for building the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _vgg(arch, cfg, **kwargs):  # omitting batch_norm, pretrained, and progress arguments\n",
    "    model = VGG(make_layers(cfgs[cfg], **kwargs))  # calls make_layers for particular cfg\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine our function for building VGG16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16(**kwargs):  # again, omitting batch_norm, pretrained, and progress arguments\n",
    "    return _vgg('vgg16', 'D', **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, num_classes=1000): # omitting weight initialization argument\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features  # used for defining specific type of VGG, e.g. VGG19\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7)) # average pooling with (7, 7) output\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),  # 512 kernels from last conv; (7, 7) pool output\n",
    "            nn.ReLU(True), # inplace=True\n",
    "            nn.Dropout(), # 0.5 by default\n",
    "            nn.Linear(4096, 4096), # 4096 output from last nn.Linear\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "         \n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)  # flattens the output of average pooling\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def make_layers(cfg):  # omitting batch normalization argument here and below\n",
    "        layers = []  # defining empty list\n",
    "        in_channels = 3  # for three-channel (e.g. RGB) images\n",
    "        for v in cfg:\n",
    "            if v == 'M':  # add pooling layers for each cfg = 'M' (see 'cfgs' below)\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:  # add convolutional layers for each cfg != 'M', where out_channels=v\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "        return nn.Sequential(*layers)  # return nn.Sequential with added layers\n",
    "\n",
    "\n",
    "    cfgs = {\n",
    "        'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],  # used for vgg11\n",
    "        'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],  # used for vgg13\n",
    "        'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],  # used for vgg16\n",
    "        'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],  # used for vgg19\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def _vgg(arch, cfg, **kwargs):  # omitting batch_norm, pretrained, and progress arguments\n",
    "        model = VGG(make_layers(cfgs[cfg], **kwargs))  # calls make_layers for particular cfg\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def vgg16(**kwargs):  # again, omitting batch_norm, pretrained, and progress arguments\n",
    "        return _vgg('vgg16', 'D', **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our **net** as a specific version of VGG (e.g. VGG16). As the name suggests, their are 16 total layers between convolutions and (fully-connected) matrix multiplications, each separated by a ReLU activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = vgg16\n",
    "net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good stuff. You can also experiment with other VGG architectures and see how they differ.\n",
    "\n",
    "You might be curious about **dilation** and __ceil_mode__. The \"Introduction to Neural Networks\" tutorial covers this:\n",
    "\n",
    "**dilation** refers to spacing between values in a convolutional or padding kernel. For instance, if we applied our 2x2 kernel with a dilation rate of 2, we would end up applying our operation over a 3x3 field but only performing elementwise multiplication on every other row or column.\n",
    "\n",
    "A visualization DEFINITELY helps to explain dilation (and other convolutional operations!) https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\n",
    "\n",
    "What about **ceil_mode**? Remember when we computed the dimensions of the output of our convolutional layer? We can do the same for the output of our pooling layer. We don't need to go over the details of the operation (see https://pytorch.org/docs/stable/\\_modules/torch/nn/modules/pooling.html if you're curious). \n",
    "\n",
    "The point is that our dimensions must be (positive) integers, and thus if the operation produces a non-integer output, we'll either need to round down (floor) or up (ceiling). When ceil_mode=False (by default), PyTorch uses the floor of the operation rather than the ceiling to compute the dimensions of the pooling output.\n",
    "\n",
    "Let's keep moving along, since we still have two more architectures to cover. For more details on forward and backward propagation, consult our \"Introduction to Neural Networks\" tutorial. We'll focus on architectures from here on.\n",
    "\n",
    "Next up is Inception v3! You can use this for reference: https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception3()\n"
     ]
    }
   ],
   "source": [
    "class Inception3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Inception3, self).__init__()\n",
    "\n",
    "        # layers go here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # defines how layers are applied to inputs\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net = Inception3()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first set of layers we'll add use the function **BasicConv2d**. This is defined in a separate class, which inherits from __nn.Module__!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the networks we've seen so far - LeNet, AlexNet, and VGG - **BasicConv2d** has two methods: __\\_\\_init\\_\\___ and **forward**. __\\_\\_init\\_\\___ defines the architecture, in this case a single two-dimensional convolution with a specified number of input and output channels and no bias, plus a **batch normalization** layer. We encountered batch normalization in our examination of VGG, though we didn't end up constructing a network with \"batch norm.\" Batch normalization takes a layer's inputs, subtracts the mean input over the batch from each input, and divides by the square root of the squared variance (plus a small epsilon value). Without getting into the details of \"batch norm\" (the theory is still kinda murky), just know that it scales our inputs to enable our network to learn faster.\n",
    "\n",
    "Now let's return to our **Inception3** class. We first use our __BasicConv2d__ function to define six convolutional layers:\n",
    "\n",
    "**Conv2d_1a_3x3** takes our three color channels as input and applies 32 3x3 convolutions with stride of 2. Our __BasicConv2d__ function applies batch normalization with the following parameters. You will see this once you execute the subsequent block of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception3(\n",
      "  (Conv2d_1a_3x3): BasicConv2d(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (Conv2d_2a_3x3): BasicConv2d(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (Conv2d_2b_3x3): BasicConv2d(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (Conv2d_3b_1x1): BasicConv2d(\n",
      "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (Conv2d_4a_3x3): BasicConv2d(\n",
      "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Inception3(nn.Module):\n",
    "    def __init__(self, num_classes=1000):  # 1000 classes for ImageNet LSVRC (by default)\n",
    "        super(Inception3, self).__init__()  # omitting aux_logits and transform_input\n",
    "        \n",
    "        self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, stride=2)  # 3x3 convolution\n",
    "        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3)  # (stride=1)\n",
    "        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)  # with padding\n",
    "        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)  # 1x1 convolution\n",
    "        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # defines how layers are applied to inputs\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net = Inception3()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our batch normalization parameters in our (bn) layers:\n",
    "\n",
    "1) **num_features**=__output_features__ from **BasicConv2d**. For instance, in **self.Conv2d_1a_3x3**, the number of output features is 32. We want to apply batch normalization to these features before passing the data forward to the next layer.\n",
    "\n",
    "2) **eps** is the small epsilon value that we add to our squared variance (before taking the squared root and dividing the difference of our input and the mean input for the batch of inputs by this number)\n",
    "\n",
    "3) We can also add **momentum**. With momentum, we have a \"running mean\" and \"running variance\" that we update each time we pass the data through a layer. We multiply our momentum term by the mean of our inputs for the layer, then subtract our momentum term from one and multiply it by our running mean. We then do the same for our variance.\n",
    "\n",
    "4) If **affine=True**, we pass our normalized mean through a linear transformation. This corresponds to __//scale and shift__ in Ioffe and Szegedy 2015 (https://arxiv.org/pdf/1502.03167.pdf)\n",
    "\n",
    "5) **track_running_stats** needs to be set to __True__ in order to record our running mean and variance (e.g. for updating these terms with momentum)\n",
    "\n",
    "**torch.nn.modules.batchnorm** code is available at https://pytorch.org/docs/stable/\\_modules/\\torch/\\nn/\\modules/\\batchnorm.html\n",
    "\n",
    "We now define a set of six **inception modules**. Each of these are a series of convolutional layers with batch normalization. We thus use our __BasicConv2d__ function to define these layers. There's a lot of code here, but it should look familiar by now. \n",
    "\n",
    "Let's start with **InceptionA**, which consists of seven convolutional layers. Again, we'll use our standard template, inheriting our class from __nn.Module__ and defining constructor and forward methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionA(nn.Module):\n",
    "    \n",
    "    def __init__ (self, in_channels, pool_features):  # pool_features=out_channels for branch_pool\n",
    "        super(InceptionA, self).__init__()\n",
    "        \n",
    "        # convolutional layers go here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # convolution and pooling applied to input here\n",
    "        \n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.cat** will concatenate our outputs (to be defined in __forward__). **1** is short for __dim=1__, which means to concatenate the outputs along our second dimension. To concatenate along the first dimension, use __dim=0__ (since PyTorch like all things Python is zero-indexed)\n",
    "\n",
    "Now let's add our convolutional layers. Again, these are all defined with batch normalization using **BasicConv2d**:\n",
    "\n",
    "As the name implies, **branch1x1** is a 1x1 convolution applied to our __in_channels__. We learn 64 convolutional kernels.\n",
    "\n",
    "**branch5x5_1** is a 1x1 convolution, also applied to our __in_channels__ (not the output of **branch1x1**! Hmm, what's going on here? And why is it called __branch5x5_1__, not **branch1x1_1** or something?). We learn 48 of these kernels.\n",
    "\n",
    "**branch5x5_2** is a 5x5 convolution, applied to 48 input channels (presumably the output of __branch5x5_1__?) with zero padding of depth 2. We learn 64 of these.\n",
    "\n",
    "**branch3x3dbl_1** learns 1x1 kernels applied to __in_channels__ with 64 outputs\n",
    "\n",
    "**branch3x3dbl_2** learns 96 3x3 kernels from 64 inputs with __padding=1__\n",
    "\n",
    "**branch3x3dbl_3** learns 96 3x3 kernels from 96 inputs with __padding=1__\n",
    "\n",
    "**branch_pool** learns 1x1 kernels applied to __in_channels__ with output equal to **pool_features**\n",
    "\n",
    "All of these convolutions are stride of 1 by default. Let's add them to our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionA(nn.Module):\n",
    "    \n",
    "    def __init__ (self, in_channels, pool_features):  # pool_features=out_channels for branch_pool\n",
    "        super(InceptionA, self).__init__()\n",
    "        \n",
    "        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "        \n",
    "        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n",
    "        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # convolution and pooling applied to input here\n",
    "        \n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to pass our **x** through these layers. We also define a __branch_pool__, which performs 3x3 average pooling over our input with stride and padding of 1. We then apply the *convolution* defined in **self.branch_pool** to our pooled output!\n",
    "\n",
    "What's different about this \"inception module\" in contrast to the other networks we've seen is that we're not passing __x__ through all of the layers in **init** sequentially. Instead, we're passing the data through *each* of the branches (1x1, 5x5, 3x3dbl, and pool), and then concatenating the outputs. Weird, huh?\n",
    "\n",
    "Let's see how the module looks with our **forward** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionA(nn.Module):\n",
    "    \n",
    "    def __init__ (self, in_channels, pool_features):  # pool_features=out_channels for branch_pool\n",
    "        super(InceptionA, self).__init__()\n",
    "        \n",
    "        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "        \n",
    "        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n",
    "        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "        \n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "        \n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got five more of these. In **InceptionB**, we pass the input through a 3x3 kernel with stride of 2, and then pass the same input sequentially through three kernels:\n",
    "\n",
    "1) 1x1 convolution with stride of 1 and 64 outputs\n",
    "2) 3x3 convolution with stride of 1, padding of 1, and 96 outputs\n",
    "3) 3x3 convolution with stride of 2 and 96 outputs\n",
    "\n",
    "We'll also do some max pooling on our input, but this time we won't pass that output through a convolutional layer. As with **InceptionA**, we return our concatenated outputs. We still need to figure out how to combine these modules!\n",
    "\n",
    "Let's take a look at **InceptionB**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionB(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionB, self).__init__()\n",
    "        self.branch3x3 = BasicConv2d(in_channels, 384, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "\n",
    "        outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's about to get weird. **InceptionC** takes an additional parameter, __channels_7x7__, and uses this parameter to define the number of output channels for the first layer in two of our sets of convolutional layers. Ok, that's not so weird, but check out the kernel sizes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionC(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, channels_7x7):\n",
    "        super(InceptionC, self).__init__()\n",
    "        self.branch1x1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "\n",
    "        c7 = channels_7x7\n",
    "        self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7_3 = BasicConv2d(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "\n",
    "        self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_5 = BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "\n",
    "        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch7x7 = self.branch7x7_1(x)\n",
    "        branch7x7 = self.branch7x7_2(branch7x7)\n",
    "        branch7x7 = self.branch7x7_3(branch7x7)\n",
    "\n",
    "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
    "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been dealing with square kernels. Now we have alternating (7, 1) and (1, 7) rectangular kernels, with asymmetric padding!?\n",
    "\n",
    "Let's keep rolling with **InceptionD**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionD(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionD, self).__init__()\n",
    "        self.branch3x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "        self.branch3x3_2 = BasicConv2d(192, 320, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch7x7x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "        self.branch7x7x3_2 = BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7x3_3 = BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7x3_4 = BasicConv2d(192, 192, kernel_size=3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = self.branch3x3_2(branch3x3)\n",
    "\n",
    "        branch7x7x3 = self.branch7x7x3_1(x)\n",
    "        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        outputs = [branch3x3, branch7x7x3, branch_pool]\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and **InceptionE**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionE(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionE, self).__init__()\n",
    "        self.branch1x1 = BasicConv2d(in_channels, 320, kernel_size=1)\n",
    "\n",
    "        self.branch3x3_1 = BasicConv2d(in_channels, 384, kernel_size=1)\n",
    "        self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 448, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3dbl_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = [\n",
    "            self.branch3x3_2a(branch3x3),\n",
    "            self.branch3x3_2b(branch3x3),\n",
    "        ]\n",
    "        branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = [\n",
    "            self.branch3x3dbl_3a(branch3x3dbl),\n",
    "            self.branch3x3dbl_3b(branch3x3dbl),\n",
    "        ]\n",
    "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll omit **InceptionAux**, since we only use this when __aux_logits=True__ in **Inception3**. Don't worry about this.\n",
    "\n",
    "Ok, let's return to our **Inception3** class. Now we're going to start adding the outputs of *passing our input through each inception module*! Recall that these outputs are concatenated outputs of multiple sets of convolutions, along with a pooling operation. Again, we're taking this from https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py and omitting **transform_input** and __aux_logits__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception3(\n",
      "  (Conv2d_1a_3x3): BasicConv2d(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (Conv2d_2a_3x3): BasicConv2d(\n",
      "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (Conv2d_2b_3x3): BasicConv2d(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (Conv2d_3b_1x1): BasicConv2d(\n",
      "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (Conv2d_4a_3x3): BasicConv2d(\n",
      "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (Mixed_5b): InceptionA(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch5x5_1): BasicConv2d(\n",
      "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch5x5_2): BasicConv2d(\n",
      "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_5c): InceptionA(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch5x5_1): BasicConv2d(\n",
      "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch5x5_2): BasicConv2d(\n",
      "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_5d): InceptionA(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch5x5_1): BasicConv2d(\n",
      "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch5x5_2): BasicConv2d(\n",
      "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_6a): InceptionB(\n",
      "    (branch3x3): BasicConv2d(\n",
      "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_6b): InceptionC(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_2): BasicConv2d(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_3): BasicConv2d(\n",
      "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_4): BasicConv2d(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_5): BasicConv2d(\n",
      "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_6c): InceptionC(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_2): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_3): BasicConv2d(\n",
      "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_4): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_5): BasicConv2d(\n",
      "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_6d): InceptionC(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_2): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_3): BasicConv2d(\n",
      "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_4): BasicConv2d(\n",
      "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_5): BasicConv2d(\n",
      "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_6e): InceptionC(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_2): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7_3): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_3): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_4): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7dbl_5): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_7a): InceptionD(\n",
      "    (branch3x3_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3_2): BasicConv2d(\n",
      "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7x3_1): BasicConv2d(\n",
      "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7x3_2): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7x3_3): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch7x7x3_4): BasicConv2d(\n",
      "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_7b): InceptionE(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3_1): BasicConv2d(\n",
      "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3_2a): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3_2b): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3a): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3b): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (Mixed_7c): InceptionE(\n",
      "    (branch1x1): BasicConv2d(\n",
      "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3_1): BasicConv2d(\n",
      "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3_2a): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3_2b): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_1): BasicConv2d(\n",
      "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_2): BasicConv2d(\n",
      "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3a): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch3x3dbl_3b): BasicConv2d(\n",
      "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (branch_pool): BasicConv2d(\n",
      "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Inception3(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(Inception3, self).__init__()\n",
    "        self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, stride=2)\n",
    "        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3)\n",
    "        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n",
    "        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)\n",
    "        self.Mixed_5b = InceptionA(192, pool_features=32)\n",
    "        self.Mixed_5c = InceptionA(256, pool_features=64)\n",
    "        self.Mixed_5d = InceptionA(288, pool_features=64)\n",
    "        self.Mixed_6a = InceptionB(288)\n",
    "        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n",
    "        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n",
    "        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n",
    "        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n",
    "        self.Mixed_7a = InceptionD(768)\n",
    "        self.Mixed_7b = InceptionE(1280)\n",
    "        self.Mixed_7c = InceptionE(2048)\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # defines how layers are applied to inputs\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net = Inception3()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's keep the focus on architecture, but for completeness (the **torchvision** version of) Inception v3 includes a loop for initializing parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception3(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000, aux_logits=True, transform_input=False):\n",
    "        super(Inception3, self).__init__()\n",
    "        \n",
    "        # layers (of layers of layers?) go here\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n",
    "                X = stats.truncnorm(-2, 2, scale=stddev)\n",
    "                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "                values = values.view(m.weight.size())\n",
    "                with torch.no_grad():\n",
    "                    m.weight.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the data propagates through the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception3(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N x 3 x 299 x 299\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # N x 32 x 149 x 149\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # N x 32 x 147 x 147\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # N x 64 x 147 x 147\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # N x 64 x 73 x 73\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # N x 80 x 73 x 73\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # N x 192 x 71 x 71\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        # N x 192 x 35 x 35\n",
    "        x = self.Mixed_5b(x)\n",
    "        # N x 256 x 35 x 35\n",
    "        x = self.Mixed_5c(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_5d(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_6a(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6b(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6c(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6d(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6e(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_7a(x)\n",
    "        # N x 1280 x 8 x 8\n",
    "        x = self.Mixed_7b(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        x = self.Mixed_7c(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        # Adaptive average pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # N x 2048\n",
    "        x = self.fc(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then put it all together inside a function **inception_v3**. We'll ignore the __pretrained__ option. **progress=True** just gives us a progress bar during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_v3(progress=True, **kwargs):\n",
    "    \n",
    "    return Inception3(**kwargs)\n",
    "\n",
    "\n",
    "    class Inception3(nn.Module):\n",
    "\n",
    "        def __init__(self, num_classes=1000):\n",
    "            super(Inception3, self).__init__()\n",
    "            self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, stride=2)\n",
    "            self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3)\n",
    "            self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n",
    "            self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n",
    "            self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)\n",
    "            self.Mixed_5b = InceptionA(192, pool_features=32)\n",
    "            self.Mixed_5c = InceptionA(256, pool_features=64)\n",
    "            self.Mixed_5d = InceptionA(288, pool_features=64)\n",
    "            self.Mixed_6a = InceptionB(288)\n",
    "            self.Mixed_6b = InceptionC(768, channels_7x7=128)\n",
    "            self.Mixed_6c = InceptionC(768, channels_7x7=160)\n",
    "            self.Mixed_6d = InceptionC(768, channels_7x7=160)\n",
    "            self.Mixed_6e = InceptionC(768, channels_7x7=192)\n",
    "            self.Mixed_7a = InceptionD(768)\n",
    "            self.Mixed_7b = InceptionE(1280)\n",
    "            self.Mixed_7c = InceptionE(2048)\n",
    "            self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                    import scipy.stats as stats\n",
    "                    stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n",
    "                    X = stats.truncnorm(-2, 2, scale=stddev)\n",
    "                    values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "                    values = values.view(m.weight.size())\n",
    "                    with torch.no_grad():\n",
    "                        m.weight.copy_(values)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # N x 3 x 299 x 299\n",
    "            x = self.Conv2d_1a_3x3(x)\n",
    "            # N x 32 x 149 x 149\n",
    "            x = self.Conv2d_2a_3x3(x)\n",
    "            # N x 32 x 147 x 147\n",
    "            x = self.Conv2d_2b_3x3(x)\n",
    "            # N x 64 x 147 x 147\n",
    "            x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "            # N x 64 x 73 x 73\n",
    "            x = self.Conv2d_3b_1x1(x)\n",
    "            # N x 80 x 73 x 73\n",
    "            x = self.Conv2d_4a_3x3(x)\n",
    "            # N x 192 x 71 x 71\n",
    "            x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "            # N x 192 x 35 x 35\n",
    "            x = self.Mixed_5b(x)\n",
    "            # N x 256 x 35 x 35\n",
    "            x = self.Mixed_5c(x)\n",
    "            # N x 288 x 35 x 35\n",
    "            x = self.Mixed_5d(x)\n",
    "            # N x 288 x 35 x 35\n",
    "            x = self.Mixed_6a(x)\n",
    "            # N x 768 x 17 x 17\n",
    "            x = self.Mixed_6b(x)\n",
    "            # N x 768 x 17 x 17\n",
    "            x = self.Mixed_6c(x)\n",
    "            # N x 768 x 17 x 17\n",
    "            x = self.Mixed_6d(x)\n",
    "            # N x 768 x 17 x 17\n",
    "            x = self.Mixed_6e(x)\n",
    "            # N x 768 x 17 x 17\n",
    "            x = self.Mixed_7a(x)\n",
    "            # N x 1280 x 8 x 8\n",
    "            x = self.Mixed_7b(x)\n",
    "            # N x 2048 x 8 x 8\n",
    "            x = self.Mixed_7c(x)\n",
    "            # N x 2048 x 8 x 8\n",
    "            # Adaptive average pooling\n",
    "            x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "            # N x 2048 x 1 x 1\n",
    "            x = F.dropout(x, training=self.training)\n",
    "            # N x 2048 x 1 x 1\n",
    "            x = x.view(x.size(0), -1)\n",
    "            # N x 2048\n",
    "            x = self.fc(x)\n",
    "            # N x 1000 (num_classes)\n",
    "            return x\n",
    "\n",
    "\n",
    "    class InceptionA(nn.Module):\n",
    "\n",
    "        def __init__(self, in_channels, pool_features):\n",
    "            super(InceptionA, self).__init__()\n",
    "            self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "\n",
    "            self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n",
    "            self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)\n",
    "\n",
    "            self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "            self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "            self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, padding=1)\n",
    "\n",
    "            self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            branch1x1 = self.branch1x1(x)\n",
    "\n",
    "            branch5x5 = self.branch5x5_1(x)\n",
    "            branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "            branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "            branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "            branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "            branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "            branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "            outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "            return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "    class InceptionB(nn.Module):\n",
    "\n",
    "        def __init__(self, in_channels):\n",
    "            super(InceptionB, self).__init__()\n",
    "            self.branch3x3 = BasicConv2d(in_channels, 384, kernel_size=3, stride=2)\n",
    "\n",
    "            self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "            self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "            self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, stride=2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            branch3x3 = self.branch3x3(x)\n",
    "\n",
    "            branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "            branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "            branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "            branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "\n",
    "            outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
    "            return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "    class InceptionC(nn.Module):\n",
    "\n",
    "        def __init__(self, in_channels, channels_7x7):\n",
    "            super(InceptionC, self).__init__()\n",
    "            self.branch1x1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "\n",
    "            c7 = channels_7x7\n",
    "            self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n",
    "            self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "            self.branch7x7_3 = BasicConv2d(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "\n",
    "            self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n",
    "            self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "            self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "            self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "            self.branch7x7dbl_5 = BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "\n",
    "            self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            branch1x1 = self.branch1x1(x)\n",
    "\n",
    "            branch7x7 = self.branch7x7_1(x)\n",
    "            branch7x7 = self.branch7x7_2(branch7x7)\n",
    "            branch7x7 = self.branch7x7_3(branch7x7)\n",
    "\n",
    "            branch7x7dbl = self.branch7x7dbl_1(x)\n",
    "            branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
    "            branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
    "            branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
    "            branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
    "\n",
    "            branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "            branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "            outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
    "            return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "    class InceptionD(nn.Module):\n",
    "\n",
    "        def __init__(self, in_channels):\n",
    "            super(InceptionD, self).__init__()\n",
    "            self.branch3x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "            self.branch3x3_2 = BasicConv2d(192, 320, kernel_size=3, stride=2)\n",
    "\n",
    "            self.branch7x7x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "            self.branch7x7x3_2 = BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "            self.branch7x7x3_3 = BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "            self.branch7x7x3_4 = BasicConv2d(192, 192, kernel_size=3, stride=2)\n",
    "\n",
    "        def forward(self, x):\n",
    "            branch3x3 = self.branch3x3_1(x)\n",
    "            branch3x3 = self.branch3x3_2(branch3x3)\n",
    "\n",
    "            branch7x7x3 = self.branch7x7x3_1(x)\n",
    "            branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n",
    "            branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n",
    "            branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n",
    "\n",
    "            branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "            outputs = [branch3x3, branch7x7x3, branch_pool]\n",
    "            return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "    class InceptionE(nn.Module):\n",
    "\n",
    "        def __init__(self, in_channels):\n",
    "            super(InceptionE, self).__init__()\n",
    "            self.branch1x1 = BasicConv2d(in_channels, 320, kernel_size=1)\n",
    "\n",
    "            self.branch3x3_1 = BasicConv2d(in_channels, 384, kernel_size=1)\n",
    "            self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "            self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "            self.branch3x3dbl_1 = BasicConv2d(in_channels, 448, kernel_size=1)\n",
    "            self.branch3x3dbl_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)\n",
    "            self.branch3x3dbl_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "            self.branch3x3dbl_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "            self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            branch1x1 = self.branch1x1(x)\n",
    "\n",
    "            branch3x3 = self.branch3x3_1(x)\n",
    "            branch3x3 = [\n",
    "                self.branch3x3_2a(branch3x3),\n",
    "                self.branch3x3_2b(branch3x3),\n",
    "            ]\n",
    "            branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "            branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "            branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "            branch3x3dbl = [\n",
    "                self.branch3x3dbl_3a(branch3x3dbl),\n",
    "                self.branch3x3dbl_3b(branch3x3dbl),\n",
    "            ]\n",
    "            branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "            branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "            branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "            outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "            return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "    class BasicConv2d(nn.Module):\n",
    "\n",
    "        def __init__(self, in_channels, out_channels, **kwargs):\n",
    "            super(BasicConv2d, self).__init__()\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "            self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            x = self.bn(x)\n",
    "            return F.relu(x, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a lot of code. Here's the article if you want to learn more about the logic behind Inception v3: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf\n",
    "\n",
    "The basic idea of Inception v3 is to reduce the computational cost of stacking large (e.g. 5x5 and 7x7) convolutions. Remember AlexNet and its 11x11 convolutions? Inception v3 does this by replacing large convolutions with sets of smaller convolutions, concatenating the output, and feeding this output as input to another network.\n",
    "\n",
    "One last thing: if you want to experiment with passing data through Inception v3, it takes tensors of size N x 3 x 299 x 299 (a size-N batch of 299x299 three-channel images)\n",
    "\n",
    "Last but certainly not least we have ResNet. It's not as verbose as Inception v3. Like VGG, there are multiple flavors of ResNet: resnet18, resnet34, resnet50, resnet101, renset152, resnet50_32x4d, and resnet101_32x8d.\n",
    "\n",
    "Here's the ResNet paper: https://arxiv.org/pdf/1512.03385\n",
    "\n",
    "One of the problems with training deeper neural networks is that error (on both training and test sets) tends to flatten over time or even increase. Updating weights becomes less effective as we add more and more layers, and may even increase the loss once the network reaches a certain depth. ResNet tackles this issue by adding *shortcut connections* - an identity mapping between layers. This is a somewhat strange finding. Maybe it will make more sense in code?\n",
    "\n",
    "The convolutional operations are simple, just 1x1 and 3x3 kernels with stride of 1. Note the padding and dilation in the 3x3 kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our **BasicBlock** class. This defines the architecture of a *residual block* of layers with skipped connections. Recall that we aren't going to pass data through the layer sequentially. We'll be skipping some connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1  # used later to define layers \n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we have here:\n",
    "\n",
    "1) A 3x3 convolution with default stride 1 and number of inputs and outputs defined by the researcher\n",
    "\n",
    "2) A normalization layer (**nn.BatchNorm2d** by default. I don't know why it's defined this way in the code) with the number of inputs equal to the outputs of the first convolutional layer\n",
    "\n",
    "3) A ReLU activation\n",
    "\n",
    "4) A 3x3 convolution with default stride 1 and number of inputs and outputs defined by the researcher (both are equal to the output of the first convolutional layer)\n",
    "\n",
    "5) Another normalization layer, identical to the first one\n",
    "\n",
    "6/7) **downsample** and **stride** are also passed as input by the researcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our forward pass proceeds with **conv1**, __bn1__, **relu**, __conv2__, **bn2**, and optionally __downsample__. We add our identity and pass the output through a ReLU activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    identity = x\n",
    "\n",
    "    out = self.conv1(x)\n",
    "    out = self.bn1(out)\n",
    "    out = self.relu(out)\n",
    "\n",
    "    out = self.conv2(out)\n",
    "    out = self.bn2(out)\n",
    "\n",
    "    if self.downsample is not None:\n",
    "        identity = self.downsample(x)\n",
    "\n",
    "    out += identity\n",
    "    out = self.relu(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a **Bottleneck** class. As the name suggests, we pass a 1x1 convolutional layer, followed by a 3x3 layer, followed by another 1x1 layer. __BasicBlock__ is used for resnet18 and resnet34, **Bottleneck** is used for resnet50, resnet101, resnet152, resnext50_32x4d, and resnext101_32x8d. The rest of the code should look similar to __BasicBlock__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we define our generic ResNet architecture (we'll get to that soon!) we can pass our specific ResNet (e.g. resnet50), which in turn calls a function **\\_resnet**. To simplify things, let's assume we're not using a pretrained model (i.e. we're not initializing our weights with the output of a ResNet previously trained on ImageNet). Let's start with **\\_resnet**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\_resnet** takes the following arguments:\n",
    "\n",
    "**arch** is the specific architecture of our model (e.g. __resnet50__)\n",
    "\n",
    "**block** is either __BasicBlock__ (for __resnet18__ and **resnet34**) or __Bottleneck__ (for **resnet50** and larger networks)\n",
    "\n",
    "**layers** is created from the __make_layer__ method of our **ResNet** class (we'll get to this next)\n",
    "\n",
    "The source code (https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py) also includes arguments for **pretrained** and __progress__. We won't use these, so don't worry about them. We've already covered these arguments in our discussion of Inception v3.\n",
    "\n",
    "Now let's take a look at our different ResNet architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(**kwargs):\n",
    "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet152(**kwargs):\n",
    "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnext50_32x4d(**kwargs):\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 4\n",
    "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnext101_32x8d(**kwargs):\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 8\n",
    "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
    "                   **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check out our generic ResNet architecture. This class takes three methods: a constructor that defines the layers, a method **make_layer** that appends layers based on the arguments passed to the method, and a __forward__ method which sequentially applies a 7x7 convolution with stride of 2 and padding of 3, batch normalization, ReLU, and 3x3 max pooling with stride of 2 and padding of 1, plus layer1, layer2, layer3, and layer4 created by **make_layer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize everything in one block of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet152(**kwargs):\n",
    "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnext50_32x4d(**kwargs):\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 4\n",
    "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnext101_32x8d(**kwargs):\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 8\n",
    "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
    "                   **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work. We've now covered five of the most well-known convolutional neural networks. LeNet-5 blazed a trail for future research and applications by demonstrating that a convolutional neural network trained with backpropagation could accurately classify (grayscale) images, including handwritten digits on checks. AlexNet was the first neural network to win the ImageNet Large Scale Visual Recognition Challenge, showing the dominance of convolutional networks over computer vision algorithms with \"hand-crafted\" features. VGG remains a powerful option for classification, despite its deceptively simple architecture of stacked 3x3 convolutions. Inception v3 is an efficient alternative to networks consisting of layered convolutions, replacing large convolutions with \"modules,\" or sets of smaller convolutions. Finally, ResNet uses \"skipped connections\" between layers, enabling very deep neural networks (up to 152 layers) to learn features.\n",
    "\n",
    "There's a lot we haven't covered in this tutorial. Although we managed to explore the architectures of the most common neural networks, we didn't spend much time on how networks compute loss or update parameters. We go into greater detail on the fundamentals of training neural networks in our \"Introduction to Neural Networks\" tutorial, but there is still much to cover.\n",
    "\n",
    "If you have any questions, comments, or critiques, please email pdonnelly@groupwaretech.com!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
