{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RECURRENT NEURAL NETWORK ARCHITECTURES FOR CHARACTER GENERATION**\n",
    "\n",
    "*Patrick Donnelly, Groupware Technology*\n",
    "\n",
    "Feedforward neural networks have come to dominate computer vision and other artifical intelligence applications. These networks use models consisting of linear operations such as convolutions, matrix multiplications, and additions to transform batches of inputs into outputs. These models are then trained through backpropagation, whereby parameters are updated via optimization: taking the partial derivative of the loss function with respect to each weight and bias.\n",
    "\n",
    "When we train a feedforward neural network, we assume temporal independence between input data. Feedforward neural networks aren't necessarily deterministic. They frequently produce different outputs depending on the sequence in which input data is fed. However, the network architecture does not explicitly incorporate time. The network simply takes a batch of observations as input. It does not have the concept of *memory*, or a _state_ that updates as additional data is presented to the network. As a result, feedforward neural networks work fine for modeling \"static\" data such as images or user data (e.g. for recommendation systems), though we can certainly apply sequential models to such data (e.g. for image captioning or sequentially-aware recommendation systems).\n",
    "\n",
    "Enter the **recurrent neural network (RNN)**. RNNs incorporate __feedback loops__ into the network architecture. These loops enable the network to take as input a **hidden state** in addition to the current input value. If this doesn't quite make sense yet, let's use an extremely simple example to compare an RNN with a feedforward network. Let's take a word and represent it as a string of letters $S$. Since we don't want to rip off Andrej Karpathy's excellent blog post (http://karpathy.github.io/2015/05/21/rnn-effectiveness/) *completely*, we'll _slightly_ change the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = ['H','E','L','L','A']\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use **one-hot encodings** to represent each letter as a vector. A one-hot encoding is a vector with a one corresponding to the index of the letter (or class, in the case of categorical data). The rest of the vector is all zeroes. We're only working with four possible letters, so we can encode our letters as one-hot vectors of length four. Since we're working with categorical (but not ordinal) data, it doesn't matter which index corresponds to which letter. If you've taken the Groupware image classification tutorial (shameless plug) or otherwise worked with labeled data, you may recognize this as the way we assigned class labels to images. Let's just start with our **H** encoded as a one followed by zeroes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = [1,0,0,0]\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for our E, L, and A. We'll be reusing our L encoding since we have two Ls (no need for two separate encodings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = [0,1,0,0]\n",
    "L = [0,0,1,0]\n",
    "A = [0,0,0,1]\n",
    "print(E)\n",
    "print(L)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's think about what we're trying to predict. For given input character(s) (letter), we'd like to generate the next (output) character. So if we give the network an **H** (as input), we'd want it to return an __E__ (as output). We can also use strings of multiple input characters to generate the next output. For instance, if we feed the network \\['H','E','L','L'\\], we'd like it to generate an 'A' (of course not an 'O'! Do it for the Bay. Also, I'm watching the A's play the O's in another window right now but don't tell my manager).\n",
    "\n",
    "Let's just start with the simple example of using one letter to generate another. For our string of length $L=5$, we thus have $L-1=5-1=4$ input/output pairs:\n",
    "\n",
    "{Input: H = \\[1,0,0,0\\], Output: E = \\[0,1,0,0\\]}\n",
    "{Input: E = \\[0,1,0,0\\], Output: L = \\[0,0,1,0\\]}\n",
    "{Input: L = \\[0,0,1,0\\], Output: L = \\[0,0,1,0\\]}\n",
    "{Input: L = \\[0,0,1,0\\], Output: A = \\[0,0,0,1\\]}\n",
    "\n",
    "Now let's say we're using a standard feedforward network to generate our output letters. Of course we'd never do this in practice. You can already see an inconsistency: there's an 'L' after the first 'L', but there's an 'A' after the second 'L.' However, our feedforward network can't tell the difference...\n",
    "\n",
    "What sort of function should we use to map our inputs to outputs? How about a simple linear transformation! Let's start with a matrix multiplication (without adding a \"bias\" term). We can multiply an $nxm$ matrix by our $4x1$ $mxp$ input vector to get our $4x1$ $nxp$ output vector (see https://en.wikipedia.org/wiki/Matrix_multiplication - I use this almost as much as I google 'untar file linux'). You might be like, \"well, wait a second, these are $1x4$ input and output vectors,\" and you would be correct, but then we get a very uninteresting 1x1 matrix multiplication. So we instead multiply a **weight matrix** by the transpose of our row vector (or just feed it a column vector) to get our output (also a column vector or transpose of our row vector).\n",
    "\n",
    "NumPy makes matrix operations much easier in Python. Let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data are currently lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(H))\n",
    "print(type(E))\n",
    "print(type(L))\n",
    "print(type(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert our data to numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "H = np.array(H)\n",
    "E = np.array(E)\n",
    "L = np.array(L)\n",
    "A = np.array(A)\n",
    "print(type(H))\n",
    "print(type(E))\n",
    "print(type(L))\n",
    "print(type(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we convert a list $L = [x_{1}, ..., x_{n}]$ to a *np.ndarray* using L = np.array(L), the default shape is (len(L),):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(4,)\n",
      "(4,)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "print(H.shape)\n",
    "print(E.shape)\n",
    "print(L.shape)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, to do matrix math, we want to convert our (len(L),) array to a (len(L),1) \"column vector\" (although one with two \"dimensions\"). We can do this for each our letters (each of which is a one-hot \"list\") using **L.reshape(len(L),1)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "(4, 1)\n",
      "(4, 1)\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "H = H.reshape(len(H),1)\n",
    "E = E.reshape(len(E),1)\n",
    "L = L.reshape(len(L),1)\n",
    "A = A.reshape(len(A),1)\n",
    "print(H.shape)\n",
    "print(E.shape)\n",
    "print(L.shape)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the column vectors we need to do matrix arithmetic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(H)\n",
    "print(E)\n",
    "print(L)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can randomly initialize a 4x4 weight matrix to map inputs to outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.26772254 -0.81721217 -0.43338741 -0.51728738]\n",
      " [-0.37003086  0.44429691 -2.13113222  0.6184106 ]\n",
      " [-0.4784692  -1.19890252 -0.41701251 -0.23615541]\n",
      " [ 0.99985447 -1.12891024 -1.4123793  -0.51689583]]\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(4,4)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of our \"network\" as a classifier. We multiply the weight matrix by each input, find the index of the maximum value of the output vector and compare it to the label. Let's call this output vector Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.26772254],\n",
       "       [-0.37003086],\n",
       "       [-0.4784692 ],\n",
       "       [ 0.99985447]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.matmul(W,H)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write a simple function to see if we correctly classified our input/output pair. Recall that our \"ground truth\" output is **E**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No we did not get this right. Please backprop and try again'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def did_we_get_this_right(Y,E):\n",
    "    if np.argmax(Y) == np.argmax(E):\n",
    "        return 'Yes we did get this right'\n",
    "    return 'No we did not get this right. Please backprop and try again'\n",
    "\n",
    "did_we_get_this_right(Y,E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add bias to our linear transformation by adding a $4x1$ vector $b$ to $W$. Let's randomly initialize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.80053434]\n",
      " [-1.64311801]\n",
      " [ 1.38740459]\n",
      " [ 2.10735781]]\n"
     ]
    }
   ],
   "source": [
    "b = np.random.randn(4,1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try our linear transformation with bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.06825688]\n",
      " [-2.01314888]\n",
      " [ 0.90893539]\n",
      " [ 3.10721228]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No we did not get this right. Please backprop and try again'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.matmul(W,H)\n",
    "Y = np.add(Y,b)\n",
    "print(Y)\n",
    "did_we_get_this_right(Y,E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we haven't actually done any weight or bias updates, so whether or not we have correctly classified our output is completely random. The purpose of this exercise is merely to show how we would map inputs to outputs using a simple linear transformation.\n",
    "\n",
    "One more thing, we'd also want to transform our output Y into a probability vector by exponentiating each output and dividing by the sum of exponentiated outputs. We call this the **softmax function**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01357192],\n",
       "       [0.0052757 ],\n",
       "       [0.09802235],\n",
       "       [0.88313002]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(Y):\n",
    "    return np.exp(Y)/np.sum(np.exp(Y))\n",
    "\n",
    "softmax(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. But you see why this algorithm is kinda dumb? Consider our output for input **L**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes we did get this right\n",
      "No we did not get this right. Please backprop and try again\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.05873781],\n",
       "       [0.00463092],\n",
       "       [0.5324013 ],\n",
       "       [0.40422997]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.matmul(W,L)\n",
    "Y = np.add(Y,b)\n",
    "print(did_we_get_this_right(Y,L))\n",
    "print(did_we_get_this_right(Y,A))\n",
    "softmax(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are actually two possible outputs for input **L**: 'L' (if it's the first 'L') and 'A' (if it's the second 'L'). We have no idea which output we're trying to predict, since the algorithm has no knowledge of sequence. How could we provide this knowledge as input?\n",
    "\n",
    "The first thing we'll do is introduce a **hidden layer** to our network. This is simply an additional linear transformation. We'll compute two matrix multiplications and additions rather than one.\n",
    "\n",
    "While we're not accounting for sequencing yet, maybe you can see how the introduction of a hidden layer will make modeling a sequence of characters easier? If not, no worries! I have no idea whether I would have grasped this intuition myself either, since I have not been presented the same sequence of data as the reader.\n",
    "\n",
    "Let's randomly initialize a second weight matrix and bias vector. We're gonna change our nomenclature here. Following Karpathy's (amazing) blog post, we'll call the weights and biases for our first layer **W_xh** and __b_xh__. The *x* refers to our input and the *h* refers to our **hidden vector**, situated between our input and output vectors. If we choose an identical length for our input, hidden, and output vectors, we can reuse the __np.random.randn()__ function we used to initialize our prior weight matrix and bias vector. Let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.98142587  0.40522554 -0.17918443  0.0747749 ]\n",
      " [-0.43499536 -0.54000442 -0.72627699 -0.05630791]\n",
      " [-0.58736728  0.7137192  -0.90068108 -1.06274096]\n",
      " [ 0.8556658  -0.06196241  1.47379536 -0.10516402]]\n",
      "[[-0.30636208]\n",
      " [-0.56215209]\n",
      " [ 0.10591097]\n",
      " [-1.10188667]]\n"
     ]
    }
   ],
   "source": [
    "W_xh = np.random.randn(4,4)\n",
    "b_xh = np.random.randn(4,1)\n",
    "print(W_xh)\n",
    "print(b_xh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a second weight matrix and bias vector to connect our hidden vector with our output vector. Since we're keeping the length of our input, hidden, and output vectors identical, we'll pass the same arguments to **np.random.randn()** as we did with __W_xh__ and **b_xh**. We'll call our weight matrix __W_hy__ and our bias vector **b_hy** since they connect our hidden vector *h* with our output vector _y_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.19617907e-01 -1.87853287e-01  9.49974131e-01  1.47702080e+00]\n",
      " [-1.14526258e+00 -5.28390386e-01 -9.85818626e-04 -4.29507084e-01]\n",
      " [ 1.12734483e+00  9.08576176e-01 -1.66398994e+00 -1.10708956e+00]\n",
      " [ 3.82694730e-02 -1.35913733e-01  1.29185449e+00  3.45197729e-01]]\n",
      "[[ 1.68147147]\n",
      " [ 0.33559643]\n",
      " [ 0.33195594]\n",
      " [-0.29763739]]\n"
     ]
    }
   ],
   "source": [
    "W_hy = np.random.randn(4,4)\n",
    "b_hy = np.random.randn(4,1)\n",
    "print(W_hy)\n",
    "print(b_hy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've worked with multilayer neural networks, you'll know that it's necessary to add an **activation function** before passing the data from one linear layer to another. If you haven't worked with multilayer neural networks (or have worked with multilayer neural networks and didn't know this), now you know. For more details, check out our \"Introduction to Neural Networks tutorial!\"\n",
    "\n",
    "An activation function needs to do a few things for us (and for others who are using the network). It's gotta be nonlinear: we want our neural network to be a **universal approximator** (https://en.wikipedia.org/wiki/Universal_approximation_theorem), not just a linear function with many parameters. Even if we don't care about whether or not our function is nonlinear, there's no point in adding addition linear layers without an activation function; the function will just \"collapse\" to a single layer (see this elegant explanation on StackExchange: https://stats.stackexchange.com/questions/267024/what-if-do-not-use-any-activation-function-in-the-neural-network)\n",
    "\n",
    "We also need an activation function that will allow our weights and biases to update effectively. Parameter updates are a form of optimization: we take the derivative of our loss function with respect to each weight and bias and set it to zero. We compute this derivative using the chain rule: sequentially multiplying each partial derivative as we \"backpropagate\" through the network. (I guess we don't need to do this sequentially since scalar multiplication is commutative.) Thus our network will only learn if our activations have nonzero derivatives. As the derivative of the activation function approaches zero, our **gradients** (a pretentious way of saying \"derivatives\") die / are killed / (euphemistically) vanish / (less violently) approach zero, and our weights and biases update less (in proportion to our *learning rate*, but that's another story).\n",
    "\n",
    "We typically apply a __hyperbolic tangent (tanh)__ activation function to **W_xh** and __b_xh__. This will squash the positive values of our output between 0 and 1 and our negative values between -1 and 0. We do have the issue of **vanishing gradients** as $x$ increases or decreases, but in practice we should be fine with a single hidden layer. (It's when we stack lots of these activations that we really run into trouble.)\n",
    "\n",
    "NumPy has a function for **tanh**. Let's apply it to the output of our first matrix multiplication and addition. We'll call this output __h__ (for \"hidden vector,\" not to be confused with our input datum **H**). \n",
    "\n",
    "We'll use this to construct a hidden vector from our input __H__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.9796093 ],\n",
       "       [-0.76039356],\n",
       "       [-0.44740916],\n",
       "       [-0.24136295]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = np.matmul(W_xh,H)\n",
    "h = np.add(h, b_xh)\n",
    "h = np.tanh(h)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass **h** through our second layer, apply softmax, and compare to our label / ground truth (the 'E' that we're supposed to generate if our model is doing its job):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No we did not get this right. Please backprop and try again\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.02789516],\n",
       "       [0.13026018],\n",
       "       [0.24194625],\n",
       "       [0.59989841]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.matmul(W_hy,h)\n",
    "Y = np.add(Y,b)\n",
    "print(did_we_get_this_right(Y,E))\n",
    "softmax(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the issue of modeling sequencing. Suppose we're presenting our second character 'E' to the network. We want to make the network aware of the sequence, in this case the 'H' that has already passed through. We can do this by modeling our hidden layer as a **hidden state**! This means that we'll update this hidden state as we pass each character through the network. How do we do this? \n",
    "\n",
    "Instead of defining our hidden vector **h** as the linear transformation (matrix multiplication and vector addition) of our input vector (call this __x__: this can be any character in our string of letters), let's start by initializing our hidden state as a vector of zeroes. \n",
    "\n",
    "We can do this using **np.zeros** and passing the shape of the array of zeroes within square brackets inside parentheses. We'll keep the dimensions identical to our prior hidden vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = np.zeros([4, 1])\n",
    "print(h.shape)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pass our first observation ('H') through the network. We first perform a linear transformation by multiplying **W_xh** by __H__ and adding **b_xh**. \n",
    "\n",
    "So far nothing's changed, except we won't directly output this to the hidden state. We'll call our transformed input __x__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.78196021],\n",
       "       [-2.07811338],\n",
       "       [ 0.80003731],\n",
       "       [ 2.96302361]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.matmul(W_xh, H)\n",
    "x = np.add(x, b)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're gonna do something new. Let's update our hidden state. To do this, we need a weight matrix **W_hh**. Let's initialize this weight matrix (as with our other weight matrices and bias vectors) with samples drawn from a Gaussian (normal) distribution of mean 0 and variance 1. We're not going to add a bias vector when we update our hidden state, though we'll continue to do this for our output. As we did before, we'll pass the dimension of our weight matrix as arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64386655 -0.85625832  1.26784127 -0.66501504]\n",
      " [-0.05879225  0.27373941 -0.29388187 -1.31669811]\n",
      " [-0.44262796 -0.78381947  1.74877914 -1.25037294]\n",
      " [-1.03120265 -1.12159143  0.5567503   0.86568674]]\n"
     ]
    }
   ],
   "source": [
    "W_hh = np.random.randn(4,4)\n",
    "print(W_hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update our hidden state, we'll multiply our weight matrix **W_hh** by the hidden state __h__. Since we're initializing our hidden state as a vector of zeroes, this won't actually change **h** the first time we perform this operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = np.matmul(W_hh, h)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add our updated hidden state __h__ to the linear transformation of our input **x**. \n",
    "\n",
    "Recall that we updated our input by performing the matrix multiplication __x = np.matmul(W_xh, H)__ and vector addition **x = np.add(x, b)**. Thus we can just update our hidden state by adding our existing hidden state __h__ to our transformed input **x**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.78196021],\n",
       "       [-2.07811338],\n",
       "       [ 0.80003731],\n",
       "       [ 2.96302361]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = h + x\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since **h** is a vector of zeroes, __h + x = x__ for the first step of our RNN.\n",
    "\n",
    "Again, we'll apply a hyperbolic tangent activation function our output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99236185],\n",
       "       [-0.96915019],\n",
       "       [ 0.66405763],\n",
       "       [ 0.99467619]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = np.tanh(h)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use this hidden state to compute our output. Again, we randomly initialize a weight matrix **W_hy** and bias vector __b_hy__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.12318582 -0.47892379 -0.98261904 -1.4032746 ]\n",
      " [ 1.28493762  0.80957265 -0.37800311 -0.13171113]\n",
      " [-0.13697523  0.40805538  1.74597203  0.5780113 ]\n",
      " [-1.05078095  0.42694039  1.38442699  0.28662435]]\n",
      "[[0.05336671]\n",
      " [0.84768325]\n",
      " [0.92051778]\n",
      " [1.73485876]]\n"
     ]
    }
   ],
   "source": [
    "W_hy = np.random.randn(4,4)\n",
    "b_hy = np.random.randn(4,1)\n",
    "print(W_hy)\n",
    "print(b_hy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we multiply **W_hy** by our hidden state __h__ and add **b_hy**. \n",
    "\n",
    "Again, we'll call this output __Y__. **softmax(Y)** gives our output as a probability vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No we did not get this right. Please backprop and try again\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.62376117e-04],\n",
       "       [2.43891723e-04],\n",
       "       [2.53676590e-01],\n",
       "       [7.45917142e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.matmul(W_hy,h)\n",
    "Y = np.add(Y,b)\n",
    "print(did_we_get_this_right(Y,E))\n",
    "softmax(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue by passing our next character through the network. We first linearly transform  (?) our **x**, now defined as the letter 'E':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3953088 ],\n",
       "       [-2.18312243],\n",
       "       [ 2.1011238 ],\n",
       "       [ 2.0453954 ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.matmul(W_xh, E)\n",
    "x = np.add(x, b)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we update our hidden state by adding our existing **h** to __x__ and taking the hyperbolic tangent of the output. \n",
    "\n",
    "This becomes the new value of **h**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.99236185]\n",
      " [-0.96915019]\n",
      " [ 0.66405763]\n",
      " [ 0.99467619]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.88265735],\n",
       "       [-0.99635072],\n",
       "       [ 0.99210221],\n",
       "       [ 0.99543475]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(h)\n",
    "h = np.tanh(h + x)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute our output for 'E':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No we did not get this right. Please backprop and try again\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.01968221e-04],\n",
       "       [1.64641171e-04],\n",
       "       [2.97475646e-01],\n",
       "       [7.02257745e-01]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.matmul(W_hy,h)\n",
    "Y = np.add(Y,b)\n",
    "print(did_we_get_this_right(Y,E))\n",
    "softmax(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get the idea? As we pass each character, the hidden state incorporates the information of the character **x** along with its prior state (which is in turn a function of the value of prior characters).\n",
    "\n",
    "What if we want to stack layers of linear transformations and activations with hidden states? Computing all this by hand (while certainly feasbile) might get verbose and unwieldy. Plus we haven't even gotten to the hard part: updating weights and biases using some form of gradient descent (e.g. with momentum)! While we're going to keep this tutorial focused on architectures rather than optimization functions, it's helpful to understand why we might benefit from a higher-level framework.\n",
    "\n",
    "In this tutorial, we'll use **PyTorch** to define our networks. The cool thing about PyTorch is that it borrows heavily from NumPy. It uses an analogue of the NumPy array called the __Tensor__. By convention, we import PyTorch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our RNN. We'll borrow heavily from the official PyTorch tutorial: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "\n",
    "We start by importing the **torch.nn** module. This contains a lot of the stuff (including the __RNN__ class) that we need to define network architectures (including our simple RNN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our **RNN** class, which inherits from __nn.Module__, the \"base class for all neural network modules\" (see https://pytorch.org/docs/stable/\\_modules/torch/nn/modules/module.html). This will include our constructor (**init**), which defines our architecture; a __forward__ method, which defines how our data sequentially *propagates* through the network; and an **initHidden** method, which (as you may have guessed) initializes our hidden layer as a vector of zeroes (just as we did!)\n",
    "\n",
    "We will use an instance of this class to build a *character-level RNN*. Let's sketch the basic structure of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        pass\n",
    "    def forward(self, input, hidden):\n",
    "        pass\n",
    "    def initHidden(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here? As we noted, our **RNN** class inherits from __nn.Module__. We also use **super()** to call the __init__ of **nn.Module** (see https://realpython.com/python-super/ for an explanation of __super()__ - it's kinda confusing)\n",
    "\n",
    "Our **init** is going to take three arguments (in addition to __self__). For our simple RNN, we use identical input, hidden, and output sizes (each are vectors of length $4$)\n",
    "\n",
    "For **forward**, we need to pass an __input__ Tensor (we're going to have to redefine our characters as Tensors!) and our hidden state prior to passing the data through the network. Our hidden state is zero-initialized and updated based on our inputs and prior hidden state as we propagate our data.\n",
    "\n",
    "**initHidden** is simple. We don't need to pass it any arguments. It'll just take our __hidden_size__ from **init** and define our vector of zeroes for initializing our hidden state.\n",
    "\n",
    "Let's start filling out our class methods! For **init**, we need to define the following:\n",
    "\n",
    "1) The connection between our input and hidden state. We use **nn.Linear** to define a linear transformation. By default, __nn.Linear__ will apply a bias vector as well as a weight matrix to each input vector. In the tutorial, the input and hidden vectors are concatenated, and thus the input to **nn.Linear** is __input_size + hidden_size__. We won't do this; we'll just pass **input_size** as our input and __hidden_size__ as our output. \n",
    "\n",
    "2) A line of code (not in the PyTorch tutorial) to define the \"loop\" component of updating our hidden state. Recall that we need to apply **tanh** to the sum of the linear transformations of our prior hidden vector and input vector. Here we transform our hidden vector without bias.\n",
    "\n",
    "3) The connection between our hidden and output state. Again, the tutorial concatenates input and hidden vectors. Instead, we'll pass our **hidden_size** as input and __output_size__ as output.\n",
    "\n",
    "4) A softmax function to apply to our output. Following the PyTorch tutorial, we actually use logged softmax (**nn.LogSoftmax**) and pass the argument __dim=1__ to apply softmax to the column dimension (for rows, use **dim=0**)\n",
    "\n",
    "Let's see how our network looks after building out our **init** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.xh = nn.Linear(input_size, hidden_size)\n",
    "        self.hh = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.hy = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input, hidden):\n",
    "        pass\n",
    "    def initHidden(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our **forward** method. Again, if you're following along with the tutorial, we'll keep our network simple and skip the concatenation of input and hidden layers. We do three things here:\n",
    "\n",
    "1) Update our hidden state by applying **tanh** to the summed linear transformations of our input and prior hidden state\n",
    "\n",
    "2) Apply **self.yh** to our updated hidden state\n",
    "\n",
    "3) Apply softmax to the output of 3)\n",
    "\n",
    "We return both our output and our new hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.xh = nn.Linear(input_size, hidden_size)\n",
    "        self.hh = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.hy = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input, hidden):\n",
    "        hidden = self.hh(hidden) + self.xh(input)\n",
    "        output = self.hy(hidden)\n",
    "        output = softmax(output)\n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we need to do is define our **initHidden**. Again, we just need this to return a vector of zeroes with length equal to the size of our hidden state. We use __torch.zeros__ instead of **np.zeros**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.xh = nn.Linear(input_size, hidden_size)\n",
    "        self.hh = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.hy = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input, hidden):\n",
    "        hidden = self.hh(hidden) + self.xh(input)\n",
    "        output = self.hy(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we call an instance of our **RNN**? We'll set the size of our input, hidden, and output vectors to 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (xh): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (hh): Linear(in_features=4, out_features=4, bias=False)\n",
       "  (hy): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = RNN(4,4,4)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pass our input character 'H' through the network. We need to convert it to a Tensor first. We use **torch.tensor** and specify __requires_grad=False__. This tells PyTorch not to compute *gradients* or derivatives for the Tensor values. We also convert this input to a **float**, the default data type for Tensor arithmetic in PyTorch. Finally, we want to resize this input as a row vector (dimensions \\[1, 4\\]). The **view** method does this for us; we simply pass our new dimensions as arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n",
      "torch.Size([1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor(H, requires_grad=False).float()\n",
    "print(input.shape)\n",
    "input = input.view(1,4)\n",
    "print(input.shape)\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also initialize our hidden length 4 vector of zeroes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = torch.zeros(1,4)\n",
    "print(hidden.shape)\n",
    "hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the **rnn** instance of our __RNN__ to generate the output of passing 'H' through our network, along with our next hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2170, -1.5799, -1.5085, -1.2850]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[-0.1291, -0.8088,  0.6089,  0.3288]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output, next_hidden = rnn(input, hidden)\n",
    "print(output)\n",
    "print(next_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sick! Now we need a loss function. Following the PyTorch tutorial, we'll use **nn.NLLLoss**. This is the *negative log likelihood loss*, which is the same as exponentiating each of our logged softmax outputs (so we've essentially canceled out the log operation) and applying _cross entropy loss_: taking the negative natural logarithm of the predicted probability for the *ground truth* class. See https://discuss.pytorch.org/t/understanding-nllloss-function/23702. We go into greater detail about cross-entropy loss in our Introduction to Neural Networks tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need an *optimizer*. This will tell the backpropagation machine inside of PyTorch how to update our weights and biases. We'll use the _Adam_ optimizer (**torch.optim.Adam**), which uses momentum when performing parameter updates. Our *learning rate* (__lr__) controls the magnitude of our parameter updates. We multiply the learning rate by our gradients and update parameters accordingly. Let's set our learning rate to 0.01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute our loss. We do this by applying our **criterion** to our model __output__ and our ground truth output 'E' (remember, 'E' comes next after 'H'). We represent our **target** 'E' as a *class index* ranging from 0 to the total number of classes minus one. In other words, we have four classes (zero-indexed), and thus we use the index 1 to represent 'E' as the second class in the class vector ('H','E','L','A'). We define this index as a **long** Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([1]).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply our loss to our output and target and backpropagate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(output, target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very cool. What if we want to do more complicated text generation? What if we want to work with sequences of lengths longer than five characters? It turns out that \"vanilla\" RNNs aren't so great at learning from long sequences. Think about the process by which we update our hidden state. As we *backpropagate through time*, we compute the partial derivative of our loss function with respect to our weight at each point in the sequence and then multiply these derivatives together. This often leads to _exploding gradients_(!) in which we perform suboptimally large weight updates, or *vanishing gradients* in which we perform suboptimally small weight updates (and thus our network doesn't learn).\n",
    "\n",
    "How do we solve this issue? We introduce *gates* into our hidden state. This particular type of \"gated\" network is called a _Long short-term memory network_, or *LSTM*. There are two great blogs on this, and I'll be drawing from both for reference: \n",
    "\n",
    "Chris Olah: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Adam Paszke: https://apaszke.github.io/lstm-explained.html\n",
    "\n",
    "Our first gate is called a *forget gate*. This decides how much information we retain (or forget) from the input and prior hidden state. This is simply the application of a _sigmoid_ function (https://en.wikipedia.org/wiki/Sigmoid_function) to the summed linear transformations of our input and prior hidden state, with a bias term added at the end. In other words, we perform the linear transformations of input and prior hidden state without bias, add a bias term to these summed linear transformations, and pass this output through a sigmoid function. Let's call our forget gate **F**.\n",
    "\n",
    "First, we initialize our hidden state as a vector of values sampled from a standard normal distribution (using __torch.randn__). From this point on, we'll be using random rather than zero initialization for our hidden states. Let's stick with **hidden_size** to define the length of our hidden state vector. We'll keep __hidden_size__, **input_size**, and __output_size__ equal to 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 4\n",
    "input_size = 4\n",
    "output_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize our hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2458, -1.2905, -0.0355, -1.5339]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.randn(1, hidden_size)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our linear transformations: input-to-hidden and hidden-to-hidden with input and output of 4. For simplicity, we'll keep the length of all vectors in this exercise equal to 4. We'll set **bias=False**: we'll just multiply our input and hidden layers by weight matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=4, bias=False)\n",
      "Linear(in_features=4, out_features=4, bias=False)\n"
     ]
    }
   ],
   "source": [
    "xh_forget = nn.Linear(4,4, bias=False)\n",
    "hh_forget = nn.Linear(4,4, bias=False)\n",
    "print(xh_forget)\n",
    "print(hh_forget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a bias vector. Let's call it **b_h**. We can randomly initialize its values using __torch.randn__ and convert it to a **float**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "tensor([[ 0.2700,  1.3868, -2.2986, -1.3354]])\n"
     ]
    }
   ],
   "source": [
    "b_forget = torch.randn(1,4).float()\n",
    "print(b_forget.type())\n",
    "print(b_forget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add the output of our linear transformations and bias vector to update our hidden state. Let's use **input** again for our input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this is the 'H' tensor. Let's use it, along with the existing/prior (depending on your temporal perspective) value of our hidden state **h** to define the raw output of our forget gate (prior to applying our activation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1588,  1.2220, -1.9492, -0.8783]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forget_gate = xh_forget(input) + hh_forget(hidden) + b_forget\n",
    "forget_gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sick! So far we're just doing what we did with the vanilla RNN. However, instead of passing the output **h** of our summed matrix-vector multiplications through a *tanh* activation, we'll use a _sigmoid_ function. We'll call the \"activated\" output of our forget gate __F__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4604, 0.7724, 0.1246, 0.2935]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F = torch.sigmoid(forget_gate)\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've defined which information we're going to *remember* (or forget). Now we need to update our hidden state. We'll do this the same way we did with the vanilla RNN! The two linear transformations will also be identical to the ones we just applied to the input and hidden state in the forget gate. We'll also apply a bias afterward, identical to **b_h**, and pass the summed output through a sigmoid function. We call this our *input gate*.\n",
    "\n",
    "First let's define a new pair of linear transformations (without bias) along with a new bias vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=4, bias=False)\n",
      "Linear(in_features=4, out_features=4, bias=False)\n",
      "tensor([[-0.5973,  0.1780,  1.1972,  0.4832]])\n"
     ]
    }
   ],
   "source": [
    "xh_input = nn.Linear(4,4, bias=False)\n",
    "hh_input = nn.Linear(4,4, bias=False)\n",
    "b_input = torch.randn(1,4).float()\n",
    "print(xh_input)\n",
    "print(hh_input)\n",
    "print(b_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's sum our linear transformations and bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5818,  0.1083,  0.9470, -0.0015]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_gate = xh_input(input) + hh_input(hidden) + b_input\n",
    "input_gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we pass our summed linear transformations and bias through the **sigmoid** function. We'll call this __I__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3585, 0.5270, 0.7205, 0.4996]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = torch.sigmoid(input_gate)\n",
    "I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we \"create a vector of candidate values\" (borrowing language from https://colah.github.io/posts/2015-08-Understanding-LSTMs/). Again, we take our network input and prior/existing hidden state as inputs, apply linear transformations without bias, and add a bias. Again, we'll apply a sigmoid function to the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=4, bias=False)\n",
      "Linear(in_features=4, out_features=4, bias=False)\n",
      "tensor([[-0.2224, -1.0292,  1.8480,  0.2707]])\n"
     ]
    }
   ],
   "source": [
    "xh_candidate = nn.Linear(4,4, bias=False)\n",
    "hh_candidate = nn.Linear(4,4, bias=False)\n",
    "b_candidate = torch.randn(1,4).float()\n",
    "print(xh_candidate)\n",
    "print(hh_candidate)\n",
    "print(b_candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our **candidate_vector** (prior to activation) as the sum of our transformed input and hidden vectors, plus our bias term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4017, -0.5760,  1.5157,  0.4986]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_vector = xh_candidate(input) + hh_candidate(hidden) + b_candidate\n",
    "candidate_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also pass this output through a **sigmoid** activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4009, 0.3598, 0.8199, 0.6221]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.sigmoid(candidate_vector)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this \"activated candidate vector\" **C** (I think I'm making up this terminology) and add it to *another* set of linearly transformed (without bias) input and hidden vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=4, bias=False)\n",
      "Linear(in_features=4, out_features=4, bias=False)\n",
      "tensor([[-0.2224, -1.0292,  1.8480,  0.2707]])\n"
     ]
    }
   ],
   "source": [
    "xh_transform = nn.Linear(4,4, bias=False)\n",
    "hh_transform = nn.Linear(4,4, bias=False)\n",
    "b_transform = torch.randn(1,4).float()\n",
    "print(xh_transform)\n",
    "print(hh_candidate)\n",
    "print(b_candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can transform our input (input + hidden state) again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4017, -0.5760,  1.5157,  0.4986]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_input = xh_candidate(input) + hh_candidate(hidden) + b_candidate\n",
    "transformed_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but this time we'll apply a **tanh** to our transformed input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3814, -0.5198,  0.9079,  0.4610]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = torch.tanh(transformed_input)\n",
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step! We'll define an *output gate*. This is the linear transformation of our input vector and prior hidden vectors (without bias), plus a bias. Then we apply a sigmoid activation to this output. Sound familiar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=4, bias=False)\n",
      "Linear(in_features=4, out_features=4, bias=False)\n",
      "tensor([[ 0.1209,  0.3103, -0.9206,  0.3118]])\n"
     ]
    }
   ],
   "source": [
    "xh_output = nn.Linear(4,4, bias=False)\n",
    "hh_output = nn.Linear(4,4, bias=False)\n",
    "b_output = torch.randn(1,4).float()\n",
    "print(xh_output)\n",
    "print(hh_output)\n",
    "print(b_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we pass our input and hidden state through the gate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4017, -0.5760,  1.5157,  0.4986]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_gate = xh_candidate(input) + hh_candidate(hidden) + b_candidate\n",
    "output_gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and apply a sigmoid activation to the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4009, 0.3598, 0.8199, 0.6221]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O = torch.sigmoid(output_gate)\n",
    "O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to our input and hidden state, we also need to initialize an LSTM cell state. As with our hidden state, we can initialize by sampling from a standard normal distribution. We'll keep our length equal to 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6966,  0.5522, -0.5823, -1.0482]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_size = 4\n",
    "c = torch.randn(1, cell_size)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can update our LSTM cell state! Here's how we do it:\n",
    "\n",
    "1) Elementwise-multiply the output of forget gate __F__ by our prior cell state **c**\n",
    "\n",
    "2) Elementwise-multiply the output of input gate **I** by our transformed input __T__\n",
    "\n",
    "3) Add the two\n",
    "\n",
    "We'll update the value of our LSTM cell state **c** as we go along:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3207,  0.4265, -0.0726, -0.3077]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.1368, -0.2739,  0.6542,  0.2303]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.4574,  0.1526,  0.5816, -0.0773]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c_forget = F * c\n",
    "print(c_forget)\n",
    "c_transformed = I * T\n",
    "print(c_transformed)\n",
    "c = c_forget + c_transformed\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've initialized our LSTM cell state as a vector of zeroes, **c_forget** will also be a vector of zeroes, and __c__ = **c_transformed**.\n",
    "\n",
    "Updating our hidden state is as simple as applying **tanh** to our updated LSTM state and elementwise-multiplying this output by the output of our output gate __O__ (lol):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1716,  0.0545,  0.4295, -0.0480]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = O * torch.tanh(c)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very cool. PyTorch can make this much simpler! Let's take a look at how to build an LSTM in PyTorch. Check out the documentation for reference: https://pytorch.org/docs/master/nn.html#torch.nn.LSTM\n",
    "\n",
    "Let's define an **nn.LSTM** with input and hidden state vectors of length 4. Since our LSTM takes one layer by default, it's not necessary to define __num_layers = 1__, but we'll do it anyway for sake of example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(4, 4)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = nn.LSTM(input_size=4, hidden_size=4, num_layers=1)\n",
    "lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three dimensions to our input. Let's say we defined a *random* input. We could sample from a normal distribution with these arguments:\n",
    "\n",
    "1) **seq_len**: the number of time steps for our input. If we're just using our 'H' to predict 'E', we're only doing one time step. Let's stick with that.\n",
    "\n",
    "2) **batch**: the size of our batch of inputs. We only have one example here, so __batch = 1__\n",
    "\n",
    "3) **input_size = 4**, as with the argument we passed to __nn.LSTM__\n",
    "\n",
    "This is a solid explanation of what's going on here: https://stackoverflow.com/questions/45022734/understanding-a-simple-lstm-pytorch\n",
    "\n",
    "Here's what we'd get with random sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0367,  0.0207,  0.9240,  1.0845]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 4)\n",
    "print(input.size())\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we already have an input: our 'H' vector. Let's represent our 'H' as a tensor of size \\[1, 1, 4\\]. We will also have to cast this to a float (it is *long* by default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[[1,0,0,0]]]).float()\n",
    "print(input.size())\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize our hidden weights. Instead of using a zero vector, we'll again sample from a standard normal distribution. Our hidden vector is initialized randomly in three dimensions:\n",
    "\n",
    "1) **num_layers**: we'll stick with our simple LSTM with one hidden layer\n",
    "\n",
    "2) **batch**: again, we're only going to use one example\n",
    "\n",
    "3) **hidden_size = 4**, as with the argument passed to __nn.LSTM__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1794,  0.3520,  0.3231, -0.9658]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.randn(1, 1, 4)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up next is our LSTM state. This takes the same dimensions as our hidden layer. Again we'll stick with one layer, batch size of one, and **output_size** (instead of hidden_size) of 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1698,  0.7908, -0.5226, -1.8723]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.randn(1, 1, 4)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pass the outputs of these functions as arguments to our **lstm** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, (h, c) = lstm(input, (h, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine our output and updated hidden and LSTM cell states!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0395, -0.0341,  0.0680, -0.3077]]], grad_fn=<StackBackward>)\n",
      "tensor([[[ 0.0395, -0.0341,  0.0680, -0.3077]]], grad_fn=<StackBackward>)\n",
      "tensor([[[ 0.1224, -0.0679,  0.1146, -0.8863]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(h)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very cool. Note that our updated hidden state is equivalent to our output!\n",
    "\n",
    "We're going to take a look now at one more common type of RNN: the **gated recurrent unit (GRU)**. The GRU computes four vectors (see https://pytorch.org/docs/master/nn.html#torch.nn.LSTM):\n",
    "\n",
    "1) A *reset gate* vector **r**\n",
    "\n",
    "2) An *update gate* vector **z**\n",
    "\n",
    "3) A *\"new gate\"* vector **n**\n",
    "\n",
    "4) A *hidden state* vector **h**\n",
    "\n",
    "Let's go through each! PyTorch will automatically compute these gates, but it's good to know what's going on inside the GRU.\n",
    "\n",
    "Let's start with our reset gate. This takes an input vector (we'll reuse the **input** tensor from our LSTM example) and our hidden state as inputs. This means we'll have to do some more random initialization. Let's stick with the dimensions \\[num_layers=1, batch_size=1, hidden_size=4\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7185,  0.3538,  0.4405,  1.6638]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.randn(1, 1, 4)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to apply a linear transformation *with* bias. It's not necessary to specify __bias=True__ but we'll do it anyway to be explicit. We could do this by defining a weight matrix and multiplying the weight matrix by our initial hidden vector, or we can take a shortcut and use **nn.Linear** with four inputs and four outputs. We'll call this operation __xr__, since it takes our input (by convention, $x$) and generates an output necessary to compute our reset gate vector **r**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4, out_features=4, bias=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr = nn.Linear(4, 4, bias=True)\n",
    "xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to transform linearly (not splitting the infinitive this time) our hidden state. Let's call this linear transformation **hr**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4, out_features=4, bias=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr = nn.Linear(4, 4, bias=True)\n",
    "hr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get our reset gate **r**, we just need to sum our transformed input and hidden states and pass this sum through a sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4386, 0.4983, 0.1808, 0.7963]]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = torch.sigmoid(xr(input) + hr(h))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good stuff. We use identical operations to compute our update gate. We'll call our linear transformations **xz** and __hz__ since we're taking our inputs (**x, h**) and generating output **z**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=4, bias=True)\n",
      "Linear(in_features=4, out_features=4, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3397, 0.2081, 0.6019, 0.4117]]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xz = nn.Linear(4, 4, bias=True)\n",
    "print(xz)\n",
    "hz = nn.Linear(4, 4, bias=True)\n",
    "print(hz)\n",
    "z = torch.sigmoid(xz(input) + hz(h))\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll compute our \"new gate.\" Again, we'll linearly transform (with bias) our input vector and hidden state. Let's call these linear transformations **xn** and __hn__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=4, bias=True)\n",
      "Linear(in_features=4, out_features=4, bias=True)\n"
     ]
    }
   ],
   "source": [
    "xn = nn.Linear(4, 4, bias=True)\n",
    "print(xn)\n",
    "hn = nn.Linear(4, 4, bias=True)\n",
    "print(hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do something funky. We elementwise multiply our computed reset gate vector **r** by the output of our transformed hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1192,  0.1106, -0.1692,  0.7538]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = r * hn(h)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add this to our transformed input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4798,  0.3850,  0.0886,  0.7907]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = n + xn(input)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we use **n** and __z__, along with our current hidden state to generate a new hidden state. \n",
    "\n",
    "First we elementwise multiply **z** by our current hidden state and update this state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3397, 0.2081, 0.6019, 0.4117]]], grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5837,  0.0736,  0.2652,  0.6849]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(z)\n",
    "h = z * h\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we multiply (1 - z) by our transformed input **n** and add it to our updated h:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6603, 0.7919, 0.3981, 0.5883]]], grad_fn=<RsubBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9005,  0.3785,  0.3005,  1.1502]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(1 - z)\n",
    "h = (1 - z) * n + h\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. Now let's do this in a few lines of PyTorch code. First we'll define a GRU with arguments **seq_length**, __batch__, and **input_size**. We'll keep these identical to our LSTM (and what we've done in the above example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(4, 4)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru = nn.GRU(input_size=4, hidden_size=4, num_layers=1)\n",
    "gru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply pass our **input** and hidden state __h__ through the GRU. Let's initialize **h** again since we just updated it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3053,  0.2382,  1.5382, -1.3813]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.randn(1, 1, 4)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW let's pass our input and hidden state through the GRU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2184,  0.2073,  0.5252, -0.0426]]], grad_fn=<StackBackward>)\n",
      "tensor([[[-0.2184,  0.2073,  0.5252, -0.0426]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "output, h = gru(input, h)\n",
    "print(output)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our output and updated hidden state should be identical.\n",
    "\n",
    "Hopefully you'll now have a good idea how to construct RNNs in PyTorch, including LSTMs and GRUs. There's still a lot to cover, including practical applications and theoretical justifications for the design of particular gates.\n",
    "\n",
    "If you have any questions, comments, or suggestions, please contact Pat at pdonnelly@groupwaretech.com!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
