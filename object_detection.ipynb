{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Object Detection Architectures**\n",
    "\n",
    "In this notebook, we'll examine neural network architectures for object detection. We'll work through the examples in [`torchvision/models/detection`](https://github.com/pytorch/vision/tree/master/torchvision/models/detection)\n",
    "\n",
    "First up is Faster R-CNN, defined in `faster_rcnn.py`. The `FasterRCNN` class inherits properties from `GeneralizedRCNN`, defined in `generalized_rcnn.py`. Let's examine this file (it's short! 51 sloc). First let's do our imports: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine our `GeneralizedRCNN` class, which inherits from `nn.Module`. First we have our constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralizedRCNN(nn.Module):\n",
    "    def __init__(self, backbone, rpn, roi_heads, transform):\n",
    "        self.transform = transform\n",
    "        self.backbone = backbone\n",
    "        self.rpn = rpn\n",
    "        self.roi_heads = roi_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have a `forward` method that defines the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, images, targets=None):\n",
    "    if self.training and targets is None:\n",
    "        raise ValueError(\"In training mode, targets should be passed\")\n",
    "    original_image_sizes = [img.shape[-2:] for img in images]\n",
    "    images, targets = self.transform(images, targets)\n",
    "    features = self.backbone(images.tensors)\n",
    "    if isinstance(features, torch.Tensor):\n",
    "        features = OrderedDict([(0, features)])\n",
    "    proposals, proposal_losses = self.rpn(images, features, targets)\n",
    "    detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n",
    "    detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)\n",
    "    losses = {}\n",
    "    losses.update(detector_losses)\n",
    "    losses.update(proposal_losses)\n",
    "    if self.training:\n",
    "        return losses\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also examine this: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection\n",
    "\n",
    "`backbone` is a network used to classify objects within each region.\n",
    "\n",
    "`rpn` is a region proposal network. We use this to localize each object before applying our backbone to classify each localized object.\n",
    "\n",
    "`roi_heads` uses our classifications and localizations to compute detections and masks.\n",
    "\n",
    "`transform` transforms the input data.\n",
    "\n",
    "Let's see if we can now make sense of what's going on with `FasterRCNN`, starting with the class constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, backbone, num_classes=None, \n",
    "             min_size=800, max_size=1333,\n",
    "             image_mean=None, image_std=None,\n",
    "             rpn_anchor_generator=None, rpn_head=None,\n",
    "             rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000,\n",
    "             rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000,\n",
    "             rpn_nms_thresh=0.7,\n",
    "             rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3,\n",
    "             rpn_batch_size_per_image=256, rpn_positive_fraction=0.5,\n",
    "             box_roi_pool=None, box_head=None, box_predictor=None,\n",
    "             box_score_thresh=0.05, box_nms_thresh=0.5, box_detections_per_img=100,\n",
    "             box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5,\n",
    "             box_batch_size_per_image=512, box_positive_fraction=0.25,\n",
    "             bbox_reg_weights=None):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a ton of arguments. They're explained in `class FasterRCNN`:\n",
    "\n",
    "`backbone`: backbone network\n",
    "\n",
    "`num_classes`: number of classes to classify and subsequently detect\n",
    "\n",
    "`min_size`: minimum size of the image to be rescaled before feeding it to the backbone\n",
    "\n",
    "`max_size`: max size of the image to be rescaled before feeding it to the backbone\n",
    "\n",
    "We can also normalize our inputs using `image_mean` and `image_std`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next set of arguments are for the region proposal network.\n",
    "\n",
    "The Faster R-CNN paper explains the concept of \"anchors\":\n",
    "\n",
    "\"At each sliding window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is denoted as $k$ ... The $k$ proposals are parameterized relative to $k$ reference boxes, which we call *anchors* ... For a convolutional feature map of size $W \\cdot H$, there are $W \\cdot H \\cdot k$ anchors in total.\"\n",
    "\n",
    "`rpn_anchor_generator` generates anchors for a set of feature maps\n",
    "\n",
    "`rpn_head` computes the objectness and regression deltas from the RPN. What are those? Let's consider the definition of a Region Proposal Network from the paper:\n",
    "\n",
    "\"A Region Proposal Network (RPN) takes an image (of any size) as an input and outputs a set of rectangular object proposals, each with an objectness score ... 'Objectness measures membership to a set of object classes vs. background'\" \n",
    "\n",
    "In other words objectness tells us whether or not an object belongs to one of our classes. Regression loss refers to the loss associated with the difference between the predicted and ground truth bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to specify how many proposals to keep before and after applying \"non-maximum suppression\" (NMS) for our training and text sets. The default values are set to the following:\n",
    "\n",
    "`rpn_pre_nms_top_n_train = 2000` <br/>\n",
    "`rpn_pre_nms_top_n_test = 1000` <br/>\n",
    "`rpn_post_nms_top_n_train = 2000` <br/>\n",
    "`rpn_post_nms_top_n_test = 1000`\n",
    "\n",
    "We also have three thresholds:\n",
    "\n",
    "`rpn_nms_thresh`: threshold for applying NMS to the RPN proposals (default: 0.7)\n",
    "\n",
    "`rpn_fg_iou_thresh`: the overlap between the anchor and ground truth above which the box is classified as \"positive\" (default: 0.7)\n",
    "\n",
    "`rpn_bg_iou_thresh`: the overlap between the anchor and ground truth below which the box is classified as \"negative\" (default: 0.3)\n",
    "\n",
    "and two more RPN arguments:\n",
    "\n",
    "`rpn_batch_size_per_image`: \"number of anchors sampled during training for computing the loss\" (default: 256)\n",
    "\n",
    "`rpn_positive_fraction`: \"proportion of positive anchors in a mini-batch during training of the RPN\" (default: 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining arguments pertain to the (bounding) boxes:\n",
    "\n",
    "`box_roi_pool`: module for cropping and resizing feature maps according to locations determined by bounding boxes\n",
    "\n",
    "`box_head`: module that takes cropped feature maps and outputs to predictor\n",
    "\n",
    "`box_predictor`: module for computing classification and regression loss\n",
    "\n",
    "`box_score_thresh`: classification score threshold above which to return proposals during inference (default: 0.05)\n",
    "\n",
    "`box_nms_thresh`: NMS threshold for prediction head during inference (default: 0.5)\n",
    "\n",
    "`box_detections_per_img`: maximum detections per image (default: 100)\n",
    "\n",
    "`box_fg_iou_thresh`: minimum IoU between proposals and ground truth for positive classification (default: 0.5)\n",
    "\n",
    "`box_bg_iou_thresh`: maximum IoU between proposals and ground truth for negative classification (default: 0.5)\n",
    "\n",
    "`box_batch_size_per_image`: number of proposals to sample during training of classification head (default: 512)\n",
    "\n",
    "`box_positive_fraction`: \"proportion of positive proposals in a mini-batch during training of the classification head\" (default: 0.25)\n",
    "\n",
    "`bbox_reg_weights`: \"weights for encoding/decoding of bounding boxes\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
