{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommender Systems**\n",
    "\n",
    "Let's examine neural network architectures for recommender systems. We'll take a closer look at neural collaborative filtering (NCF). The code comes from NVIDIA's [Deep Learning Examples](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/NCF) repository and is written in PyTorch. Let's get into it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `README.md` gives a nice overview of the model architecture. There are \"two branches\" of the model. The \"Multi Layer Perceptron (MLP) branch... transforms the input through fully connected layers with ReLU activations and dropout.\" The \"Matrix Factorization (MF) branch... performs collaborative filtering factorization. Each use and each item has two embedding vectors associated with it: one for the MLP branch and the other for the MF branch. The outputs from these branches are concatenated and fed into the final fully connected layer with sigmoid activation. This can be interpreted as the probability of a user interacting with a given item.\"\n",
    "\n",
    "If this doesn't make sense just yet, hopefully it'll become clearer once we dive into the code. `ncf.py` looks like the logical next step. What are the functions in this file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parse_args` contains the command-line arguments necessary to train the model, along with their data type: string (`str`), integer (`int`), or 32-bit floating point (`float`) and (optionally) default value:\n",
    "\n",
    "`--data`: \"path to test and training data files\" (`str`). Presumably this is a path to a directory containing test files in one subdirectory and training files in another. From experience, our data in this directory needs to be in a particular structure or we'll get errors. There is no default value for `--data`. We need to pass a directory or our script will not train.\n",
    "\n",
    "`-e`, or `--epochs`: number of forward and backward passes through the full dataset (`int`, default: 30).\n",
    "\n",
    "`-b`, or `--batch_size`: number of examples in a training batch (`int`, default: $2^{20}$). That is an enormous batch size compared to what we typically see for e.g. convolutional neural networks.\n",
    "\n",
    "`--valid_batch_size`: number of examples in a validation batch (`int`, default: $2^{20}$).\n",
    "\n",
    "`-f`, or `--factors`: number of predictive factors (`int`, default: 64). This defines the size of the last layer of the NCF. The [paper](http://papers.www2017.com.au.s3-website-ap-southeast-2.amazonaws.com/proceedings/p173.pdf) introducing NCF notes that large factors may lead to overfitting and experiments with factors of size 8, 16, 32, and 64. We should be mindful of this when training the model.\n",
    "\n",
    "`--layers`: number of nodes per hidden layer in MLP. By default, this is a list of `int` data: `[256, 256, 128, 64]`.\n",
    "\n",
    "`-n`, or `--negative_samples`: To optimize the network, we use \"log loss with negative sampling\" (see the paper). This parameter tells the network how many negative examples to use for each iteration (forward and backward pass of a batch). Again, we pass our argument as an `int` (default: 4)\n",
    "\n",
    "`-l`, or `--learning_rate`: We update our parameters by the negative of our gradient, scaling by our learning rate (`float`, default: 0.0045).\n",
    "\n",
    "`-k`, or `--topk`: To evaluate our accuracy, we need to tell our recommender system how many examples to consider when applying it to our validation data. We set this to 10 by default (data type: `int`).\n",
    "\n",
    "`--seed`, or `-s`: sets a random seed. Not sure what this will be used for (`int`, default: 1). \n",
    "\n",
    "`--threshold`, or `-t`: sets early stopping for our training. Not sure how this works just yet (`float`, default: 1.0).\n",
    "\n",
    "`--beta1`, or `-b1`: When we use the Adam optimizer, we need to set two `beta` values. We use `beta1` to compute a decaying average of prior gradients. For details, there's a great [blog post](http://ruder.io/optimizing-gradient-descent/index.html#adam) on Adam and other optimizers (`float`, default: 0.25).\n",
    "\n",
    "`--beta2`, or `-b2`: Similar to `beta1`, we use `beta2` to compute a decaying average of prior squared gradients (`float`, default: 0.5).\n",
    "\n",
    "`--eps`: We also need a (very small) epsilon value for computing gradient updates with Adam (`float`, default: 1e-8).\n",
    "\n",
    "`--dropout`: randomly sets input parameters to zero with a given probability (`float`, default: 0.5).\n",
    "\n",
    "`--checkpoint_dir`: outputs the model to a specified directory (`str`, default: `/data/checkpoints/`)\n",
    "\n",
    "`--load_checkpoint_path`: loads a model and (presumably) initializes this model with parameters from the checkpoint (`str`, no default path) \n",
    "\n",
    "`--mode`: runs the script in either `train` mode (by which the model trains for the the number of `epochs` specified) or `test` mode (by which the model \"runs a single evaluation\" - presumably a full forward pass of all the test data?). The default is `train` and the type is a `str` (we only have our two options: `train` and `test`)\n",
    "\n",
    "`--grads_accumulated`: \"number of gradients to accumulate before performing an optimization step\" (`int`, default: 1)\n",
    "\n",
    "`--opt_level`: \"optimization level for automatic mixed precision\" (`str`, default: `02`, choices: `['00', '02']`)\n",
    "\n",
    "`--local_rank`: \"necessary for multi-GPU training\" (`int`, default: `0`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next function is `init_distributed`, which \"initializes distributed communication\" for GPUs. Let's not worry about the specific logic here. We want to examine the neural network itself.\n",
    "\n",
    "Looks like there are two more functions in `ncf.py`: `val_epoch` and `main`, and neither defines the network architecture. However, if we return to the `README.md`, the \"Advanced\" section notes that \"the model architecture is defined in `neumf.py`. Let's examine it.\n",
    "\n",
    "First we have our imports and a line defining `LOGGER.model = 'ncf'`. Let's comment out the line beginning with `sys.path.append` (if we run it in the block below, we get a `NameError: name '__file__' is not defined)`). `logger` resides in the same directory (`NCF`) as `neumf.py`, and thus we'll also comment out those lines - they won't run unless we clone the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "from os.path import abspath, join, dirname\n",
    "\n",
    "#sys.path.append(abspath(dirname(__file__) + '/'))\n",
    "\n",
    "#from logger.logger import LOGGER\n",
    "#from logger import tags\n",
    "\n",
    "#LOGGER.model = 'ncf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything else in `neumf.py` resides in the `NeuMF` class. Let's sketch the basic structure of the class and its functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, nb_users, nb_items,\n",
    "                 mf_dim, mlp_layer_sizes, dropout=0):\n",
    "        \n",
    "        # self methods go here\n",
    "        \n",
    "        def glorot_uniform(layer):\n",
    "            \n",
    "            # code to initialize with glorot uniform\n",
    "            \n",
    "            pass\n",
    "            \n",
    "        def lecunn_uniform(layer):\n",
    "        \n",
    "            # code to initialize with lecun uniform\n",
    "            \n",
    "            pass\n",
    "            \n",
    "    def forward(self, user, item, sigmoid=False):\n",
    "    \n",
    "        # defines forward propagation?\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build out this class. Our constructor takes `self` and five arguments: `nb_users`, `nb_items`, `mf_dim`, `mlp_layer_sizes`, and `dropout`. We also define a term `nb_mlp_layers` equal to the length of `mlp_layer_sizes`.\n",
    "\n",
    "Now we get into the network architecture! In lines 56-60, we see four `nn.Embedding` layers followed by `dropout` (zero by default). What is `nn.Embedding` exactly? If we go to the [documentation](https://pytorch.org/docs/stable/nn.html), search for \"Embedding,\" and click on `[SOURCE]`, we see that the `Embedding` class inherits from the general `Module` class and is defined in `torch.nn.modules.sparse` as \"a simple lookup table that stores embeddings of a fixed dictionary and size.\"\n",
    "\n",
    "Returning to the first layer of our `NeuMF` class, we see that `num_embeddings` - the size of our dictionary - is equal to the `nb_users` argument passed to the constructor. `embedding_dim` - the size of each embedding vector - is equal to `mf_dim`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus our architecture in lines 56-60 looks like this:\n",
    "\n",
    "1) A layer `mf_user_embed` that learns `nb_users` embeddings of size `mf_dim`\n",
    "\n",
    "2) A layer `mf_item_embed` that learns `nb_items` embeddings of size `mf_dim`\n",
    "\n",
    "3) A layer `mlp_user_embed` that learns `nb_users` embeddings of size `mlp_layer_sizes[0] // 2`\n",
    "\n",
    "4) A layer `mlp_item_embed` that learns `nb_items` embeddings of size `mlp_layer_sizes[0] // 2`\n",
    "\n",
    "5) An optional dropout layer (omitted by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a step back and examine other implementations of NCF. If we Google \"neural collaborative filtering pytorch,\" something like [this](https://github.com/yihong-chen/neural-collaborative-filtering) should come up. This is based off of the original NCF paper (He at al. 2017). The NCF authors implemented their model in TensorFlow with Keras. This repo is in PyTorch instead. Again, we'll examine `neumf.py`. The second class `NeuMFEngine` inherits from `Engine`, and thus won't run without importing from that `engine.py` file. We'll omit that and focus on the `NeuMF` class, analogous to the same class in the prior example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(NeuMF, self).__init__()\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        pass\n",
    "        \n",
    "    def init_weight(self):\n",
    "        pass\n",
    "    \n",
    "    def load_pretrain_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our constructor `__init__` uses the `config` argument to define a set of variables:\n",
    "\n",
    "`num_users`: the number of embeddings we need to learn for `embedding_user_mlp` (multi-layer perceptron) and `embedding_user_mf` (matrix factorization). Presumably this is equal to the number of users, and we need to learn one embedding per user?\n",
    "\n",
    "`num_items`: Just as for users, we learn `mlp` and `mf` embeddings for each item.\n",
    "\n",
    "`latent_dim_mf`: We also need to specify the \"latent dimension\" of the `mf` embeddings vectors - in other words, the size of these vectors.\n",
    "\n",
    "`latent_dim_mlp`: Just as for the `mf` embeddings, we need to define the latent dimension (size) of the `mlp` vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fc_layers` is set to `torch.nn.ModuleList()`. We then append `nn.Linear` layers of `in_size` input size and `out_size` output size according to the `layers` defined in `config`.\n",
    "\n",
    "We then have an `affine_output` layer that concatenates the output of the final layer in `layers` with `latent_dim_mf`, and returns one output feature.\n",
    "\n",
    "Finally, we define a `logistic` layer as `torch.nn.Sigmoid()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forward` defines how we pass our data through the network. We have data for users (`user_indices`) and items (`item_indices`), and perform the following operations:\n",
    "\n",
    "1) Apply our `mlp` and `mf` embeddings to each set of indices. \n",
    "\n",
    "2) Concatenate our `mlp` embeddings for users and items (into `mlp_vector`) and do the same for our `mf` embeddings (concatenating these into `mf_vector`).\n",
    "\n",
    "3) Add additional layers (with `ReLU` activation) to `mlp_vector` as defined in `fc_layers`.\n",
    "\n",
    "4) Concatenate `mlp_vector` and `mf_vector`.\n",
    "\n",
    "5) Apply a linear transformation to the output of 4) as defined in `affine_output`.\n",
    "\n",
    "6) Pass the output of 5) through a sigmoid activation function and return this as our rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's another function for loading pretrained weights, but we've covered the model architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
