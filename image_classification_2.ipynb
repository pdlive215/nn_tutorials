{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMAGE CLASSIFICATION WITH PYTORCH: ADDITIONAL MODEL ARCHITECTURES**\n",
    "\n",
    "*Patrick Donnelly*\n",
    "\n",
    "In our \"Convolutional Neural Network Architectures for Image Classification\" tutorial, we covered some of the most common algorithms for classifying images. This included AlexNet, VGG, ResNet (and ResNeXt), and Inception v3. In this tutorial, we cover the remaining architectures in the PyTorch Model Zoo (`torchvision.models`): SqueezeNet, DenseNet, GoogLeNet, Shufflenet v2, and MobileNet v2. Source code is available at https://github.com/pytorch/vision/tree/master/torchvision/models. Let's get into it!\n",
    "\n",
    "First up is **SqueezeNet** (Iandola et al. 2016). The motivation behind SqueezeNet is simple: to attain the accuracy of a canonical deep neural network for image classification (e.g. AlexNet) with far (50x) fewer parameters. You can view the paper here: https://arxiv.org/pdf/1602.07360.pdf\n",
    "\n",
    "The first innovation of SqueezeNet we'll examine is the *Fire module*. We'll use this module to construct our convolutional neural network (CNN). Iandola et al (2016, p. 3). identify three \"strategies\" to build a more \"efficient\" CNN using the Fire module:\n",
    "\n",
    "1) \"**Replace 3x3 filters with 1x1 filters.**\" This is straightforward: a 3x3 filter has nine weights, while a 1x1 filter only has a single weight. Thus we learn nine times fewer weights per filter.\n",
    "\n",
    "2) \"**Decrease the number of input channels to 3x3 filters.**\" Fewer input channels means fewer parameters to learn. Our parameters learned is equivalent to our number of inputs multiplied by the number of filters learned multiplied by the dimension of the filters (in this case 3x3). The first strategy of SqueezeNet was to reduce the filter size. This strategy involves reducing the input. Logically speaking, there's one other thing we could do! What about our third strategy?\n",
    "\n",
    "3) \"**Downsample late in the network so that convolution layers have larger activation maps**.\" Well, they're not suggesting we reduce our number of output channels... At the end of the day, we need our output channels to equal the number of potential classes from which we're predicting an output. However, there are many, many, many ways in which we can downsample from our input (determined by our data) to our output (determined by our number of classes). The authors of the paper hypothesize that retaining larger activation maps will lead to higher accuracy, as they put it, \"with all else held equal.\" This seems kinda obvious. The more interesting idea might be whether the increased accuracy from larger activation maps can sufficiently compensate for the decreased accuracy of smaller input channels to 3x3 filters, along with replacing 3x3 filters with 1x1 filters. You can import the model from torchvision and find out for yourself!\n",
    "\n",
    "We've gone long enough without working in code, so let's go ahead and sketch out our SqueezeNet. The source code is available at https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py. The torchvision source code includes two versions of SqueezeNet: 1.0 and 1.1. We'll use version 1.1. First let's do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # we need PyTorch, of course\n",
    "import torch.nn as nn  # allowing us to use nn as shorthand for torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define our class and give it a constructor and a `forward` method (similar to what we've seen with some of our other CNNs). Our `SqueezeNet` class will inherit from `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=1000):  # for ImageNet\n",
    "        super(SqueezeNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.features = features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be filling out `self.features` with the layers of our neural network, and then applying these features to our input in `forward`. But first let's define a separate Fire module class! This will also take a constructor and a `forward` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fire(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Fire, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the paper (p. 4). What goes into our Fire module?\n",
    "\n",
    "1) \"A *squeeze* convolution layer\" of 1x1 filters\n",
    "\n",
    "2) \"An *expand* layer\" with both 1x1 and 3x3 filters\n",
    "\n",
    "Our Fire module also includes three *hyperparameters*: parameters that govern how we learn our model parameters:\n",
    "\n",
    "1) $s_{1x1}$: the number of (1x1) filters in the squeze layer\n",
    "\n",
    "2) $e_{1x1}$: the number of 1x1 filters in the expand layer\n",
    "\n",
    "3) (you guessed it) $e_{3x3}$: the number of 3x3 filters in the expand layer\n",
    "\n",
    "We'll also construct our network such that $s_{1x1}$ < $e_{1x1} + e_{3x3}$, since we want relatively few input channels to our 3x3 filters (strategy 2) in our introductory blurb)\n",
    "\n",
    "Our Fire module thus takes the following parameters:\n",
    "\n",
    "1) `inplanes`: the number of inputs to the squeeze layer\n",
    "\n",
    "2) `squeeze_planes`: the number of 1x1 filters to learn in the squeeze layer\n",
    "\n",
    "3) `expand1x1_planes`: the number of 1x1 filters to learn in the expand layer\n",
    "\n",
    "4) `expand3x3_planes`: the number of 3x3 filters to learn in the expand layer\n",
    "\n",
    "Let's start filling out our Fire module!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fire(nn.Module):\n",
    "    \n",
    "    def __init__(self, inplanes, squeeze_planes, \n",
    "                 expand1x1_planes, expand3x3_planes):\n",
    "        super(Fire, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our arguments to define several class variables:\n",
    "\n",
    "1) `self.inplanes`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
