{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning through Operations**\n",
    "\n",
    "In this notebook, we'll take a look at the operations necessary to train neural networks. We'll implement everything in PyTorch and explicate each implementation as we go along. We'll start with CPU implementations; these ops generally have corresponding implementations for GPUs and other devices.\n",
    "\n",
    "First we need to import PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define an input tensor `x`, a ground truth output tensor `y`, and a model output tensor `Y`. We arbitrarily set `x` equal to a randomly-sampled floating point number in the interval `[0, 1)` and `y` equal to `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9971])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1)\n",
    "print(x)\n",
    "y = torch.tensor([1])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set the model output `Y` to the input `x` plus a constant `b`. As with `x`, we arbitrarily set `b` equal to a randomly-sampled floating point number in the interval `[0, 1)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8354])\n",
      "tensor([1.8325])\n"
     ]
    }
   ],
   "source": [
    "b = torch.rand(1)\n",
    "print(b)\n",
    "Y = x.add(b)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pause and examine the `tensor` objects we've created. `tensor` is defined in `pytorch/c10/core`. In this directory, we see a bunch of `.h` and `.cpp` files that begin with `Tensor`. Header files define `tensor` attributes:\n",
    "\n",
    "`TensorImpl.h`: implementation <br/>\n",
    "`TensorOptions.h`: options <br/>\n",
    "`TensorTypeId.h`: type IDs (e.g. for CPU, CUDA, XLA) <br/>\n",
    "`TensorTypeSet.h`: \"a representation of a set of TensorTypeIds\"\n",
    "\n",
    "Each of these has functions defined in a corresponding C++ file. We can call the attributes `dtype`, `device`, and `layout` on each tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32, torch.int64, torch.float32, torch.float32\n",
      "cpu, cpu, cpu, cpu\n",
      "torch.strided, torch.strided, torch.strided, torch.strided\n"
     ]
    }
   ],
   "source": [
    "print(x.dtype, y.dtype, b.dtype, Y.dtype, sep=', ')\n",
    "print(x.device, y.device, b.device, Y.device, sep=', ')\n",
    "print(x.layout, y.layout, b.layout, Y.layout, sep=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply unary operations to each tensor. In `pytorch/aten/src/native`, in `UnaryOps.h`, `DECLARE_DISPATCH` defines a `_stub` for each unary op within the `at::native` namespace. In the same directory, `UnaryOps.cpp` implements each of these operations. Some of these implementations are explicitly defined in this file; some are defined elsewhere; all have an `IMPLEMENT_UNARY_OP_VEC` and `DEFINE_DISPATCH` in this file. Let's apply these unary ops to our input `x`. Some ops (e.g. `angle`, `real`) will not work on `x`; we omit these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9971])\n",
      "tensor([0.0757])\n",
      "tensor([1.4951])\n",
      "tensor([0.7840])\n",
      "tensor([0.5427])\n",
      "tensor([1.5397])\n",
      "tensor([0.8415])\n",
      "tensor([0.1585])\n",
      "tensor([2.1086])\n",
      "tensor([2.7105])\n",
      "tensor([0.9971])\n",
      "tensor([0.6917])\n",
      "tensor([-0.0041])\n",
      "tensor([1.0029])\n",
      "tensor([0.7305])\n",
      "tensor([0.8399])\n",
      "tensor([1.1708])\n",
      "tensor([0.9986])\n",
      "tensor([1.5476])\n",
      "tensor([0.7604])\n",
      "tensor([0.0017])\n"
     ]
    }
   ],
   "source": [
    "print(x.abs())\n",
    "print(x.acos())\n",
    "print(x.asin())\n",
    "print(x.atan())\n",
    "print(x.cos())\n",
    "print(x.cosh())\n",
    "print(x.erf())\n",
    "print(x.erfc())\n",
    "print(x.erfinv())\n",
    "print(x.exp())\n",
    "print(x.frac())\n",
    "print(x.log1p())\n",
    "print(x.log2())\n",
    "print(x.reciprocal())\n",
    "print(x.sigmoid())\n",
    "print(x.sin())\n",
    "print(x.sinh())\n",
    "print(x.sqrt())\n",
    "print(x.tan())\n",
    "print(x.tanh())\n",
    "print(x.lgamma())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply binary operations to multiple tensors. Binary ops are defined (natively) in `pytorch/aten/src/ATen/native`, in `BinaryOps.h` and `BinaryOps.cpp`. Some unary and binary ops are still defined in the legacy `pytorch/aten/src/TH` directory, in the (awkwardly titled) files `THTensorMath.cpp`, `THTensorMoreMath.cpp`, and `THTensorEvenMoreMath.cpp`. We won't worry about these for now.\n",
    "\n",
    "Let's call our (native) binary ops on `x` and `b`. We can do this either by calling the binary op as a method of `x` and passing the argument `b`, or by passing `x` and `b` as arguments of the `torch` method. For instance, we could call `x.add(b)` or `torch.add(x,b)`. Let's stick with the former since it's more concise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8325])\n",
      "tensor([0.1618])\n",
      "tensor([0.8330])\n",
      "tensor([1.1937])\n",
      "tensor([0.8735])\n"
     ]
    }
   ],
   "source": [
    "print(x.add(b))\n",
    "print(x.sub(b))\n",
    "print(x.mul(b))\n",
    "print(x.div(b))\n",
    "print(x.atan2(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike unary ops, many binary ops in PyTorch are still defined in the legacy `TH` directory. However, we do see our operation `x.add(b)` that we used to add `b` to `x` to produce our model output `Y`.\n",
    "\n",
    "To understand how PyTorch computes `x.add(b)`, we have to navigate to `pytorch/aten/src/ATen/native/cpu`. This directory contains (among other \"kernels\", including `UnaryOpsKernel.cpp`) our `BinaryOpsKernel.cpp` file. Here we see the `void` function `add_kernel` calls *another* function `vec256::fmadd`, defined in `pytorch/aten/src/ATen/cpu/vec256`, in `vec256_base.h`, which returns `a * b + c` for `const` inputs `a`, `b`, and `c`. Each of these inputs references `typename T`, defined as a generic template in `vec256_base.h`, and defined for other data types in (e.g. for floating point vectors) `vec256_float.h` and `vec256_double.h`. For our simple example (adding two `float32` vectors of length 1), `x.add(b)` is equivalent to `x + b`.\n",
    "\n",
    "What if we want to compare the loss between our actual output `y` and our (predicted) model output `Y`? PyTorch has several loss functions defined in `Loss.cpp`. We call these as methods of `torch` and pass `y` and `Y` as arguments. For each of these, we pass our model output `Y` first and our *target* \"actual\" output `y` second. Some of these loss functions require more than two inputs; we omit these here. Another (`binary_cross_entropy_with_logits`) requires us to cast our target to a `float`; we do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8325])\n",
      "tensor([0.1484])\n"
     ]
    }
   ],
   "source": [
    "print(torch.hinge_embedding_loss(Y, y))\n",
    "print(torch.binary_cross_entropy_with_logits(Y, y.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our optional arguments `weight` and `pos_weight` are not defined, `loss` is defined as `(1 - target).mul_(input).add_(max_val).add_((-max_val).exp_().add_((-input -max_val).exp_()).log_())`. We then call `apply_loss_reduction` (defined in the same `Loss.cpp` file) on `loss` and `reduction` (we also do not explicitly pass `reduction` as an argument here).\n",
    "\n",
    "To update our *bias* parameter `b`, we first need to compute our *gradient*. This is simply the partial derivative of our loss function with respect to our input.\n",
    "\n",
    "To illustrate this, let's re-intialize `b` with the argument `requires_grad=True`. This tells PyTorch to allocate memory for `b`'s gradient and compute it when we backpropagate. We don't need to specify `requires_grad=True` for `Y`. PyTorch will automatically define a `grad_fn` equal to `<AddBackward0>` since `Y = x.add(b)`. This `grad_fn` changes with the function input. For instance, if we define `Y = x.mul(b)`, our `grad_fn` becomes `<MulBackward0>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2604])\n",
      "tensor([0.9283], requires_grad=True)\n",
      "tensor([1.1887], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "Y = x.add(b)\n",
    "print(x)\n",
    "print(b)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check our gradient, we'll find that it haven't been computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute our loss using `binary_cross_entropy_with_logits`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2659], grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.binary_cross_entropy_with_logits(Y, y.float())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute gradients by calling `backward` on `loss`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that our gradient has updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2335])\n"
     ]
    }
   ],
   "source": [
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this gradient to update our value of `b`, most simply by scaling the gradient by a *learning rate* and subtracting this value from the prior value of `b`.\n",
    "\n",
    "Now let's make our function *slightly* more complex. We'll add a *weight* `w` such that `y = w * x + b`. We'll also initialize `w` by randomly sampling over the interval `[0, 1)`. Again, we'll assume that our target value `y` is `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2901])\n",
      "tensor([0.5289])\n",
      "tensor([0.7278], requires_grad=True)\n",
      "tensor([0.8812], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1)\n",
    "w = torch.rand(1)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "Y = x.mul(w).add(b)\n",
    "print(x)\n",
    "print(w)\n",
    "print(b)\n",
    "print(Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
