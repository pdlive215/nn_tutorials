{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation functions**\n",
    "\n",
    "As we did with `optimizers`, let's examine the logic of activation functions and their effect on model loss and accuracy. We'll use our simple four-layer CNN trained on MNIST and swap in different functions for our default `relu` activations. This network is optimized with `SGD`, which we've found isn't a great optimizer for the task (better to use SGD with some form of momentum). But maybe we'll get more variation in performance across activation functions with higher loss and lower accuracy? \n",
    "\n",
    "There are a ton of activation functions defined in `torch.nn` and `torch.nn.functional`. We'll use `nn.functional` and follow the order of the functions in the [documentation](https://pytorch.org/docs/stable/nn.functional.html). For some of these activations, we have the option of running the operation in-place by adding a `_`, or by passing `inplace=True` as an argument. We won't worry about that here. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Threshold** replaces our input with a `value` below a specified `threshold`. There are no defaults for `threshold` or `value`. Let's arbitarily set our `threshold` at 1 and set the rest to -1. Thus we'll end up with a modified (and discontinuous) version of `relu`. Kinda weird, but it's not the best activation function. Let's examine performance:\n",
    "\n",
    "`Epoch       Loss       Accuracy\n",
    "   1         .1607      .9526\n",
    "   2         .1033      .9690\n",
    "   3         .0777      .9763\n",
    "   4         .0598      .9814\n",
    "   5         .0669      .9794\n",
    "   6         .0516      .9836\n",
    "   7         .0490      .9846\n",
    "   8         .0473      .9852\n",
    "   9         .0435      .9864\n",
    "   10        .0463      .9861`\n",
    "   \n",
    "Not great initially, but not terrible for an odd function with somewhat arbitrary arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReLU** is simply $max(0, x)$, zeroing out the negative values of the input. We've already tested this with `SGD` optimizer in the `optimizers` notebook. We can enter the values here:\n",
    "\n",
    "Loss:\n",
    "\n",
    "`Epoch       Threshold(-1,1)       ReLU  \n",
    "   1          .1607                .1017\n",
    "   2          .1033                .0614\n",
    "   3          .0777                .0562\n",
    "   4          .0598                .0409\n",
    "   5          .0669                .0384\n",
    "   6          .0516                .0336\n",
    "   7          .0490                .0343\n",
    "   8          .0473                .0391\n",
    "   9          .0435                .0288\n",
    "   10         .0463                .0314`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "`Epoch       Threshold(-1,1)       ReLU  \n",
    "   1          .9526                .9669\n",
    "   2          .9690                .9828\n",
    "   3          .9763                .9809\n",
    "   4          .9814                .9864\n",
    "   5          .9794                .9873\n",
    "   6          .9836                .9893\n",
    "   7          .9846                .9876\n",
    "   8          .9852                .9878\n",
    "   9          .9864                .9909\n",
    "   10         .9861                .9895`\n",
    "   \n",
    "ReLU is pretty good, and it clearly outperforms the threshold function (though `threshold` is getting close by epoch 10!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hardtanh** is a (very) coarse approximation of `tanh`, by which $x=1$ if $x>1$ and $x=-1$ if $x<-1$. Otherwise $x$ remains $x$.\n",
    "\n",
    "Loss:\n",
    "\n",
    "`Epoch     Threshold(-1,1)    ReLU    Hardtanh  \n",
    "   1        .1607            .1017    .2441\n",
    "   2        .1033            .0614    .1427\n",
    "   3        .0777            .0562    .1027\n",
    "   4        .0598            .0409    .0820\n",
    "   5        .0669            .0384    .0698\n",
    "   6        .0516            .0336    .0610\n",
    "   7        .0490            .0343    .0551\n",
    "   8        .0473            .0391    .0515\n",
    "   9        .0435            .0288    .0471\n",
    "   10       .0463            .0314    .0458`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "`Epoch     Threshold(-1,1)    ReLU    Hardtanh  \n",
    "   1        .9526            .9669    .9320\n",
    "   2        .9690            .9828    .9598\n",
    "   3        .9763            .9809    .9706\n",
    "   4        .9814            .9864    .9770\n",
    "   5        .9794            .9873    .9798\n",
    "   6        .9836            .9893    .9827\n",
    "   7        .9846            .9876    .9839\n",
    "   8        .9852            .9878    .9850\n",
    "   9        .9864            .9909    .9864\n",
    "   10       .9861            .9895    .9862`\n",
    "   \n",
    "Loss and accuracy for `hardtanh` isn't even as good at the threshold function for eight out of 10 epochs, though it slightly outperforms `threshold` by the end of training. ReLU remains the best option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReLU6** caps the ReLU output at 6, computing $min(max(0,x),6)$\n",
    "\n",
    "Loss:\n",
    "\n",
    "`Epoch     Threshold    ReLU    Hardtanh   ReLU6\n",
    "   1        .1607       .1017    .2441     .1694   \n",
    "   2        .1033       .0614    .1427     .0989\n",
    "   3        .0777       .0562    .1027     .0710\n",
    "   4        .0598       .0409    .0820     .0571\n",
    "   5        .0669       .0384    .0698     .0545\n",
    "   6        .0516       .0336    .0610     .0424\n",
    "   7        .0490       .0343    .0551     .0421\n",
    "   8        .0473       .0391    .0515     .0416\n",
    "   9        .0435       .0288    .0471     .0342\n",
    "   10       .0463       .0314    .0458     .0383`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "`Epoch     Threshold    ReLU    Hardtanh   ReLU6  \n",
    "   1        .9526       .9669    .9320     .9497\n",
    "   2        .9690       .9828    .9598     .9721\n",
    "   3        .9763       .9809    .9706     .9778\n",
    "   4        .9814       .9864    .9770     .9822\n",
    "   5        .9794       .9873    .9798     .9828\n",
    "   6        .9836       .9893    .9827     .9866\n",
    "   7        .9846       .9876    .9839     .9853\n",
    "   8        .9852       .9878    .9850     .9864\n",
    "   9        .9864       .9909    .9864     .9888\n",
    "   10       .9861       .9895    .9862     .9869`\n",
    "\n",
    "ReLU (not ReLU6) still looks like our best bet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ELU** takes the minimum of 0 and x, rather than the maximum (as with ReLU):\n",
    "\n",
    "$ELU(x) = max(0,x) + min(0, \\alpha * (exp(x) - 1))$\n",
    "\n",
    "Loss:\n",
    "\n",
    "`Epoch     Threshold    ReLU    Hardtanh   ReLU6     ELU\n",
    "   1        .1607       .1017    .2441     .1694    .1603\n",
    "   2        .1033       .0614    .1427     .0989    .0928\n",
    "   3        .0777       .0562    .1027     .0710    .0689\n",
    "   4        .0598       .0409    .0820     .0571    .0548\n",
    "   5        .0669       .0384    .0698     .0545    .0585\n",
    "   6        .0516       .0336    .0610     .0424    .0417\n",
    "   7        .0490       .0343    .0551     .0421    .0411\n",
    "   8        .0473       .0391    .0515     .0416    .0449\n",
    "   9        .0435       .0288    .0471     .0342    .0333\n",
    "   10       .0463       .0314    .0458     .0383    .0379`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "`Epoch     Threshold    ReLU    Hardtanh   ReLU6     ELU  \n",
    "   1        .9526       .9669    .9320     .9497    .9528\n",
    "   2        .9690       .9828    .9598     .9721    .9744\n",
    "   3        .9763       .9809    .9706     .9778    .9782\n",
    "   4        .9814       .9864    .9770     .9822    .9827\n",
    "   5        .9794       .9873    .9798     .9828    .9816\n",
    "   6        .9836       .9893    .9827     .9866    .9868\n",
    "   7        .9846       .9876    .9839     .9853    .9858\n",
    "   8        .9852       .9878    .9850     .9864    .9850\n",
    "   9        .9864       .9909    .9864     .9888    .9898\n",
    "   10       .9861       .9895    .9862     .9869    .9863`\n",
    "   \n",
    "ELU is a solid second or third, but ReLU is still best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SELU** is computed as $scale * (max(0,x) + min(0, \\alpha * (exp(x) - 1)))$\n",
    "\n",
    "Here $\\alpha = 1.6732632423543772848170429916717$\n",
    "and $scale = 1.0507009873554804934193349852946$\n",
    "\n",
    "Loss:\n",
    "\n",
    "`Ep  Threshold   ReLU    Hardtanh   ReLU6     ELU      SELU\n",
    " 1    .1607      .1017    .2441     .1694    .1603    .1389\n",
    " 2    .1033      .0614    .1427     .0989    .0928    .0780\n",
    " 3    .0777      .0562    .1027     .0710    .0689    .0607\n",
    " 4    .0598      .0409    .0820     .0571    .0548    .0496\n",
    " 5    .0669      .0384    .0698     .0545    .0585    .0574\n",
    " 6    .0516      .0336    .0610     .0424    .0417    .0393\n",
    " 7    .0490      .0343    .0551     .0421    .0411    .0389\n",
    " 8    .0473      .0391    .0515     .0416    .0449    .0446\n",
    " 9    .0435      .0288    .0471     .0342    .0333    .0328\n",
    " 10   .0463      .0314    .0458     .0383    .0379    .0361`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "`Ep  Threshold    ReLU    Hardtanh   ReLU6     ELU      SELU\n",
    " 1    .9526       .9669    .9320     .9497    .9528    .9591\n",
    " 2    .9690       .9828    .9598     .9721    .9744    .9777\n",
    " 3    .9763       .9809    .9706     .9778    .9782    .9806\n",
    " 4    .9814       .9864    .9770     .9822    .9827    .9840\n",
    " 5    .9794       .9873    .9798     .9828    .9816    .9810\n",
    " 6    .9836       .9893    .9827     .9866    .9868    .9878\n",
    " 7    .9846       .9876    .9839     .9853    .9858    .9869\n",
    " 8    .9852       .9878    .9850     .9864    .9850    .9853\n",
    " 9    .9864       .9909    .9864     .9888    .9898    .9903\n",
    " 10   .9861       .9895    .9862     .9869    .9863    .9874`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CELU** is defined as $max(0,x) + min(0, \\alpha*(exp(x/a)-1))$, and confusingly rhymes with SELU?\n",
    "\n",
    "Loss:\n",
    "\n",
    "`Ep  Thld  ReLU    Hrdtnh   ReLU6    ELU     SELU    CELU\n",
    " 1   .161  .1017   .2441    .1694   .1603   .1389   .1603\n",
    " 2   .103  .0614   .1427    .0989   .0928   .0780   .0928\n",
    " 3   .078  .0562   .1027    .0710   .0689   .0607   .0689\n",
    " 4   .060  .0409   .0820    .0571   .0548   .0496   .0548\n",
    " 5   .067  .0384   .0698    .0545   .0585   .0574   .0585\n",
    " 6   .052  .0336   .0610    .0424   .0417   .0393   .0417\n",
    " 7   .049  .0343   .0551    .0421   .0411   .0389   .0411\n",
    " 8   .047  .0391   .0515    .0416   .0449   .0446   .0449\n",
    " 9   .044  .0288   .0471    .0342   .0333   .0328   .0333\n",
    " 10  .046  .0314   .0458    .0383   .0379   .0361   .0379`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "`Ep  Thld  ReLU    Hrdtnh   ReLU6    ELU     SELU    CELU\n",
    " 1   .953  .9669   .9320    .9497   .9528   .9591   .9528\n",
    " 2   .969  .9828   .9598    .9721   .9744   .9777   .9744\n",
    " 3   .976  .9809   .9706    .9778   .9782   .9806   .9782\n",
    " 4   .981  .9864   .9770    .9822   .9827   .9840   .9827\n",
    " 5   .979  .9873   .9798    .9828   .9816   .9810   .9816\n",
    " 6   .984  .9893   .9827    .9866   .9868   .9878   .9868\n",
    " 7   .985  .9876   .9839    .9853   .9858   .9869   .9858\n",
    " 8   .985  .9878   .9850    .9864   .9850   .9853   .9850\n",
    " 9   .986  .9909   .9864    .9888   .9898   .9903   .9898\n",
    " 10  .986  .9895   .9862    .9869   .9863   .9874   .9863`\n",
    " \n",
    "Still haven't found an activation function that can beat ReLU.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leaky ReLU** augments ReLU by adding a small slope (default: 0.01) to negative values, yielding $max(0,x) + negative_slope * min(0,x)$ for input $x$\n",
    "\n",
    "We'll record Leaky ReLU and the activation function with the top accuracy (ReLU) to four digits and the rest to three. (ReLU is winning at every epoch.)\n",
    "\n",
    "Loss:\n",
    "\n",
    "`Ep  Thld   ReLU   Hdtnh  ReLU6  ELU  SELU  CELU   LRLU\n",
    " 1   .161  .1017   .244   .169  .160  .139  .160  .1679\n",
    " 2   .103  .0614   .143   .099  .093  .078  .093  .1001\n",
    " 3   .078  .0562   .103   .071  .069  .061  .069  .0729\n",
    " 4   .060  .0409   .082   .057  .055  .050  .055  .0584\n",
    " 5   .067  .0384   .070   .055  .059  .057  .059  .0612\n",
    " 6   .052  .0336   .061   .043  .042  .039  .042  .0451\n",
    " 7   .049  .0343   .055   .042  .041  .039  .041  .0438\n",
    " 8   .047  .0391   .052   .042  .045  .045  .045  .0465\n",
    " 9   .044  .0288   .047   .034  .033  .033  .033  .0347\n",
    " 10  .046  .0314   .046   .038  .038  .036  .038  .0398`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "`Ep  Thld   ReLU   Hdtnh  ReLU6  ELU  SELU  CELU   LRLU\n",
    " 1   .953  .9669   .932   .950  .953  .959  .953  .9504\n",
    " 2   .969  .9828   .960   .972  .974  .978  .974  .9710\n",
    " 3   .976  .9809   .971   .978  .978  .981  .978  .9772\n",
    " 4   .981  .9864   .977   .982  .983  .984  .983  .9821\n",
    " 5   .979  .9873   .980   .983  .982  .981  .982  .9812\n",
    " 6   .984  .9893   .983   .987  .987  .988  .987  .9854\n",
    " 7   .985  .9876   .984   .985  .986  .987  .986  .9848\n",
    " 8   .985  .9878   .985   .986  .985  .985  .985  .9846\n",
    " 9   .986  .9909   .986   .989  .990  .990  .990  .9888\n",
    " 10  .986  .9895   .986   .987  .986  .987  .986  .9862`\n",
    " \n",
    "Intuitively, it seems that Leaky ReLU, if not an improvement over ReLu, should at least be close. This test suggests otherwise - it's generally worse than the other ReLU variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PReLU** modifies Leaky ReLU, replacing the constant *negative_slope* with a learnable *weight* parameter. This isn't entirely straightforward to implement, and there is no suggested default for the initial weight in the `nn.functional.prelu` implementation, so we'll skip this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RReLU** is Leaky ReLU, but with a randomized slope for $x<0$, uniformly sampled with lower bound 0.125 and upper bound 0.333.\n",
    "\n",
    "Let's simplify it further, and reduce everything but the top accuracy to two digits (except for `RReLU`)\n",
    "\n",
    "Loss:\n",
    "\n",
    "`Ep  Thld  ReLU  Htnh  RLU6 ELU  SELU CELU LRLU RReLU\n",
    " 1   .16  .1017  .24   .17  .16  .14  .16  .17  .1730\n",
    " 2   .10  .0614  .14   .10  .09  .08  .09  .10  .1027\n",
    " 3   .08  .0562  .10   .07  .07  .06  .07  .07  .0746\n",
    " 4   .06  .0409  .08   .06  .06  .05  .06  .06  .0594\n",
    " 5   .07  .0384  .07   .06  .06  .06  .06  .06  .0627\n",
    " 6   .05  .0336  .06   .04  .04  .04  .04  .05  .0452\n",
    " 7   .05  .0343  .06   .04  .04  .04  .04  .04  .0450\n",
    " 8   .05  .0391  .05   .04  .05  .05  .05  .05  .0475\n",
    " 9   .04  .0288  .05   .03  .03  .03  .03  .04  .0351\n",
    " 10  .05  .0314  .05   .04  .04  .04  .03  .04  .0434`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "`Ep  Thld  ReLU  Htnh  RLU6 ELU  SELU CELU LRLU RReLU\n",
    " 1   .95  .9669  .93   .95  .95  .96  .95  .95  .9495\n",
    " 2   .97  .9828  .96   .97  .97  .98  .97  .97  .9704\n",
    " 3   .98  .9809  .97   .98  .98  .98  .98  .98  .9773\n",
    " 4   .98  .9864  .98   .98  .98  .98  .98  .98  .9815\n",
    " 5   .98  .9873  .98   .98  .98  .98  .98  .98  .9806\n",
    " 6   .98  .9893  .98   .99  .99  .99  .99  .99  .9855\n",
    " 7   .99  .9876  .98   .99  .99  .99  .99  .99  .9838\n",
    " 8   .99  .9878  .99   .99  .99  .99  .99  .99  .9848\n",
    " 9   .99  .9909  .99   .99  .99  .99  .99  .99  .9888\n",
    " 10  .99  .9895  .99   .99  .99  .99  .99  .99  .9853`\n",
    " \n",
    "The results are similar to Leaky ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GLU** is a \"gated linear unit.\" We split the input in half into $a$ and $b$, pass $b$ through a sigmoid function $\\sigma$, and compute the elementwise product of $a$ and $\\sigma(b)$. We'll skip this one because it doesn't work with our network's given input size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GeLU** is the product of the input $x$ and the cumulative distribution function of $x$.\n",
    "\n",
    "For the rest of the activation functions, we'll just compare to the top performing function (as of now, it's all ReLU). We'll also accelerate things by training for just five epochs.\n",
    "\n",
    "`Loss                          Accuracy`\n",
    "\n",
    "`Ep    ReLU     GeLU           ReLU     GeLU\n",
    " 1    .1017    .1567          .9669    .9538\n",
    " 2    .0614    .0930          .9828    .9739\n",
    " 3    .0562    .0682          .9809    .9787\n",
    " 4    .0409    .0558          .9864    .9826\n",
    " 5    .0384    .0589          .9873    .9818`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log Sigmoid** is the (natural) log of the sigmoid function: $\\log(\\frac{1}{1+\\exp{-x_{i}}})$\n",
    "\n",
    "`Loss                          Accuracy`\n",
    "\n",
    "`Ep    ReLU   LogSigmoid       ReLU   LogSigmoid\n",
    " 1    .1017    .3961          .9669    .8748\n",
    " 2    .0614    .2659          .9828    .9198\n",
    " 3    .0562    .2245          .9809    .9364\n",
    " 4    .0409    .1799          .9864    .9421\n",
    " 5    .0384    .1465          .9873    .9570`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hardshrink** uses the parameter $\\lambda$ (default: 0.5) and returns the input $x$ if $x > |\\lambda|$, 0 otherwise.\n",
    "\n",
    "If this or any subsequent function doesn't come close to ReLU after an epoch, we'll keep moving on.\n",
    "\n",
    "`Loss                          Accuracy`\n",
    "\n",
    "`Ep    ReLU   Hardshrink       ReLU   Hardshrink\n",
    " 1    .1017    .2039          .9669    .9415`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tanhshrink** is simply $x - \\tanh(x)$ for input $x$\n",
    "\n",
    "`Loss                          Accuracy`\n",
    "\n",
    "`Ep    ReLU   Tanhshrink       ReLU   Tanhshrink\n",
    " 1    .1017    2.3013          .9669   .1135`\n",
    " \n",
    "I'm fairly certain `tanhshrink` isn't supposed to plug into this network, at least not without a change to the output of the prior layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softsign** is $\\frac{x}{1+|x|}$ for input $x$\n",
    "\n",
    "`Loss                          Accuracy`\n",
    "\n",
    "`Ep    ReLU   Softsign         ReLU   Softsign\n",
    " 1    .1017    .5148          .9669    .8710`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softplus** \"is a smooth approximation of the ReLU function\" (`nn` documentation), defined as follows:\n",
    "\n",
    "Softplus$(x) = \\frac{1}{\\beta} * \\log(1 + exp(\\beta * x))$\n",
    "\n",
    "By default, $\\beta=1$. Above a threshold (default: 20), \"the implementation reverts to the linear function\" (does this mean it just returns $x$?)\n",
    "\n",
    "`Loss                          Accuracy`\n",
    "\n",
    "`Ep    ReLU   Softplus         ReLU   Softplus\n",
    " 1    .1017    .2312          .9669    .9291`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmin** is equal to softmax(-$x$) for input $x$, or $\\frac{\\exp(-x_{i})}{\\sum_{j}\\exp(-x_{j})}$. The elements of the softmin and softmax tensors are all in the range $[0,1]$ and sum to 1.\n",
    "\n",
    "`Loss                          Accuracy`\n",
    "\n",
    "`Ep    ReLU   Softmin          ReLU   Softmin\n",
    " 1    .1017   2.3013           .9669   0.1135`\n",
    " \n",
    "The loss and accuracy after one epoch is identical to that in `tanhshrink`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax** is $\\frac{\\exp(x_{i})}{\\sum_{j}\\exp(x_{j})}$ (see Softmin)\n",
    "\n",
    "`Loss                          Accuracy`\n",
    "\n",
    "`Ep    ReLU   Softmax          ReLU   Softmax\n",
    " 1    .1017   2.3013           .9669   .1135`\n",
    " \n",
    "The network appears simply not to train with some of these activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softshrink** is $x - \\lambda$ if $x > \\lambda$, $x + \\lambda$, if $x < -\\lambda$, and 0 otherwise.\n",
    "\n",
    "`Loss                          Accuracy`\n",
    "\n",
    "`Ep    ReLU   Softshrink       ReLU   Softshrink\n",
    " 1    .1017    2.3013         .9669    .1135`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gumbel Softmax** \"samples from the Gumbel-Softmax distribution\" (link in `nn.functional` documentation).\n",
    "\n",
    "This one also doesn't work for `main.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log Softmax** computes the natural logarithm of the softmax function.\n",
    "\n",
    "This one not only doesn't work for `main.py` - it returns `nan` loss early on in the first epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tanh** is the hyperbolic tangent of the input.\n",
    "\n",
    "`Loss                          Accuracy`\n",
    "\n",
    "`Ep    ReLU    Tanh            ReLU    Tanh\n",
    " 1    .1017   .3013           .9669   .9164`\n",
    " \n",
    "Ah finally, the network is training again, but still not up to par with ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid** (saving the tried and true for last) is defined in the description of Log Sigmoid: $\\frac{1}{1+\\exp{-x_{i}}}$\n",
    "\n",
    "`Loss                          Accuracy`\n",
    "\n",
    "`Ep    ReLU   Sigmoid          ReLU   Sigmoid\n",
    " 1    .1017   2.3040          .9669   .0982`\n",
    " \n",
    "Even sigmoid doesn't work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew! That's a lot of activation functions. All that work to confirm that ReLU is awesome (for simple CNNs at least). We didn't get into their use for different tasks - perhaps ReLU wouldn't have been so dominant if we weren't just running tests on a simple convolutional neural network. That's for another time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
