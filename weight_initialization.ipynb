{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weight Initialization**\n",
    "\n",
    "Let's continue our exploration of PyTorch operations with weight initialization. We'll stick with our simple four-layer CNN which we'll use to train MNIST. The network is defined in our `Net` class from `pytorch/examples/mnist/main.py`. To run this, we need to import `torch` and its `nn` and `nn.functional` modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and run our `Net` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing data through our network, we need to initialize our weights and biases. How is this done? Let's examine `nn.Conv2d`. This is defined in `nn/modules/conv.py`. The `Conv2d` class inherits from `_ConvNd`. Here, we see that parameters are initialized using the `reset_parameters` method, which uses `init.kaiming_uniform_` to initialize weights and `init.uniform_` to initialize biases.\n",
    "\n",
    "As for `nn.Linear2d`, this is defined in `nn/modules/linear.py`. We see what appears to be the exact same `reset_parameters` method. This uses Kaiming uniform initialization for weights with `a=math.sqrt(5)`, and uniform initialization for biases with upper and lower bound defined by `1 / math.sqrt(fan_in)`, and `fan_in` defined by the `_calculate_fan_in_and_fan_out` function in `init`.\n",
    "\n",
    "But let's say we want to initialize our weights manually. How do we do this? Let's first consider our `conv1` layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(1, 20, 5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to see something cool? Calling `nn.Conv2d` already initialized our weights and biases. Let's examine our weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0669,  0.1622,  0.1756, -0.0422,  0.1894],\n",
       "          [-0.0522,  0.0733,  0.0707,  0.0096, -0.0305],\n",
       "          [-0.0752, -0.1516,  0.0352, -0.0037,  0.1162],\n",
       "          [-0.0378,  0.1643,  0.0995,  0.0105, -0.0450],\n",
       "          [-0.1489,  0.0070, -0.1296,  0.0568, -0.0617]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0550,  0.1872, -0.1869, -0.0354,  0.1116],\n",
       "          [-0.1696,  0.0501, -0.1834,  0.0462, -0.0881],\n",
       "          [-0.1783, -0.1509, -0.1834, -0.0580, -0.1496],\n",
       "          [ 0.1374, -0.0101,  0.0193,  0.1987, -0.1191],\n",
       "          [ 0.0914, -0.0171, -0.1371, -0.0735,  0.1027]]],\n",
       "\n",
       "\n",
       "        [[[-0.0061,  0.0697,  0.1853,  0.0829,  0.0110],\n",
       "          [ 0.1133,  0.1665, -0.1011, -0.1990,  0.0659],\n",
       "          [ 0.0577, -0.0833, -0.0402,  0.1506, -0.1789],\n",
       "          [-0.0279, -0.1408,  0.0101,  0.0709, -0.1537],\n",
       "          [-0.1510, -0.0629,  0.0632, -0.0332, -0.0309]]],\n",
       "\n",
       "\n",
       "        [[[-0.1224, -0.0527,  0.0092, -0.1067,  0.1646],\n",
       "          [-0.1782,  0.1241, -0.0065, -0.0518, -0.0702],\n",
       "          [ 0.1868, -0.1593, -0.1373, -0.1070,  0.1788],\n",
       "          [ 0.0186,  0.1986, -0.0909,  0.0406, -0.1669],\n",
       "          [-0.1289,  0.1373,  0.0134,  0.0194,  0.1892]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0478, -0.1673, -0.0549,  0.1076, -0.0561],\n",
       "          [-0.0852,  0.0563,  0.1534,  0.0855, -0.0317],\n",
       "          [-0.1864,  0.1290, -0.1913,  0.0085, -0.1684],\n",
       "          [ 0.1512, -0.0034,  0.1710, -0.0326,  0.1596],\n",
       "          [ 0.1947, -0.1024,  0.1311, -0.0205, -0.0453]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0880,  0.1457,  0.1473, -0.1960, -0.1232],\n",
       "          [ 0.1573,  0.1884, -0.1566, -0.1309, -0.1963],\n",
       "          [ 0.0875, -0.1528, -0.1808, -0.0448,  0.1908],\n",
       "          [-0.1419,  0.0522,  0.1969, -0.1847, -0.0120],\n",
       "          [-0.1276, -0.0604, -0.0092,  0.0712, -0.0284]]],\n",
       "\n",
       "\n",
       "        [[[-0.1537,  0.1860,  0.1071, -0.0677,  0.0654],\n",
       "          [-0.0397, -0.1572,  0.0327,  0.0144,  0.0661],\n",
       "          [-0.1993, -0.0784, -0.1298, -0.0518,  0.0651],\n",
       "          [-0.0739,  0.0726,  0.1334, -0.1595, -0.1705],\n",
       "          [ 0.1631,  0.0120,  0.0690, -0.1199, -0.1362]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0156,  0.0136,  0.0139, -0.1306, -0.1245],\n",
       "          [ 0.0102,  0.1912,  0.0710,  0.0700, -0.0241],\n",
       "          [ 0.0225, -0.0921,  0.0561, -0.1013, -0.0042],\n",
       "          [-0.1366,  0.0057,  0.1186,  0.0412, -0.0062],\n",
       "          [-0.1281,  0.1844, -0.1115,  0.1380, -0.0319]]],\n",
       "\n",
       "\n",
       "        [[[-0.0211,  0.0890, -0.0128, -0.1538,  0.0076],\n",
       "          [ 0.0229, -0.1321,  0.1316,  0.1340,  0.1459],\n",
       "          [ 0.1466,  0.1068,  0.1781,  0.1031, -0.1156],\n",
       "          [ 0.0523, -0.0251, -0.1335,  0.0826,  0.1809],\n",
       "          [-0.0846, -0.0340,  0.0181, -0.1790,  0.0165]]],\n",
       "\n",
       "\n",
       "        [[[-0.1240,  0.1653,  0.0318, -0.0191, -0.0255],\n",
       "          [ 0.1299,  0.0798,  0.1460,  0.0185, -0.0103],\n",
       "          [-0.0139,  0.1232,  0.0407,  0.1779,  0.1943],\n",
       "          [-0.0974, -0.1350,  0.0098, -0.0165, -0.0408],\n",
       "          [ 0.1469,  0.0576,  0.0148, -0.0225, -0.0089]]],\n",
       "\n",
       "\n",
       "        [[[-0.0030,  0.1082, -0.1690, -0.1562,  0.0976],\n",
       "          [ 0.1883,  0.1701, -0.0617,  0.0339, -0.1611],\n",
       "          [-0.0125, -0.0763, -0.1488,  0.1570,  0.1544],\n",
       "          [ 0.0490, -0.1012,  0.0026, -0.1873, -0.0450],\n",
       "          [ 0.0581, -0.1502,  0.1924, -0.1004,  0.1980]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0521,  0.0252, -0.1084,  0.1153, -0.0523],\n",
       "          [-0.0853,  0.1953,  0.0422,  0.0093, -0.1726],\n",
       "          [ 0.0533,  0.1875, -0.0119,  0.0027, -0.0185],\n",
       "          [-0.1392,  0.1795, -0.1728,  0.1872,  0.1954],\n",
       "          [-0.0563,  0.0579, -0.1848,  0.1766,  0.0205]]],\n",
       "\n",
       "\n",
       "        [[[-0.0334,  0.0885, -0.0456,  0.0165, -0.0178],\n",
       "          [ 0.1152, -0.0128, -0.1355,  0.1420, -0.0438],\n",
       "          [-0.1591,  0.0583,  0.1344, -0.1395, -0.0149],\n",
       "          [-0.0649,  0.0451, -0.0665, -0.1808,  0.1531],\n",
       "          [ 0.0197,  0.1750,  0.0173, -0.1699, -0.0563]]],\n",
       "\n",
       "\n",
       "        [[[-0.1560, -0.1298,  0.0371, -0.0719, -0.1383],\n",
       "          [ 0.0735, -0.0225, -0.0329,  0.1210, -0.0549],\n",
       "          [ 0.0123, -0.0310,  0.0118,  0.1975, -0.0434],\n",
       "          [-0.0678, -0.0844, -0.0125, -0.0474,  0.0243],\n",
       "          [ 0.1431,  0.1309,  0.1889, -0.0514,  0.0955]]],\n",
       "\n",
       "\n",
       "        [[[-0.0383, -0.0077, -0.1107,  0.0641, -0.0055],\n",
       "          [-0.1727, -0.1008, -0.0500,  0.0019,  0.1743],\n",
       "          [-0.1749,  0.0129, -0.0818,  0.1158, -0.0721],\n",
       "          [ 0.1708,  0.0119,  0.0363,  0.1618, -0.1008],\n",
       "          [ 0.0606,  0.0814, -0.1521, -0.0356, -0.0850]]],\n",
       "\n",
       "\n",
       "        [[[-0.1340,  0.0112, -0.0893, -0.0852, -0.1669],\n",
       "          [-0.1630, -0.0343, -0.0838, -0.0406, -0.1271],\n",
       "          [ 0.1585, -0.0697,  0.1916, -0.0409, -0.0559],\n",
       "          [ 0.0066, -0.1212, -0.0792, -0.0845,  0.1939],\n",
       "          [-0.1514,  0.0265, -0.0850,  0.0412,  0.0685]]],\n",
       "\n",
       "\n",
       "        [[[-0.0234,  0.0620, -0.1000, -0.1414, -0.0547],\n",
       "          [-0.0093,  0.0885,  0.0205, -0.0461,  0.1836],\n",
       "          [-0.0246, -0.1633,  0.1485, -0.0472, -0.1051],\n",
       "          [-0.0008,  0.0202,  0.0093,  0.1746, -0.1072],\n",
       "          [-0.1597,  0.1408,  0.0815,  0.0785, -0.0906]]],\n",
       "\n",
       "\n",
       "        [[[-0.1357, -0.1193, -0.0184,  0.1330, -0.1043],\n",
       "          [ 0.0269, -0.1047, -0.0043,  0.0887, -0.1294],\n",
       "          [-0.0257, -0.0654,  0.0190, -0.0847,  0.0855],\n",
       "          [ 0.0161, -0.0976,  0.0341,  0.1427,  0.1636],\n",
       "          [ 0.1096,  0.1947, -0.0586, -0.0174,  0.1700]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1668,  0.1245, -0.0412,  0.0373,  0.1628],\n",
       "          [-0.1285,  0.0140,  0.0470,  0.0824,  0.1812],\n",
       "          [ 0.0282, -0.1655, -0.0601,  0.1535,  0.1810],\n",
       "          [-0.0567, -0.1702,  0.0957,  0.0307,  0.0687],\n",
       "          [ 0.1865, -0.0205,  0.1508,  0.0950, -0.1195]]],\n",
       "\n",
       "\n",
       "        [[[-0.0869, -0.0566, -0.0505,  0.1457,  0.1594],\n",
       "          [ 0.1276,  0.1026, -0.1636, -0.1491, -0.0610],\n",
       "          [ 0.1184,  0.0013,  0.1132, -0.1974, -0.1154],\n",
       "          [-0.0318,  0.1799,  0.0656, -0.0103,  0.0885],\n",
       "          [-0.0031, -0.0496, -0.1079,  0.0956, -0.1706]]]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our initialization creates a weight tensor of 20 5x5 filters. If we call `conv1.weight.shape`, we see these three dimensions, along with a fourth dimension (batch size of 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 5, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our bias vector adds a constant value to each of the 20 channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1917,  0.0543,  0.0215, -0.1588, -0.1407,  0.0339,  0.1019, -0.1864,\n",
       "        -0.1028,  0.1651,  0.1361,  0.0833, -0.1999,  0.1216,  0.0081,  0.0904,\n",
       "        -0.1882, -0.0160, -0.0861, -0.1152], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of this vector is just its length. There are no other dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we train our network with default `kaiming_uniform` initialization, we get the following average loss and accuracy on `examples/mnist/main.py`. We'll just do five epochs. It's a relatively simple classification task, so we get fairly quick convergence:\n",
    "\n",
    "`Epoch     Average Loss           Accuracy\n",
    "   1          .1017                .9669\n",
    "   2          .0614                .9828\n",
    "   3          .0562                .9809\n",
    "   4          .0409                .9864\n",
    "   5          .0384                .9873`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to change the initialization of our two convolutional and two linear layers? We can use a function from `torch.nn.init`. For example, what if we were to initialize everything with zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pp/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.constant(conv1.weight, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this changes all of our weights to zeros. What if we were to do this to all of the weights and biases in each of our layers? We could define our layers in a list:\n",
    "\n",
    "`layers = [self.conv1, self.conv2, self.fc1, self.fc2]`\n",
    "\n",
    "and then loop over this list:\n",
    "\n",
    "`for layer in layers:\n",
    "     torch.nn.init.constant_(layer.weight, 0)\n",
    "     torch.nn.init.constant_(layer.bias, 0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, our network doesn't learn anything with zero initialization. Our average loss is constant at 2.3010, and our accuracy is .1135. Our network is built to classify one of 10 classes, so this isn't much better than randomly guessing. What if we changed our initialization from zeros to ones?\n",
    "\n",
    "`for layer in layers:\n",
    "     torch.nn.init.constant_(layer.weight, 1)\n",
    "     torch.nn.init.constant_(layer.bias, 1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't help matters - we're still stuck with a network that isn't learning. There are actually functions, `ones_` and `zeros_` that do constant initialization with these values, but neither helps us here. We run into a different problem with `normal` initialization, where our loss quickly goes to `nan`.\n",
    "\n",
    "What if our biases are interfering with our training process? We could try initializing our biases to zero instead. Let's see if we can get our network to train with weights initialized to one and biases initialized to zero:\n",
    "\n",
    "`for layer in layers:\n",
    "     torch.nn.init.ones_(layer.weight)\n",
    "     torch.nn.init.zeros_(layer.bias)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't appear to work either, nor does it work with weights uniformly sampled from the default range (0, 1), nor with weights sampled from the normal distribution (mean: 0, standard devation: 1.0). At least for our simple convolutional network, we need something better. PyTorch gives us a few options that will work for our `nn.Conv2d` layers: `xavier_uniform_`, `xavier_normal_`, `kaiming_uniform_` (the one we previously used to initialize weights), `kaiming_normal_`, and `orthogonal_`. We'll ignore `sparse`, which requires us to set a fraction of our data to zero. \n",
    "\n",
    "Let's start with the definition of `xavier_uniform`. This samples our weights uniformly from $(-a, a)$ generated by the following distribution, optionally scaled by a `gain` (set by default to 1.0):\n",
    "\n",
    "$a = \\sqrt{\\frac{6}{fan\\_in + fan\\_out}}$\n",
    "\n",
    "where `fan_in` is the number of inputs and `fan_out` is the number of outputs. To examine this, we'll use the `xavier_uniform` distribution to initialize our `conv1` weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pp/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0942, -0.0483, -0.0113,  0.0327,  0.0735],\n",
       "          [ 0.0142, -0.0276, -0.0245,  0.0523,  0.0204],\n",
       "          [ 0.1066,  0.0465, -0.0645, -0.0937,  0.1053],\n",
       "          [ 0.0195, -0.0727,  0.0543,  0.0214, -0.0776],\n",
       "          [ 0.0332,  0.0985,  0.0767, -0.0131, -0.0736]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0087, -0.0006, -0.0852, -0.0240, -0.0907],\n",
       "          [ 0.0076,  0.0365,  0.0887, -0.0775,  0.0775],\n",
       "          [-0.0799,  0.0880, -0.0429, -0.0929,  0.0177],\n",
       "          [-0.0411, -0.0939,  0.0384,  0.0185,  0.0767],\n",
       "          [ 0.0339, -0.0310,  0.1029,  0.0458,  0.0971]]],\n",
       "\n",
       "\n",
       "        [[[-0.0427,  0.0389,  0.0045, -0.0134,  0.0908],\n",
       "          [-0.0892,  0.0697,  0.0492,  0.0887,  0.0481],\n",
       "          [ 0.1003,  0.1027, -0.0305,  0.0905, -0.0963],\n",
       "          [-0.0206, -0.0073, -0.0381,  0.0650, -0.0942],\n",
       "          [-0.1059,  0.0646, -0.0283, -0.1051, -0.0370]]],\n",
       "\n",
       "\n",
       "        [[[-0.0484, -0.1037,  0.0298,  0.0819, -0.0948],\n",
       "          [ 0.0238,  0.0971,  0.0405,  0.0370,  0.0016],\n",
       "          [-0.0733, -0.0890,  0.0848, -0.0877,  0.0734],\n",
       "          [-0.0734,  0.1050,  0.1006, -0.0319,  0.0269],\n",
       "          [ 0.0207, -0.0390,  0.0972, -0.0496,  0.0542]]],\n",
       "\n",
       "\n",
       "        [[[-0.0465,  0.0491,  0.1001, -0.0619,  0.0986],\n",
       "          [-0.0756, -0.0164,  0.0057, -0.0532, -0.0013],\n",
       "          [-0.0485,  0.0211,  0.0197, -0.0608,  0.0322],\n",
       "          [ 0.0978,  0.0520, -0.0310,  0.0745,  0.0910],\n",
       "          [ 0.0870,  0.0136, -0.0669,  0.0788,  0.0979]]],\n",
       "\n",
       "\n",
       "        [[[-0.1030, -0.0160, -0.0441,  0.0130, -0.0680],\n",
       "          [ 0.0548, -0.0057,  0.0578,  0.0171, -0.0793],\n",
       "          [ 0.0013, -0.0923,  0.0481, -0.0551, -0.0703],\n",
       "          [-0.0969,  0.1040, -0.0468, -0.0999,  0.0620],\n",
       "          [-0.1063,  0.0166,  0.0718, -0.1045, -0.0314]]],\n",
       "\n",
       "\n",
       "        [[[-0.0253,  0.0663,  0.0531, -0.1006, -0.0842],\n",
       "          [ 0.0432,  0.0134,  0.0707, -0.0913,  0.0295],\n",
       "          [-0.0330, -0.0244, -0.0013,  0.0262,  0.0504],\n",
       "          [ 0.0566, -0.0471,  0.0642, -0.0977, -0.0810],\n",
       "          [ 0.0359, -0.0063, -0.0043,  0.0118, -0.0321]]],\n",
       "\n",
       "\n",
       "        [[[-0.0996, -0.1034,  0.0779, -0.0722, -0.0125],\n",
       "          [ 0.0663, -0.0530,  0.0646, -0.0841, -0.0878],\n",
       "          [ 0.0188,  0.0612, -0.0573,  0.0047, -0.0969],\n",
       "          [ 0.0390,  0.0369,  0.0701, -0.0536,  0.0995],\n",
       "          [ 0.0789, -0.0320, -0.0680, -0.0685,  0.0771]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1002, -0.0456,  0.0257, -0.0944,  0.0906],\n",
       "          [ 0.1062, -0.0383, -0.0695,  0.0574,  0.1012],\n",
       "          [-0.0980,  0.0830, -0.0943,  0.0398, -0.0372],\n",
       "          [ 0.0305, -0.0230,  0.0739, -0.0086, -0.0566],\n",
       "          [-0.1067,  0.0096, -0.0667,  0.0212,  0.0739]]],\n",
       "\n",
       "\n",
       "        [[[-0.0817,  0.0998, -0.0842, -0.0305, -0.0577],\n",
       "          [ 0.0413, -0.0634, -0.0852, -0.0711, -0.0352],\n",
       "          [ 0.0725, -0.0923,  0.0090, -0.0938,  0.0035],\n",
       "          [-0.0896, -0.0808, -0.0425, -0.0855,  0.0045],\n",
       "          [ 0.0373, -0.0042, -0.0460,  0.0650, -0.0828]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0807,  0.0222,  0.0872,  0.0750, -0.0340],\n",
       "          [-0.0276, -0.0165,  0.0480, -0.0091,  0.0619],\n",
       "          [-0.0570, -0.0312,  0.0952,  0.0816, -0.0356],\n",
       "          [-0.0752, -0.0807, -0.0920,  0.0921,  0.0345],\n",
       "          [ 0.0753,  0.0758,  0.0368,  0.0400,  0.0263]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0046,  0.0124, -0.0667,  0.0131,  0.0154],\n",
       "          [-0.0166, -0.0393,  0.0192, -0.0056,  0.0545],\n",
       "          [ 0.0287,  0.0014,  0.0863, -0.0575, -0.0411],\n",
       "          [-0.0927, -0.0412,  0.0037, -0.0098,  0.0107],\n",
       "          [-0.0923, -0.0315, -0.0719, -0.0853,  0.1011]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0429,  0.0293,  0.0545, -0.0372,  0.0132],\n",
       "          [-0.0156,  0.0290, -0.0467, -0.0093,  0.0602],\n",
       "          [ 0.0835,  0.0378, -0.0137, -0.0182,  0.0147],\n",
       "          [ 0.0049, -0.0713, -0.0059,  0.0026, -0.0032],\n",
       "          [-0.1056,  0.0513,  0.0452,  0.0721,  0.0445]]],\n",
       "\n",
       "\n",
       "        [[[-0.0266,  0.0974, -0.0829, -0.1068, -0.0463],\n",
       "          [ 0.0943,  0.0153, -0.0949,  0.0228,  0.0796],\n",
       "          [ 0.0571,  0.0964, -0.0301,  0.0305, -0.0874],\n",
       "          [-0.0810, -0.0511, -0.0229, -0.0922,  0.0889],\n",
       "          [ 0.0802,  0.0898,  0.0119,  0.0274, -0.1018]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0409, -0.0315, -0.0549,  0.1022,  0.0729],\n",
       "          [-0.0849,  0.0353,  0.0239,  0.0876, -0.0237],\n",
       "          [-0.0511,  0.0752, -0.0698, -0.0454,  0.0177],\n",
       "          [ 0.0098, -0.0123, -0.0440, -0.0722, -0.0208],\n",
       "          [ 0.1046, -0.0676, -0.0422,  0.0973, -0.0254]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0329,  0.0293, -0.1014,  0.0090,  0.0107],\n",
       "          [ 0.0789,  0.0480, -0.0335,  0.0386, -0.0464],\n",
       "          [ 0.0601, -0.0791, -0.0340,  0.0381,  0.0061],\n",
       "          [ 0.0433,  0.0605, -0.0748, -0.0310, -0.0850],\n",
       "          [-0.0824, -0.0665,  0.0618, -0.0235, -0.0445]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0976, -0.0506,  0.0135, -0.0930, -0.1069],\n",
       "          [ 0.0290, -0.0729,  0.0536, -0.0797,  0.1025],\n",
       "          [-0.0161, -0.0589, -0.0174, -0.0770,  0.0659],\n",
       "          [ 0.0373,  0.0538, -0.0908, -0.0130,  0.1014],\n",
       "          [ 0.0699,  0.0415, -0.0558, -0.0976, -0.0211]]],\n",
       "\n",
       "\n",
       "        [[[-0.0738,  0.0896,  0.0002,  0.0008, -0.1027],\n",
       "          [-0.0788, -0.1015,  0.0914,  0.0028,  0.0112],\n",
       "          [-0.0671, -0.0955, -0.0460, -0.0402, -0.0062],\n",
       "          [-0.0468, -0.0342, -0.0138, -0.0014, -0.1013],\n",
       "          [ 0.0875, -0.0576,  0.0944, -0.0898,  0.0194]]],\n",
       "\n",
       "\n",
       "        [[[-0.0014,  0.0879,  0.0061,  0.0505, -0.0577],\n",
       "          [ 0.1045,  0.0198,  0.0842,  0.1033, -0.0965],\n",
       "          [ 0.0346, -0.0181,  0.0444,  0.0245,  0.0412],\n",
       "          [-0.0136, -0.0148, -0.0188,  0.0053, -0.0584],\n",
       "          [-0.0731,  0.0695,  0.0395,  0.0358,  0.0745]]],\n",
       "\n",
       "\n",
       "        [[[-0.0117, -0.0509,  0.0842,  0.0484, -0.0662],\n",
       "          [ 0.0384, -0.0345,  0.0233,  0.0326, -0.0163],\n",
       "          [-0.0292,  0.0605, -0.0782,  0.0065, -0.0665],\n",
       "          [ 0.0574, -0.0876,  0.0086,  0.0915,  0.0483],\n",
       "          [-0.0333, -0.0847, -0.0484,  0.0734, -0.0825]]]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform(conv1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the mean value of our weights is much smaller than when using prior distributions (except for `normal`, which could potentially initialize closer to zero). For completeness, we'll also include Kaiming and orthogonal initializers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean initialization\n",
      "Zeros: 0.0\n",
      "Ones: 1.0\n",
      "Uniform: 0.49642595648765564\n",
      "Normal: 0.007640771102160215\n",
      "Xavier uniform: -0.002580041531473398\n",
      "Xavier normal: 0.001514861243776977\n",
      "Kaiming uniform: -0.011375376023352146\n",
      "Kaiming normal: -0.00296521233394742\n",
      "Orthogonal: 0.006012782920151949\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean initialization\")\n",
    "torch.nn.init.zeros_(conv1.weight)\n",
    "print (\"Zeros: \" + str(torch.mean(conv1.weight).item()))\n",
    "torch.nn.init.ones_(conv1.weight)\n",
    "print (\"Ones: \" + str(torch.mean(conv1.weight).item()))\n",
    "torch.nn.init.uniform_(conv1.weight)\n",
    "print (\"Uniform: \" + str(torch.mean(conv1.weight).item()))\n",
    "torch.nn.init.normal_(conv1.weight)\n",
    "print (\"Normal: \" + str(torch.mean(conv1.weight).item()))\n",
    "torch.nn.init.xavier_uniform_(conv1.weight)\n",
    "print (\"Xavier uniform: \" + str(torch.mean(conv1.weight).item()))\n",
    "torch.nn.init.xavier_normal_(conv1.weight)\n",
    "print (\"Xavier normal: \" + str(torch.mean(conv1.weight).item()))\n",
    "torch.nn.init.kaiming_uniform_(conv1.weight)\n",
    "print (\"Kaiming uniform: \" + str(torch.mean(conv1.weight).item()))\n",
    "torch.nn.init.kaiming_normal_(conv1.weight)\n",
    "print (\"Kaiming normal: \" + str(torch.mean(conv1.weight).item()))\n",
    "torch.nn.init.orthogonal_(conv1.weight)\n",
    "print (\"Orthogonal: \" + str(torch.mean(conv1.weight).item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's record average loss and accuracy for our Xavier uniform initialization. You can compare this with the \"initial\" Kaiming uniform initialization, but we'll do a separate run with zero initialized biases so that we get a stricter comparison across different forms of weight initialization:\n",
    "\n",
    "`Epoch     Average Loss           Accuracy\n",
    "   1          .0745                .9781\n",
    "   2          .0587                .9817\n",
    "   3          .0434                .9859\n",
    "   4          .0446                .9852\n",
    "   5          .0346                .9884`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually better than what we got with Kaiming uniform. Let's compare now with `xavier_normal_`. Here we sample from a normal distribution with a mean of 0 and a variance of $\\sigma^{2}$, where\n",
    "\n",
    "$\\sigma = \\sqrt \\frac{2}{fan\\_in + fan\\_out}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare average loss:\n",
    "\n",
    "`Epoch     Xavier uniform       Xavier normal\n",
    "   1          .0745                .0921\n",
    "   2          .0587                .0702\n",
    "   3          .0434                .0478\n",
    "   4          .0446                .0414\n",
    "   5          .0346                .0423`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and accuracy:\n",
    "\n",
    "`Epoch     Xavier uniform       Xavier normal\n",
    "   1          .9781                .9717\n",
    "   2          .9817                .9773\n",
    "   3          .9859                .9848\n",
    "   4          .9852                .9859\n",
    "   5          .9884                .9884`\n",
    "   \n",
    "Our loss is slightly higher with `xavier_normal_`, but we end up with the same accuracy after five epochs of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Xavier uniform, Kaiming uniform loss samples uniformly between a lower and upper bound $(-bound, bound)$. This time we use the following function to generate our $bound$:\n",
    "\n",
    "$\\sqrt \\frac{6}{(1 + a^{2}) \\cdot fan\\_in}$\n",
    "\n",
    "We set $a$ to the negative slope of the rectifier (e.g. for a leaky ReLU activation function). Since we're using ReLU, our negative values are set to zero, and thus we'll just use the default value of 0 for $a$. We end up with the following:\n",
    "\n",
    "$\\sqrt \\frac{6}{2 \\cdot fan\\_in}$\n",
    "\n",
    "Thus if $fan\\_in > fan\\_out$, the mean value of our weights will be greater with Xavier uniform than with Kaiming uniform. If $fan\\_in < fan\\_out$, the mean value will be greater with Kaiming uniform than with Xavier uniform. However, we also have the option of using $fan\\_out$ instead of $fan\\_in$ to compute our $bound$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add Kaiming uniform to our average loss and accuracy tables:\n",
    "\n",
    "Average loss:\n",
    "\n",
    "`Epoch   Xavier uniform    Xavier normal    Kaiming uniform\n",
    "   1         .0745             .0921            .0615\n",
    "   2         .0587             .0702            .0501\n",
    "   3         .0434             .0478            .0416\n",
    "   4         .0446             .0414            .0441\n",
    "   5         .0346             .0423            .0360`\n",
    "   \n",
    "Accuracy:\n",
    "\n",
    "`Epoch    Xavier uniform     Xavier normal  Kaiming uniform\n",
    "   1         .9781             .9717            .9807\n",
    "   2         .9817             .9773            .9839\n",
    "   3         .9859             .9848            .9861\n",
    "   4         .9852             .9859            .9860\n",
    "   5         .9884             .9884            .9883`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add Kaiming normal to our loss and accuracy tables:\n",
    "\n",
    "Average loss:\n",
    "\n",
    "`Epoch  Xavier uni  Xavier norm   Kaiming uni   Kaiming norm\n",
    "   1      .0745        .0921        .0615         .0824\n",
    "   2      .0587        .0702        .0501         .0654\n",
    "   3      .0434        .0478        .0416         .0489\n",
    "   4      .0446        .0414        .0441         .0416\n",
    "   5      .0346        .0423        .0360         .0452`\n",
    "   \n",
    "Accuracy:\n",
    "\n",
    "`Epoch  Xavier uni   Xavier norm  Kaiming uni  Kaiming norm\n",
    "   1      .9781        .9717        .9807         .9733\n",
    "   2      .9817        .9773        .9839         .9771\n",
    "   3      .9859        .9848        .9861         .9840\n",
    "   4      .9852        .9859        .9860         .9851\n",
    "   5      .9884        .9884        .9883         .9852`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `orthogonal_` initializes the tensor with an orthogonal matrix. See Saxe et al. 2014: https://arxiv.org/pdf/1312.6120.pdf. This yields the following:\n",
    "\n",
    "Average loss:\n",
    "\n",
    "`Epoch Xavier uni  Xavier norm  Kaiming uni  Kaiming norm  Orthogonal\n",
    "   1      .0745      .0921        .0615         .0824        .0855\n",
    "   2      .0587      .0702        .0501         .0654        .0613\n",
    "   3      .0434      .0478        .0416         .0489        .0499\n",
    "   4      .0446      .0414        .0441         .0416        .0354\n",
    "   5      .0346      .0423        .0360         .0452        .0448`\n",
    "   \n",
    "Accuracy:\n",
    "\n",
    "`Epoch Xavier uni  Xavier norm  Kaiming uni  Kaiming norm  Orthogonal\n",
    "   1      .9781      .9717        .9807         .9733        .9737\n",
    "   2      .9817      .9773        .9839         .9771        .9805\n",
    "   3      .9859      .9848        .9861         .9840        .9838\n",
    "   4      .9852      .9859        .9860         .9851        .9885\n",
    "   5      .9884      .9884        .9883         .9852        .9853`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of accuracy, there isn't too much variation between these functions, at least for this simple image classification task.\n",
    "\n",
    "Out of curiosity, how does PyTorch initialize other layers? This is done in `nn.modules` using the `reset_parameters` method of the constructor of each `Module`.\n",
    "\n",
    "Attention layers initialize weights with Xavier uniform, `in_proj_bias` and `out_proj_bias` with zeros, and `bias_k` and `bias_v` with Xavier normal.\n",
    "\n",
    "Batch, group, instance, and layer normalization layers initialize weights with ones and biases with zeros.\n",
    "\n",
    "Convolutional, linear, and bilinear layers initialize weights with Kaiming uniform and biases uniformly, with `bound` defined as `1 / math.sqrt(fan_in)`.\n",
    "\n",
    "RNN layers initialize weights uniformly, with `bound` defined as `1 / math.sqrt(self.hidden_size)`.\n",
    "\n",
    "Sparse layers initialize weights normally.\n",
    "\n",
    "Transformer layers initialize weights with Xavier uniform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
