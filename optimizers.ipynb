{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fun with optimzers**\n",
    "\n",
    "Neural networks are generally trained with some form of gradient descent. Yet there are tons of flavors of \"gradient-based optimization algorithms\" - see this [fantastic blog post](http://ruder.io/optimizing-gradient-descent/), which will serve as a primary reference for this tutorial (Ruder 2016)\n",
    "\n",
    "We'll cover mathematical logic, high-level execution in PyTorch, and lower-level implementation in PyTorch's C/C++ backend. Let's get into it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook, we'll refer to our parameters at $\\theta$ and update them from time $t$ to time $t+1$. In other words, we'll perform some operation to update our parameters over one time step from $\\theta_{t}$ to $\\theta_{t+1}$\n",
    "\n",
    "Each gradient descent optimizer begins by computing the partial derivative of the cost function $J(\\theta)$ at time $t$ (i.e. $J(\\theta_{t})$) with respect to each parameter. The *gradient* $\\nabla$ is just the vectorized set of derivatives (scalar:vector :: derivative:gradient ?) We then scale the gradient by a learning rate $\\eta$ and subtract the result from our $\\theta_{t}$ to get our $\\theta_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent**\n",
    "\n",
    "The most straightforward implementation of gradient descent is (arguably) \"batch\" or \"vanilla gradient descent.\" This computes the gradients using all of our data and then performs one parameter update. In math:\n",
    "\n",
    "(1) $\\theta_{t+1} = \\theta_{t} - \\eta \\cdot J(\\theta_{t})$\n",
    "\n",
    "However, batch gradient descent doesn't work well with large datasets. Imagine if we were performing batch gradient descent on the ImageNet 2012 dataset (something like 138 GB). We'd have to load all 138 GB into memory and use the entire dataset to update our parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, what if we were to update parameters one example at a time? This is the idea behind *stochastic gradient descent*. In math:\n",
    "\n",
    "(2) $\\theta_{t+1} = \\theta_{t} - \\eta \\cdot J(\\theta_{t}; x^{(i)}; y^{(i)})$\n",
    "\n",
    "The superscipted $(i)$ simply tells us to calculate our gradient and perform our updates after passing each example through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet PyTorch implements neither pure batch nor pure stochastic gradient descent. Instead, it uses something called \"mini-batch gradient descent.\" Rather than performing parameter updates over the entire dataset (batch) or one example at a time (stochastic), mini-batch gradient descent calculates the gradient and applies parameter updates to a \"mini-batch\" of *n* training examples:\n",
    "\n",
    "(3) $\\theta_{t+1} = \\theta_{t} - \\eta \\cdot J(\\theta_{t}; x^{(i:i+n)}; y^{(i:i+n)})$\n",
    "\n",
    "Here the superscripted $(i:i+n)$ tells us to calculate our gradient and perform updates for $n$ observations $i$ through $i+n$, where $i$ is the first observation of our mini-batch.\n",
    "\n",
    "PyTorch actually refers to mini-batch gradient descent as \"stochastic gradient descent,\" which is kind of confusing for someone trying to wrap their head around optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [documentation](https://pytorch.org/docs/stable/optim.html) explains the use of optimizers in PyTorch. `torch.optim.Optimizer` is the base class which contains our optimizer subclasses: `Adadelta`, `Adagrad`, `Adam`, `AdamW`, `SparseAdam`, `Adamax`, `ASGD`, `LBFGS`, `RMSprop`, `Rprop`, and `SGD`.\n",
    "\n",
    "However, before doing any of this, we need to train a model. A relatively easy way to do this is to use the `mnist` network from the PyTorch `examples` repository. From the command line:\n",
    "\n",
    "`git clone https://github.com/pytorch/examples.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is defined in `main.py`. Let's run the network from the command line for ten epochs and report the accuracy on the validation data:\n",
    "\n",
    "`python main.py --epochs=10`\n",
    "\n",
    "We get the following loss and accuracy on our test set after each epoch. These numbers were consistent across two runs of the script for the first epoch:\n",
    "\n",
    "`Epoch 1: Average loss: 0.1017, Accuracy: 9669/10000 (97%)\n",
    "Epoch 2: Average loss: 0.0614, Accuracy: 9828/10000 (98%)\n",
    "Epoch 3: Average loss: 0.0562, Accuracy: 9809/10000 (98%)\n",
    "Epoch 4: Average loss: 0.0409, Accuracy: 9864/10000 (99%)\n",
    "Epoch 5: Average loss: 0.0384, Accuracy: 9873/10000 (99%)\n",
    "Epoch 6: Average loss: 0.0336, Accuracy: 9893/10000 (99%)\n",
    "Epoch 7: Average loss: 0.0343, Accuracy: 9876/10000 (99%)\n",
    "Epoch 8: Average loss: 0.0391, Accuracy: 9878/10000 (99%)\n",
    "Epoch 9: Average loss: 0.0288, Accuracy: 9909/10000 (99%)\n",
    "Epoch 10: Average loss: 0.0314, Accuracy: 9895/10000 (99%)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we search for `optim`, we see that our `optimizer` is defined as follows:\n",
    "\n",
    "`optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)`\n",
    "\n",
    "Thus we're using (mini-batch) \"stochastic\" gradient descent. If we consult the documentation, we see that `optim.SGD` takes six arguments (data type in parentheses):\n",
    "\n",
    "`params` (iterable): the parameters we're optimizing, in this case the parameters from our model trained on the MNIST dataset (a five-layer convolutional neural network that suspiciously resembles - but is not identical to - LeNet-5)\n",
    "\n",
    "`lr` (float): our learning rate $\\eta$. This is optionally passed as an argument when calling `python main.py`. If no argument is passed, `lr` defaults to `0.01`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`momentum` (float): We can optionally add momentum to accelerate gradient descent. [Sutskever et al. 2013](http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf) includes the \"classical\" mathematical formalization of momentum:\n",
    "\n",
    "(4) $v_{t+1} = \\mu v_{t} - \\epsilon \\nabla f(\\theta_{t})$ <br/>\n",
    "(5) $\\theta_{t+1} = \\theta_{t} + v_{t+1}$\n",
    "\n",
    "Let's unpack this. $\\nabla f(\\theta_{t})$ is just the gradient of our loss function at time $t$ (the formalization here uses $f(t)$ instead of $J(t)$). Similarly, $\\epsilon$ is our learning rate, equivalent to $\\eta$ in our formulations.\n",
    "\n",
    "Rather than simply subtracting our scaled (by the learning rate) gradient from $\\theta_{t}$ to get $\\theta_{t+1}$, we now introduce a momentum term $v$, which we similarly update from time $t$ to time $t+1$. We scale $v_{t}$ by $\\mu$ and update the result in the same manner as we update our parameters using gradient descent:\n",
    "\n",
    "(6) $v_{t+1} = \\mu v_{t} - \\epsilon \\nabla f(\\theta_{t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update our parameters to $\\theta_{t+1}$, we simply add our updated momentum term to $\\theta_{t}$:\n",
    "\n",
    "(7) $\\theta_{t+1} = \\theta_{t} + v_{t+1}$\n",
    "\n",
    "By default, PyTorch implements `SGD` without momentum (`momentum=0`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dampening` isn't explained in the documentation. Someone [campaigned for its removal](https://github.com/pytorch/pytorch/issues/6) a while back, but the PyTorch overlords decided to set it to 0 instead. Let's not worry about it for now - it's only an argument for `SGD` - we can examine its implementation when we dive into the C/C++ backend.\n",
    "\n",
    "`weight_decay` adds a L2 regularization term (again, this is 0 by default, and we'll worry about implementation when we get to the backend).\n",
    "\n",
    "`nesterov` computes updates using Nesterov's Acclerated Gradient if set to `True` (again, it's `False` by default). All this means is that we add our scaled momentum term at time $t$ to our parameters before computing our gradients. Once we've updated our momentum, the parameter update is identical:\n",
    "\n",
    "(8) $v_{t+1} = \\mu v_{t} - \\epsilon \\nabla f(\\theta_{t} + \\mu v_{t})$\n",
    "\n",
    "(9) $\\theta_{t+1} = \\theta_{t} + v_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adagrad**\n",
    "\n",
    "Next up, we'll examine the Adagrad optimization algorithm. Instead of using a fixed learning rate, Adagrad \"**ada**pts the learning rate to the parameters... smaller updates (lower learning rates) for parameters associated with frequently occurring features, and larger updates (higher learning rates) for parameters associated with infrequent features\" (Ruder 2016). \n",
    "\n",
    "How does this work? We need to define different learning rates for different parameters. Recall that our gradient at time $t$ is defined as $\\nabla J(\\theta_{t})$ (or $\\nabla f(\\theta_{t})$ if you prefer the more general function symbol $f(t)$ to the more specific loss function symbol $J(t)$). We can specify that we are computing the gradient for parameters $\\theta$ by rewriting the gradient as $\\nabla_{\\theta} J(\\theta_{t})$\n",
    "\n",
    "Now let's say we want to specify the gradient further, to represent a single parameter $\\theta_{i}$ rather than the entire set of parameters $\\theta$. At a single point in time, our set of parameters becomes $\\theta_{t}$ (easily represented as a vector) and our single parameter becomes $\\theta_{t,i}$ (which we can represent as an element of the vector $\\theta_{t}$). Our gradient $g$ for this single parameter $i$ at time $t$ is thus $\\eta_{t,i} = \\nabla_{\\theta} J(\\theta_{t,i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this definition $g_{i,t}$ of our gradient $g$ for parameter $i$ at time $t$, we can redefine (mini-batch) stochastic gradient descent as performing the following update:\n",
    "\n",
    "(10) $\\theta_{t+1,i} = \\theta_{t,i} - \\eta \\cdot \\eta_{t,i}$\n",
    "\n",
    "So where does Adagrad come in? In place of the general learning rate $\\eta$, we now have this term:\n",
    "\n",
    "(11) $\\frac{\\eta}{\\sqrt{G_{t,(i,i)} + \\epsilon}}$\n",
    "\n",
    "Here $G_{t}$ is a diagonal matrix (all the entries are zero except for those on the main diagonal). The elements of $G_{t}$ are denoted by $i,i$, and each of these \"is the sum of the squares of the gradients with respect to $\\theta_{i}$ up to time step $t$ (Ruder 2016). We add $\\epsilon$ so that our denominator cannot equal zero (thus making the term undefined). Putting it all together, we have the following rule to update parameters with Adagrad:\n",
    "\n",
    "(12) $\\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\eta}{\\sqrt{G_{t,(i,i)} + \\epsilon}} \\cdot \\eta_{t,i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then extend Adagrad to all parameters by taking the product of our modified diagonal matrix and our vector of parameters:\n",
    "\n",
    "(13) $\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{G_{t} + \\epsilon}} \\cdot \\eta_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement Adagrad in PyTorch. `torch.optim.Adagrad` takes five arguments:\n",
    "\n",
    "`params`: same as `SGD`.\n",
    "\n",
    "`lr`: same as `SGD`. The default `lr` for Adagrad is 0.01, identical to that in `examples/mnist/main.py`, so let's use that value.\n",
    "\n",
    "`lr_decay`: We can also decrease our learning rate by a specified factor after a specified number of epochs. It isn't clear how PyTorch implements this, but we're going to stick with the default value of `0` anyway, so let's not worry about it for now.\n",
    "\n",
    "`weight_decay`: same as `SGD` (default also `0`).\n",
    "\n",
    "`eps`: this is our $\\epsilon$ - we'll use the (tiny) default value of 1e-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we swap in `torch.optim.Adagrad` as our optimizer (making sure to remove the `momentum` argument, which is not present in `Adagrad`)? Our `optimizer` now equals `optim.Adagrad(model.parameters())` - or `optim.Adagrad(model.parameters(), lr=args.lr)` if we explicitly pass the learning rate as `args.lr`. However, `args.lr` is set to the default value of `0.01`, so we don't need to include (or exclude) this argument.\n",
    "\n",
    "Let's compare accuracy with `SGD` (not worrying for now about training speed):\n",
    "\n",
    "`Epoch    Loss (SGD)    Loss (Adagrad)   Acc (SGD)   Acc (Adagrad)\n",
    " 1        0.1017        0.0479           0.9669      0.9843\n",
    " 2        0.0614        0.0349           0.9828      0.9888\n",
    " 3        0.0562        0.0315           0.9809      0.9897\n",
    " 4        0.0409        0.0291           0.9864      0.9908\n",
    " 5        0.0384        0.0285           0.9873      0.9907\n",
    " 6        0.0336        0.0251           0.9893      0.9912\n",
    " 7        0.0343        0.0253           0.9876      0.9920\n",
    " 8        0.0391        0.0306           0.9878      0.9899\n",
    " 9        0.0288        0.0266           0.9909      0.9922\n",
    " 10       0.0314        0.0294           0.9895      0.9897`\n",
    " \n",
    "What we see is that Adagrad tends to converge faster (in terms of the number of epochs necessary to train to e.g. 99% accuracy). However, by 10 epochs the gap between SGD and Adagrad has decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adadelta**\n",
    "\n",
    "Whereas Adagrad stores all prior squared gradients, Adadelta limits the number of accumulated gradients to a constant $w$. We do this by first defining our \"sum of gradients... recursively... as a decaying average of all past squared gradients\" (Ruder 2016). The output of this accumulation is a \"running average\" $E[g^{2}]_{t}$, which is computed as a weighted sum of its value at the prior time step $E[g^{2}]_{t-1}$ and the value of the gradient at the current time $g^{2}_{t}$. The relative weight of the prior average and current gradient is defined by $\\gamma$ such that:\n",
    "\n",
    "(14) $E[g^{2}]_{t} = \\gamma[g^{2}]_{t-1} + (1-\\gamma)g^{2}_{t}$\n",
    "\n",
    "Note that PyTorch uses the parameter `rho` ($\\rho$) rather than gamma ($\\gamma$) to weight the prior average relative to the current gradient. Ruder suggests 0.9 for this (momentum) term, and PyTorch implements this value by default in `torch.optim.Adadelta`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a \"parameter update vector\" $\\Delta \\theta_{t}$ as the change in our gradient at time $t$. Again, this is simply the gradient scaled (negatively) by the learning rate (since we are subtracting our scaled gradient from the prior parameter values). This yields the following expression:\n",
    "\n",
    "(15) $\\Delta \\theta_{t} = -\\eta \\cdot g_{t,i}$\n",
    "\n",
    "To get our updated parameter vector, we simply add our parameter update vector to our current parameter vector (if we're updating from $t$ to $t+1$. If we're updating from $t-1$ to $t$, we'd be updating from the prior to current parameter vector):\n",
    "\n",
    "(16) $\\theta_{t+1} = \\theta_{t} + \\Delta \\theta_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our definition of $\\Delta \\theta_{t}$ in (15) to define the parameter update vector in Adagrad as follows:\n",
    "\n",
    "(17) $\\Delta \\theta_{t} = - \\frac{\\eta}{\\sqrt{G_{t} + \\epsilon}} \\cdot g_{t}$\n",
    "\n",
    "We then swap in our running average $E[g^{2}]_{t}$ for Adagrad's diagonal matrix of summed square gradients $G_{t}$:\n",
    "\n",
    "(18) $\\Delta \\theta_{t} = - \\frac{\\eta}{\\sqrt{E[g^{2}]_{t} + \\epsilon}} \\cdot g_{t}$\n",
    "\n",
    "Since our denominator is equivalent to the root mean squared (RMS) error:\n",
    "\n",
    "(19) $\\Delta \\theta_{t} = - \\frac{\\eta}{RMS[g]_{t}} \\cdot g_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruder (2016), citing the author of the Adadelta paper (Zeiler 2012), points out that there's a mismatch between the (hypothetica) unites of the parameter update $\\Delta \\theta_{t}$ and the original parameter vector $\\theta_{t}$. To solve this problem, Zeiler constructs an \"exponentially decaying average\" analogous to our momentum term in (14) but with squared parameter updates $\\Delta [\\theta^{2}]_{t}$:\n",
    "\n",
    "(20) $E[\\Delta \\theta^{2}]_{t} = \\gamma E[\\Delta \\theta^{2}]_{t-1} + (1-\\gamma) \\Delta \\theta^{2}_{t}$\n",
    "\n",
    "As with (19), we can redefine (20) as the RMS error of parameter updates:\n",
    "\n",
    "(21) $RMS[\\Delta \\theta]_{t} = \\sqrt{E[\\theta^{2}]_{t} + \\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't actually know the RMS error of parameter updates at time $t$, we use the RMS error of parameter updates up to the prior time $t-1$. Swapping the RMS error up to $t-1$ for our learning rate in (19) gives us our parameter update vector for Adadelta:\n",
    "\n",
    "(22) $\\theta_{t} = - \\frac{RMS[\\Delta \\theta]_{t-1}}{RMS[g]_{t}} \\cdot g_{t}$\n",
    "\n",
    "And again, we get our updated parameter vector by adding our parameter update vector to our current parameter vector:\n",
    "\n",
    "(23) $\\theta_{t+1} = \\theta_{t} + \\Delta \\theta_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and implement Adadelta in PyTorch. `torch.optim.Adadelta` takes the following arguments:\n",
    "\n",
    "`params`: our model parameters (same as in `SGD` and `Adagrad`).\n",
    "\n",
    "`rho`: momentum term by which we weight our current and squared gradients when computing the running average. We'll use the default value 0.9 - no need to pass a value explicitly to `optim.Adadelta`.\n",
    "\n",
    "`eps`: epsilon value added to denominator. Again, we'll accept the default value 1e-6.\n",
    "\n",
    "`lr`: our learning rate, which starts at 1.0 before adapting to parameters - we'll accept this default.\n",
    "\n",
    "`weight_decay`: optional L2 penalty - we'll accept the default of zero penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now swap in `Adadelta` so that `optimizer = optim.Adadelta(model.parameters())` (making sure to remove `lr=args.lr` if it's still there, or at least (re-)setting `lr` to its default value of `1.0` for `Adadelta`. We'll run for 10 epochs again, recording our loss and accuracy for the test set after each epoch and comparing it to our results for `SGD` and `Adagrad`:\n",
    "\n",
    "`Ep / Loss: SGD / Adagrad / Adadelta  Acc: SGD / Adagrad / Adadelta\n",
    " 1    .1017      .0479      .0315     .9669     .9843      .9901\n",
    " 2    .0614      .0349      .0280     .9828     .9888      .9902\n",
    " 3    .0562      .0315      .0295     .9809     .9897      .9912\n",
    " 4    .0409      .0291      .0345     .9864     .9908      .9905\n",
    " 5    .0384      .0285      .0364     .9873     .9907      .9918\n",
    " 6    .0336      .0251      .0276     .9893     .9912      .9924\n",
    " 7    .0343      .0253      .0360     .9876     .9920      .9922\n",
    " 8    .0391      .0306      .0289     .9878     .9899      .9929\n",
    " 9    .0288      .0266      .0323     .9909     .9922      .9933\n",
    " 10   .0314      .0294      .0366     .9895     .9897      .9931`\n",
    " \n",
    "Adadelta appears to converge to higher accuracy quicker than Adagrad, which in turn converges quicker than SGD. While it generally has the highest accuracy, its loss is slightly higher than Adagrad's for five of the seven epochs between four and 10 (inclusive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RMSprop**\n",
    "\n",
    "RMSprop is simply the first stage of the Adadelta update, as defined in (14):\n",
    "\n",
    "(24) $E[g^{2}]_{t} = \\gamma[g^{2}]_{t-1} + (1-\\gamma)g^{2}_{t}$\n",
    "\n",
    "We then swap this into our gradient update function (18), generating the following:\n",
    "\n",
    "(25) $\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{E[g^{2}]_{t} + \\epsilon}} \\cdot g_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the mathematical implementation of RMSprop is straightforward once we know how to implement Adadelta, `torch.optim.RMSprop` actually takes seven arguments. We already know `params`, `lr` (default: 0.01), `eps` ($\\epsilon$, default: 1e-8), and `weight_decay` (default: 0). `centered` is a boolean variable (default: `False`) that \"normalizes the gradient by an estimation of its variance\" if `True` (the `torch.optim` documentation does not say how this is done.)\n",
    "\n",
    "As for the other two arguments, the PyTorch documentation defines the \"effective learning rate\" as $\\alpha / (\\sqrt{v} + \\epsilon)$. This appears to refer to the coefficient on the gradient $g_{t}$, where $\\alpha$ (`alpha`) is our initial learning rate $\\eta$, $v$ is our \"weighted moving average of the squared gradient,\" and $\\epsilon$ is `epsilon`. The remaining argument `momentum` is (apparently) the $\\gamma$ from (24), but is set to 0 by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train `main.py` for 10 epochs using `torch.optim.RMSprop` with default arguments, recording our model loss and accuracy on the test set:\n",
    "\n",
    "Loss:\n",
    "\n",
    "`Ep    SGD       Adagrad    Adadelta   RMSprop\n",
    " 1    .1017      .0479      .0315      .2838\n",
    " 2    .0614      .0349      .0280      .1506\n",
    " 3    .0562      .0315      .0295      .1037\n",
    " 4    .0409      .0291      .0345      .1589\n",
    " 5    .0384      .0285      .0364      .0973\n",
    " 6    .0336      .0251      .0276      .1391\n",
    " 7    .0343      .0253      .0360      .3066\n",
    " 8    .0391      .0306      .0289      .2229\n",
    " 9    .0288      .0266      .0323      .1393\n",
    " 10   .0314      .0294      .0366      .1693`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "`Ep    SGD       Adagrad    Adadelta  RMSprop\n",
    " 1    .9669      .9843      .9901     .9172\n",
    " 2    .9828      .9888      .9902     .9592\n",
    " 3    .9809      .9897      .9912     .9733\n",
    " 4    .9864      .9908      .9905     .9740\n",
    " 5    .9873      .9907      .9918     .9756\n",
    " 6    .9893      .9912      .9924     .9678\n",
    " 7    .9876      .9920      .9922     .9484\n",
    " 8    .9878      .9899      .9929     .9491\n",
    " 9    .9909      .9922      .9933     .9693\n",
    " 10   .9895      .9897      .9931     .9686`\n",
    " \n",
    "Yikes. Not very accurate, at least with default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adam**\n",
    "\n",
    "The Adaptive Moment Estimation (Adam) optimizer computes both an exponentially decaying average of prior gradients (first moment) and prior squared gradients (second moment, like Adadelta and RMSprop):\n",
    "\n",
    "(26) $m_{t} = \\beta_{1} m_{t-1} + (1 - \\beta_{1}) g_{t}$\n",
    "\n",
    "(27) $v_{t} = \\beta_{2} v_{t-1} + (1 - \\beta_{2}) g^{2}_{t}$\n",
    "\n",
    "However, when we initialize $m_{t}$ and $v_{t}$ as zero vectors, both terms bias towards zero early in the training process and when $\\beta_{1}$ and $\\beta_{2}$ are close to 1 (small decay rate). The Adam optimizer corrects for this by dividing the momentum terms by their decay rates:\n",
    "\n",
    "(28) $\\hat m_{t} = \\frac{m_{t}}{1 - \\beta^{t}_{1}}$\n",
    "\n",
    "(29) $\\hat v_{t} = \\frac{v_{t}}{1 - \\beta^{t}_{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've computed $\\hat m_{t}$ and $\\hat v_{t}$, we can plug in the former term in place of our gradient in (25). As one might expect, the initial learning rate is divided by the square root of the latter term. However, our epsilon is outside of the root:\n",
    "\n",
    "(30) $\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{^v_{t}}+\\epsilon} \\hat m_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our network using Adam with default argument values. We also have the option to use `amsgrad` (default: False), which we'll get to next.\n",
    "\n",
    "Loss:\n",
    "\n",
    "`Ep    SGD      Adagrad   Adadelta  RMSprop    Adam\n",
    " 1    .1017     .0479     .0315     .2838     .0368\n",
    " 2    .0614     .0349     .0280     .1506     .0358\n",
    " 3    .0562     .0315     .0295     .1037     .0289\n",
    " 4    .0409     .0291     .0345     .1589     .0309\n",
    " 5    .0384     .0285     .0364     .0973     .0275\n",
    " 6    .0336     .0251     .0276     .1391     .0255\n",
    " 7    .0343     .0253     .0360     .3066     .0398\n",
    " 8    .0391     .0306     .0289     .2229     .0292\n",
    " 9    .0288     .0266     .0323     .1393     .0324\n",
    " 10   .0314     .0294     .0366     .1693     .0442`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "`Ep    SGD      Adagrad   Adadelta  RMSprop    Adam\n",
    " 1    .9669     .9843     .9901     .9172     .9876\n",
    " 2    .9828     .9888     .9902     .9592     .9880\n",
    " 3    .9809     .9897     .9912     .9733     .9990\n",
    " 4    .9864     .9908     .9905     .9740     .9905\n",
    " 5    .9873     .9907     .9918     .9756     .9920\n",
    " 6    .9893     .9912     .9924     .9678     .9930\n",
    " 7    .9876     .9920     .9922     .9484     .9898\n",
    " 8    .9878     .9899     .9929     .9491     .9913\n",
    " 9    .9909     .9922     .9933     .9693     .9917\n",
    " 10   .9895     .9897     .9931     .9686     .9903`\n",
    " \n",
    "Adam thus performs similarly to Adagrad and Adadelta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AMSGrad**\n",
    "\n",
    "AMSGrad introduces an additional step to Adam (and the `AdamW` variant, which we'll get to next):\n",
    "\n",
    "(31) $\\hat v_{t} = max(\\hat v_{t-1}, v_{t})$\n",
    "\n",
    "We then pass $\\hat v_{t}$ to our gradient update step. Let's compare loss and accuracy for `Adam` with `amsgrad=true` to our other optimizers:\n",
    "\n",
    "Loss:\n",
    "\n",
    "`Ep    SGD      Adagrad   Adadelta  RMSprop    Adam     Adam(AMSGrad)\n",
    " 1    .1017     .0479     .0315     .2838     .0368     .0364\n",
    " 2    .0614     .0349     .0280     .1506     .0358     .0361\n",
    " 3    .0562     .0315     .0295     .1037     .0289     .0289\n",
    " 4    .0409     .0291     .0345     .1589     .0309     .0295\n",
    " 5    .0384     .0285     .0364     .0973     .0275     .0262\n",
    " 6    .0336     .0251     .0276     .1391     .0255     .0226\n",
    " 7    .0343     .0253     .0360     .3066     .0398     .0237\n",
    " 8    .0391     .0306     .0289     .2229     .0292     .0295\n",
    " 9    .0288     .0266     .0323     .1393     .0324     .0280\n",
    " 10   .0314     .0294     .0366     .1693     .0442     .0291`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "`Ep    SGD      Adagrad   Adadelta  RMSprop    Adam     Adam(AMSGrad)\n",
    " 1    .9669     .9843     .9901     .9172     .9876     .9881\n",
    " 2    .9828     .9888     .9902     .9592     .9880     .9885\n",
    " 3    .9809     .9897     .9912     .9733     .9990     .9898\n",
    " 4    .9864     .9908     .9905     .9740     .9905     .9918\n",
    " 5    .9873     .9907     .9918     .9756     .9920     .9917\n",
    " 6    .9893     .9912     .9924     .9678     .9930     .9928\n",
    " 7    .9876     .9920     .9922     .9484     .9898     .9928\n",
    " 8    .9878     .9899     .9929     .9491     .9913     .9912\n",
    " 9    .9909     .9922     .9933     .9693     .9917     .9915\n",
    " 10   .9895     .9897     .9931     .9686     .9903     .9929`\n",
    " \n",
    "Loss and accuracy for Adam is thus similar with or without AMSGrad. We'll remove this from future comparisons, sticking to the default arguments for each optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdamW**\n",
    "\n",
    "This one isn't explained in Ruder (2016), but there's a paper that goes into AdamW's modifications to Adam (Loschilov & Hutter 2019). AdamW \"decouples the optimal choice of weight decay factor from the setting of the learning rate.\" The paper goes into what this means exactly. Let's see if it works on MNIST with our simple CNN:\n",
    "\n",
    "Loss:\n",
    "\n",
    "`Ep    SGD      Adagrad   Adadelta  RMSprop    Adam     AdamW\n",
    " 1    .1017     .0479     .0315     .2838     .0368     .0380\n",
    " 2    .0614     .0349     .0280     .1506     .0358     .0320\n",
    " 3    .0562     .0315     .0295     .1037     .0289     .0283\n",
    " 4    .0409     .0291     .0345     .1589     .0309     .0267\n",
    " 5    .0384     .0285     .0364     .0973     .0275     .0278\n",
    " 6    .0336     .0251     .0276     .1391     .0255     .0399\n",
    " 7    .0343     .0253     .0360     .3066     .0398     .0372\n",
    " 8    .0391     .0306     .0289     .2229     .0292     .0344\n",
    " 9    .0288     .0266     .0323     .1393     .0324     .0389\n",
    " 10   .0314     .0294     .0366     .1693     .0442     .0346`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "`Ep    SGD      Adagrad   Adadelta  RMSprop    Adam     AdamW\n",
    " 1    .9669     .9843     .9901     .9172     .9876     .9871\n",
    " 2    .9828     .9888     .9902     .9592     .9880     .9905\n",
    " 3    .9809     .9897     .9912     .9733     .9990     .9913\n",
    " 4    .9864     .9908     .9905     .9740     .9905     .9925\n",
    " 5    .9873     .9907     .9918     .9756     .9920     .9912\n",
    " 6    .9893     .9912     .9924     .9678     .9930     .9891\n",
    " 7    .9876     .9920     .9922     .9484     .9898     .9915\n",
    " 8    .9878     .9899     .9929     .9491     .9913     .9910\n",
    " 9    .9909     .9922     .9933     .9693     .9917     .9910\n",
    " 10   .9895     .9897     .9931     .9686     .9903     .9918`\n",
    " \n",
    "There doesn't seem to be much differentiating our four `Ada` optimizers. Maybe we need a tougher classification problem, such as CIFAR, or maybe another task altogether?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaMax**\n",
    "\n",
    "Recall our formula for computing our second moment in Adam. We can rewrite $g_{t}^{2}$ as the equivalent $|g_{t}|^{2}$:\n",
    "\n",
    "(32) $v_{t} = \\beta_{2} v_{t-1} + (1 - \\beta_{2}) |g_{t}|^{2}$\n",
    "\n",
    "Then we can generalize our updates to any degree of moment ($l_{p}$ norm):\n",
    "\n",
    "(33) $v_{t} = \\beta^{p}_{2} v_{t-1} + (1 - \\beta^{p}_{2}) |g_{t}|^{p}$\n",
    "\n",
    "As $p$ increases without bound, $v_{t}$ converges to the following value, denoted by $u_{t}$:\n",
    "\n",
    "(34) $u_{t} = \\beta^{\\infty}_{2} v_{t-1} + (1 - \\beta^{\\infty}_{2}) |g_{t}|^{\\infty} = max(\\beta_{2} \\cdot v_{t-1}, |g_{t}|)$\n",
    "\n",
    "Swapping this into (30), we have our parameter update rule for AdaMax:\n",
    "\n",
    "(35) $\\theta_{t-1} = \\theta_{t} - \\frac{\\eta}{u_{t}} \\hat m_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test loss and accuracy for `torch.optim.Adamax` with defaults:\n",
    "\n",
    "Loss:\n",
    "\n",
    "`Ep   SGD     Adagrad  Adadelta RMSprop   Adam    AdamW    Adamax\n",
    " 1   .1017    .0479    .0315    .2838    .0368    .0380    .0422\n",
    " 2   .0614    .0349    .0280    .1506    .0358    .0320    .0382\n",
    " 3   .0562    .0315    .0295    .1037    .0289    .0283    .0348\n",
    " 4   .0409    .0291    .0345    .1589    .0309    .0267    .0265\n",
    " 5   .0384    .0285    .0364    .0973    .0275    .0278    .0217\n",
    " 6   .0336    .0251    .0276    .1391    .0255    .0399    .0244\n",
    " 7   .0343    .0253    .0360    .3066    .0398    .0372    .0246\n",
    " 8   .0391    .0306    .0289    .2229    .0292    .0344    .0246\n",
    " 9   .0288    .0266    .0323    .1393    .0324    .0389    .0249\n",
    " 10  .0314    .0294    .0366    .1693    .0442    .0346    .0337`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "`Ep   SGD     Adagrad  Adadelta RMSprop   Adam    AdamW    Adamax\n",
    " 1   .9669    .9843    .9901    .9172    .9876    .9871    .9869\n",
    " 2   .9828    .9888    .9902    .9592    .9880    .9905    .9874\n",
    " 3   .9809    .9897    .9912    .9733    .9990    .9913    .9871\n",
    " 4   .9864    .9908    .9905    .9740    .9905    .9925    .9912\n",
    " 5   .9873    .9907    .9918    .9756    .9920    .9912    .9933\n",
    " 6   .9893    .9912    .9924    .9678    .9930    .9891    .9923\n",
    " 7   .9876    .9920    .9922    .9484    .9898    .9915    .9928\n",
    " 8   .9878    .9899    .9929    .9491    .9913    .9910    .9919\n",
    " 9   .9909    .9922    .9933    .9693    .9917    .9910    .9930\n",
    " 10  .9895    .9897    .9931    .9686    .9903    .9918    .9907`\n",
    " \n",
    "This is about as good as we've done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SparseAdam**\n",
    "\n",
    "This is a \"lazy version of Adam suitable for sparse tensors\" (Ruder 2016). We only perform updates on moments \"that show up in the gradient.\" If we try to run `torch.optim.SparseAdam` on `main.py`, we get the following error message:\n",
    "\n",
    "`RuntimeError: SparseAdam does not support dense gradients, please consider Adam instead`\n",
    "\n",
    "Ok, on to the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ASGD**\n",
    "\n",
    "`torch.optim.ASGD` applies Averaged Stochastic Gradient Descent. According to [Wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Averaging), ASGD \"records an average of its parameter vector over time.\" We then use this averaged parameter vector to update our parameters. Let's see how it performs:\n",
    "\n",
    "Loss:\n",
    "\n",
    "`Ep  SGD    Adagrad Adadelta RMSprop  Adam   AdamW   Adamax  ASGD\n",
    " 1  .1017   .0479   .0315    .2838   .0368   .0380   .0422  .1680\n",
    " 2  .0614   .0349   .0280    .1506   .0358   .0320   .0382  .1000\n",
    " 3  .0562   .0315   .0295    .1037   .0289   .0283   .0348  .0727\n",
    " 4  .0409   .0291   .0345    .1589   .0309   .0267   .0265  .0586\n",
    " 5  .0384   .0285   .0364    .0973   .0275   .0278   .0217  .0616\n",
    " 6  .0336   .0251   .0276    .1391   .0255   .0399   .0244  .0451\n",
    " 7  .0343   .0253   .0360    .3066   .0398   .0372   .0246  .0440\n",
    " 8  .0391   .0306   .0289    .2229   .0292   .0344   .0246  .0457\n",
    " 9  .0288   .0266   .0323    .1393   .0324   .0389   .0249  .0346\n",
    " 10 .0314   .0294   .0366    .1693   .0442   .0346   .0337  .0392`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "\n",
    "`Ep  SGD    Adagrad Adadelta RMSprop  Adam   AdamW   Adamax  ASGD\n",
    " 1  .9669   .9843   .9901    .9172   .9876   .9871   .9869  .9504\n",
    " 2  .9828   .9888   .9902    .9592   .9880   .9905   .9874  .9711\n",
    " 3  .9809   .9897   .9912    .9733   .9990   .9913   .9871  .9773\n",
    " 4  .9864   .9908   .9905    .9740   .9905   .9925   .9912  .9819\n",
    " 5  .9873   .9907   .9918    .9756   .9920   .9912   .9933  .9811\n",
    " 6  .9893   .9912   .9924    .9678   .9930   .9891   .9923  .9852\n",
    " 7  .9876   .9920   .9922    .9484   .9898   .9915   .9928  .9850\n",
    " 8  .9878   .9899   .9929    .9491   .9913   .9910   .9919  .9848\n",
    " 9  .9909   .9922   .9933    .9693   .9917   .9910   .9930  .9889\n",
    " 10 .9895   .9897   .9931    .9686   .9903   .9918   .9907  .9866`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LBFGS**\n",
    "\n",
    "The L-BFGS optimizer, implemented with `torch.optim.LBFGS`, is \"heavily inspired by *minFunc*,\" though it's unclear from the documentation alone what L-BFGS does specifically. `LBFGS` also requires this additional `closure` argument. Since the implementation is unclear, the optimizer isn't covered in Ruder (2016), and the script won't run without the `closure` argument in `step()`, we'll skip this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rprop**\n",
    "\n",
    "Rprop is the \"resilient backpropagation algorithm.\" Ruder (2016) doesn't mention Rprop, and the documentation doesn't explain its implementation. We get pretty terrible average loss after one epoch (14.2588), with accuracy of .9050, and it doesn't even appear that the loss is converging, so let's also skip this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That covers the optimizers in PyTorch! For MNIST, it looks like we're good with any of the momentum-based optimizers. It'd be interesting to compare accuracy and loss across more complex classification problems (e.g. RGB, 100 or 1000 classes) and other tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
