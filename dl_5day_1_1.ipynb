{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEEP LEARNING IN FIVE DAYS**\n",
    "\n",
    "*Patrick Donnelly*\n",
    "\n",
    "Day one: Defining a use case for deep learning\n",
    "\n",
    "Chapter one: Introduction and image classification\n",
    "\n",
    "In *deep learning* (DL), a team of researchers or engineers use *artifical neural networks* (ANNs) to learn models from data (*training*). These models are then used to generate predictions from data (*inference*). In other words: \n",
    "\n",
    "`training: data + network = model` and\n",
    "\n",
    "`inference: model + data = predictions`\n",
    "\n",
    "Strictly speaking, a *prediction* refer to a forecast, or \"statement about a future event\" (https://en.wikipedia.org/wiki/Prediction). We'll use the term more loosely to refer to the output of passing data through a trained model.\n",
    "\n",
    "In this workshop, we'll examine the primary use cases for deep learning. We'll do this kinda backwards, identifying use cases for algorithms rather than vice versa. Ultimately, it doesn't matter whether we start with use cases or with algorithms. We just need to come up with a mapping of one to the other.\n",
    "\n",
    "Throughout this tutorial, we will be using *PyTorch*, a *framework* for training *deep neural networks*. We'll get into the details of the mathematical operations involved in training neural networks later. For now, just know that a *framework* defines these operations at a high level of abstraction. It'll make our code less verbose, and it'll enable engineers to build neural networks without explictly defining and optimizing all the operations, for better or worse. \n",
    "\n",
    "PyTorch is written in Python with a C++ backend and optimized acceleration for NVIDIA GPUs with CUDA and cuDNN. CUDA is NVIDIA's software platform for parallel processing with CPUs and NVIDIA GPUs. cuDNN optimizes the operations for training neural networks for NVIDIA GPUs. You can run PyTorch on CPU-only systems, but you'll quickly find that you'll need GPU or some form of parallel processor if you want to train neural networks with millions of parameters on gigabytes of data in a reasonable amount of time.\n",
    "\n",
    "We import PyTorch as `torch`. There's another framework out there called Torch, on which PyTorch is loosely based. It's written in Lua and ceased active development in 2018 (see https://en.wikipedia.org/wiki/Torch_(machine_learning). PyTorch is also influenced by NumPy. You'll see that a lot of `torch` is analogous or even identical to `numpy`, and a lot of the `torch.nn` module for constructing neural networks is analogous or identical to the `nn` module in Lua Torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's begin mapping out the use cases for deep learning. We'll start with perhaps the best-known set of DL applications: the domain of computer vision (CV). Our input for each of these examples is an array of pixels, which we typically represent as a *tensor* data type in PyTorch. For instance, the commonly-used MNIST dataset consists of 60,000 training images and 10,000 test images. Remember, it's necessary to partition our dataset into training and test data. We can't evaluate how our model is doing with the data used for training the model. Machine learning 101!\n",
    "\n",
    "Each image in the MNIST dataset is 28 pixels high and 28 pixels wide. These are grayscale images, and thus only have one color channel, in \"contrast\" to e.g. RGB images that have three color channels. Our tensor takes four dimensions: batch size, channels, height, and width. We now know that `channels` is 1, `height` is 28, and `width` is 28.\n",
    "\n",
    "`batch_size` tells us how many images to feed to the network at once. Let's go with a single image for now, but if we want to exploit the parallelism of a GPU, not to mention multiple GPUs and multiple nodes of multiple GPUs, we'll want to increase our batch size. Typically this is done in multiples of two (since we're software engineers) until saturating GPU memory (then you can decrease by n/4, increase by n/8, etc.)\n",
    "\n",
    "Let's create a random 28x28 image. We'll let each pixel take values between 0 and 255 inclusive (8 bits). To generate this image, we'll pass the parameters `0` (lower bound, inclusive), `256` (upper bound, exclusive), and `(28,28)` (width and height of array) to `torch.randint`. Throughout, we'll use `x` to denote an input datum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[135, 208,  73,  90,  53, 101,  32,  40,  92, 108, 183, 255, 110, 103,\n",
       "           2,  30, 205, 111, 233, 226, 108, 189, 101, 155, 233, 105,   8,   5],\n",
       "        [143, 183,  51,  99, 105, 143,  73, 231, 254, 175, 125,  84, 164, 210,\n",
       "          38, 129,  56, 249, 118, 169,  42, 125, 227, 203, 164, 215, 249, 168],\n",
       "        [ 83,  89,  69,  87,  19, 213, 153, 181, 200,  87, 211, 130, 111, 190,\n",
       "          86,  75,  57, 161, 197,  53,  24, 103,  12, 174, 226, 144,  64, 195],\n",
       "        [162, 118,  41, 212, 118,  83, 146,  91, 227, 178,  36, 231, 105, 165,\n",
       "         156,  95, 185,  96, 250, 141, 177,  13, 106, 203, 150, 247, 103, 180],\n",
       "        [249,   5,  76,  47,  75, 170,  54, 151,  87, 105, 155,  85, 210,  97,\n",
       "         248,  82, 166, 250, 107,  44, 213, 242, 164,  75,  26, 233,  72,  45],\n",
       "        [230,  91,  63, 108,  38, 221, 228,  89, 111,  25, 226,  94, 109,  73,\n",
       "         179, 242, 123, 147,  90, 158, 231, 157, 201, 236, 206, 199, 207, 138],\n",
       "        [ 76, 169, 150, 127, 219, 191, 119,  72, 236,  24, 203, 250, 163, 113,\n",
       "          13, 220, 171, 106,  11, 247, 125, 119, 169, 166, 103, 112,  36, 156],\n",
       "        [ 65, 237,  26,   4, 242,  74,  18, 166, 194, 184,   1, 169, 149, 218,\n",
       "         157,  54, 139, 106,  55,  89, 185,  80,  50, 163, 165, 171, 108, 132],\n",
       "        [158, 136,  32, 168,   9, 110,  80,   2,   2, 120, 108, 218,  93,  69,\n",
       "          48, 252, 160,  16, 153, 157, 160,  58,  43,   9, 178, 212, 247, 116],\n",
       "        [199, 216, 215, 102,  57, 228, 253, 195,  51, 170, 253, 246, 216, 173,\n",
       "         166,  92, 121, 186, 109, 131,  24, 209,  27,   3, 195, 131,  16,  42],\n",
       "        [ 35, 241, 245, 146, 238, 194,  62, 154, 195, 164, 127,  16,  73,  48,\n",
       "         135, 113, 147,  56, 219, 207, 178, 222, 200,  21, 221, 150,  89,  46],\n",
       "        [220, 132,  95,  98,  26,  26, 186,  67, 169, 119, 203, 187,  23, 151,\n",
       "         113, 233,  92,  63, 158, 192, 117,  16, 206, 162,  88, 185,  32, 127],\n",
       "        [109, 243,  87, 125, 194,   3, 181, 245, 150, 119, 125, 137,  98, 206,\n",
       "         167, 166, 243,  21, 217,  12,  99,  87,   0,  97, 100, 183, 201,   6],\n",
       "        [127, 130, 164,  68, 107, 248, 217, 112,  56,   4, 224, 187, 107, 210,\n",
       "          77, 145, 119,  18,  67, 136, 142,  46,  45, 176,  32,  51, 144,  12],\n",
       "        [ 39, 116, 111, 224,   7,  42, 154, 146, 101, 181, 184, 217, 182, 112,\n",
       "          12, 149,  98, 102, 237, 186,  45,   5, 223,  26,  78, 232, 102,  69],\n",
       "        [146, 237, 106, 186,  49,  39,  27, 144,  53, 208, 229, 125, 237, 250,\n",
       "         246,  90, 246, 141, 107, 189, 241,  87,   6,  12, 234, 177,  75, 250],\n",
       "        [242,  92,  78, 108, 139, 120, 191, 191, 122, 251,  57, 148,  40, 170,\n",
       "         229,  79,  24, 110, 218, 105,  59, 127,  56,  90, 167, 219, 191, 120],\n",
       "        [ 74,  60,  29,  40,  38,  72,  59,  20, 142,  59,  85, 214,  28, 236,\n",
       "         100, 155,  98, 157,   5, 118, 133,  65,  91,  76,  10,   0,  55, 175],\n",
       "        [ 49, 138,  93,  72, 254, 194, 151, 107, 135, 150,  69, 202, 174, 250,\n",
       "         185, 204, 191, 141, 152, 221,  16, 216, 224, 233, 208,  68,  13, 202],\n",
       "        [ 31, 102, 178,  86, 195,  44,  24, 116, 112, 101, 225,  81, 187, 172,\n",
       "         226, 196, 211, 246, 234, 143, 164,  78, 162, 181,  50, 152, 122, 140],\n",
       "        [ 15, 126, 235, 108, 166, 164, 123, 247, 152, 103, 220,  32, 204,  20,\n",
       "         109, 150, 139, 121,  13,  89, 168, 184,  30, 136, 159, 101, 172, 113],\n",
       "        [124, 157,  82, 213, 113, 131,  14,  80, 205, 101,  69, 226, 236, 213,\n",
       "         172, 104, 130, 189, 131, 113, 230,  60, 138, 210,  67,  14, 222, 235],\n",
       "        [235,  99,  59, 242, 215,  61, 184, 179,  38, 223, 193, 241, 105, 163,\n",
       "          27, 166, 179, 136, 227, 135, 192, 131, 234, 105, 155, 108, 196,  78],\n",
       "        [226, 193,  27, 108, 252,  20,  49, 153,   5,  58,  94, 188, 241, 123,\n",
       "          44,  75, 188, 208, 171, 157, 168,  68,  76,  92,  68,  78, 166, 207],\n",
       "        [205,  91,  24, 171, 101, 237, 236,  56, 216,  93, 211, 244,  32, 105,\n",
       "         244, 234, 130, 237, 122,  51, 108, 195,  32,  32, 210, 175, 167, 109],\n",
       "        [254, 197,  14,  37,  88,  85, 232, 132, 255, 238, 251,  27,  12, 182,\n",
       "          59, 103, 130,  30, 170, 204,  25, 202,   5, 187, 226, 104,  91, 161],\n",
       "        [ 58, 209, 110, 172, 128, 114, 182,  94,  45, 241,  87,  99, 247,  74,\n",
       "         156,  22,  41, 150, 126,  79,  98, 135, 151,  55,  91, 192, 185,  33],\n",
       "        [ 68, 110, 162, 160, 199, 132, 118, 138,  30, 224, 224,  79, 197, 161,\n",
       "          35,   4,  39, 120,  91,  36,  91, 147, 250, 114,  67, 150,  83, 130]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(0, 256, (28,28))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investigate the shape of `x` with `x.shape`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.shape)\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to *pad* our original image with four layers of zeroes before passing it through LeNet-5. This is because LeNet-5 expects a 32x32 image as input, and our image is of size 28x28. We can do this with `numpy.pad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32)\n",
      "[[  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0 135 ...   5   0   0]\n",
      " ...\n",
      " [  0   0  68 ... 130   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.pad(x, (2,2), 'constant')\n",
    "print(x.shape)\n",
    "print(x)\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops, looks like applying np.pad converted our `torch.Tensor` to an `np.ndarray`. Fortunately, it's really easy to convert data stored in an `ndarray` to a `Tensor`. We should also note that (for object `x`) `type(x)` is a Python function that returns the object type (in this case a `torch.Tensor`), while `x.type()` is a `torch.Tensor` method that returns the more specific data type (in this case a `torch.FloatTensor`, or array of 32-bit floating point data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32])\n",
      "tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0., 135.,  ...,   5.,   0.,   0.],\n",
      "        ...,\n",
      "        [  0.,   0.,  68.,  ..., 130.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.]])\n",
      "torch.FloatTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(x)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(x.type())\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can reshape the image with the `view` method. We pass our dimensions `(batch size, channels, height, width)` as parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "          [  0.,   0., 135.,  ...,   5.,   0.,   0.],\n",
       "          ...,\n",
       "          [  0.,   0.,  68.,  ..., 130.,   0.,   0.],\n",
       "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(1, 1, 32, 32)\n",
    "print(x.shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider first the task of *image classification*, also known as *object recognition*. The goal here is to take our input image and identify it as belonging to one of a number of classes. To take a canonical example for a canonical dataset, our classes could correspond to digits. Our output in this case would be a vector of length 10 (the number of digits). Let's further assume that our random image is the digit `4` (it doesn't look much like a 4, but nevermind). We would thus represent our *target* output as a *one-hot-encoding*. Think of this as an array of zeroes where the index of the *ground truth* class - the class corresponding to the label of the image - is flipped from zero to one. Let's call this vector `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0,0,0,0,1,0,0,0,0,0]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch makes it even easier to define class labels. Just define a one-dimensional tensor with value equal to the class index. We'll assign that value to `y` (replacing the list from our prior code block):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.Tensor([4])\n",
    "print(y.type())\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks in PyTorch expect a `torch.FloatTensor` input, our label `y` needs to be a 64-bit \"long\" integer (`torch.LongTensor`). Let's cast `y` to `long`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.LongTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.long()\n",
    "print(y.type())\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Now we need to build a *neural network* to generate a model output from our input `x`. We'll call our model output `Y`. Our model output will be a *probability vector* whose (non-negative) entries sum to one (see https://en.wikipedia.org/wiki/Probability_vector). Interestingly, a one-hot encoding is a type of probability vector, despite its deterministic appearance (probability vectors are also konwn as \"stochastic vectors\").\n",
    "\n",
    "To generate this model output, we'll apply a series of operations to our input. We'll get into the details more later. For now, let's consider the seminal network *LeNet-5* as an example of a *convolutional neural network* for image classification. Named for Yann LeCun, one of the \"founding fathers\" of deep learning, LeNet-5 contains *linear*, also known as *dense* or *fully-connected* operations (matrix multiplications with optional additions), as well as (two-dimensional) *convolutional* operations, which generate feature maps by sliding a *kernel* (also known as a *filter*) across the input(s) with a particular *stride*.\n",
    "\n",
    "We also apply *rectified linear unit (ReLU)* activation functions, which zero out negative values before passing the output on to the next *layer* of operations. Finally, we apply *max pooling*, which downsamples our input values by taking the maximum value over a specified region.\n",
    "\n",
    "Before doing all this, we need to import a couple more modules. `torch.nn` gives us the modules and classes we need to define our layers. `torch.nn.functional` helps us with our pooling and activation functions. By convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our network. This is based on https://github.com/kuangliu/pytorch-cifar/blob/master/models/lenet.py and modified to take one-channel inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)   \n",
    "        return x\n",
    "    \n",
    "net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pass `x` through `net`. Again, we'll call our model output `Y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ -3.6606,   5.5909, -10.9270,  -8.1490, -12.8929,  12.9511,   4.3634,\n",
       "           5.5695,  -6.7247,   1.1291]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = net(x)\n",
    "print(Y.shape)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our output is a 1x10 tensor, analogous to a vector of length 10. Note also that our tensor is not a probability vector. We can convert it to a probability vector using `torch.softmax`. However, when we update our model parameters (a few cells down), we will pass our raw output `Y`, not our probability vector `torch.softmax(Y, dim = 1)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.0954e-08, 6.3517e-04, 4.2586e-11, 6.8501e-10, 5.9633e-12, 9.9855e-01,\n",
       "         1.8612e-04, 6.2173e-04, 2.8463e-09, 7.3305e-06]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(Y, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our cells sum to 1. We have to do `sum` twice, since we sum over two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(torch.softmax(Y, dim = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can update our model parameters. First we'll import the `torch.optim` package, which will enable us to define our optimizer function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's *backpropagate* (update our weights and biases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "loss = ce_loss(Y, y)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've skimmed over a lot of the details in defining a basic template. Let's now consider some use cases!\n",
    "\n",
    "While there are many applications of image classification, most of them require additional algorithms. For instance, *optical character recognition* (OCR) involves first *detecting* an object within a larger image and then classifying the image. Although an image classification network such as LeNet-5 could be used to identify handwritten digits, we would need another algorithm to identify objects in an image as \"digits\" before applying a classification network such as LeNet-5 to classify these images as one digit or another. (There are other ways to do object detection - this is just an example.)\n",
    "\n",
    "Perhaps the most famous application of convolutional neural networks for image classification is the *ImageNet Large Scale Visual Recognition Challenge* (ILSVRC). ILSVRC is an annual competition to evaluate the accuracy of CNNs in classifying approximately 140 GB (for ILSVRC 2012) of color images as belonging to one of 1000 classes (to be precise, the network gets five guesses...) This may seem kinda odd. How can a competition between research teams be more influential for applied deep learning than, uh, an actual application? But without the dominance of CNNs for image classification, beginning in 2012 with AlexNet producing half the error rate of non-ANN algorithms, we wouldn't have seen the proliferation of more practical deep learning applications.\n",
    "\n",
    "The three instances cited here each involve more than simple classification. However, each are common applications which could not be done efficiently without a convolutional neural network for image classification:\n",
    "\n",
    "1. Reading bank checks: LeCun et al. (1998) propose an application of LeNet-5(!!) for identifying the handwritten digits in deposited checks. This system was \"deployed commercially and read several million checks per day\" (http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) \n",
    "\n",
    "2. Image search (eBay): Take a photo with your phone and upload it to eBay's mobile app. eBay will identify the photo and search for similar items. While the *search* part of the application goes beyond simple classification, the classification is presumably done using an image classifcation network (https://www.ebayinc.com/stories/news/find-it-on-ebay-using-pictures-instead-of-words/)\n",
    "\n",
    "3. Facial recognition (Facebook): Facebook used applied facial recognition for \"tagging\" friends well before the technology extended to controversial applications such as law enforcement body cameras and airport check-ins (https://www.facebook.com/notes/facebook/making-photo-tagging-easier/467145887130/)\n",
    "\n",
    "In the next section, we'll examine neural networks for more complex computer vision tasks. Many of these follow logically from classification. See you there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
